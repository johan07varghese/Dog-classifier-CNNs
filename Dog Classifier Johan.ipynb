{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_files       \n",
    "from keras.utils import np_utils\n",
    "import numpy as np\n",
    "\n",
    "# define function to load train, test, and validation datasets\n",
    "def load_dataset(path):\n",
    "    data = load_files(path)\n",
    "    dog_files = np.array(data['filenames'])\n",
    "    dog_targets = np_utils.to_categorical(np.array(data['target']), 133)\n",
    "    return dog_files, dog_targets\n",
    "\n",
    "# load train, test, and validation datasets\n",
    "train_files, train_targets = load_dataset('dogImages/train')\n",
    "valid_files, valid_targets = load_dataset('dogImages/valid')\n",
    "test_files, test_targets = load_dataset('dogImages/test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['dogImages/train\\\\095.Kuvasz\\\\Kuvasz_06442.jpg',\n",
       "       'dogImages/train\\\\057.Dalmatian\\\\Dalmatian_04054.jpg',\n",
       "       'dogImages/train\\\\088.Irish_water_spaniel\\\\Irish_water_spaniel_06014.jpg',\n",
       "       ..., 'dogImages/train\\\\029.Border_collie\\\\Border_collie_02069.jpg',\n",
       "       'dogImages/train\\\\046.Cavalier_king_charles_spaniel\\\\Cavalier_king_charles_spaniel_03261.jpg',\n",
       "       'dogImages/train\\\\048.Chihuahua\\\\Chihuahua_03416.jpg'],\n",
       "      dtype='<U99')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import image                  \n",
    "from tqdm import tqdm\n",
    "\n",
    "def path_to_tensor(img_path):\n",
    "    # loads RGB image as PIL.Image.Image type\n",
    "    img = image.load_img(img_path, target_size=(224, 224))\n",
    "    # convert PIL.Image.Image type to 3D tensor with shape (224, 224, 3)\n",
    "    x = image.img_to_array(img)\n",
    "    # convert 3D tensor to 4D tensor with shape (1, 224, 224, 3) and return 4D tensor\n",
    "    return np.expand_dims(x, axis=0)\n",
    "\n",
    "def paths_to_tensor(img_paths):\n",
    "    list_of_tensors = [path_to_tensor(img_path) for img_path in tqdm(img_paths)]\n",
    "    return np.vstack(list_of_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6680/6680 [03:59<00:00, 27.84it/s]\n",
      "100%|██████████| 835/835 [01:20<00:00,  8.79it/s]\n",
      "100%|██████████| 836/836 [01:01<00:00, 13.58it/s]\n"
     ]
    }
   ],
   "source": [
    "from PIL import ImageFile                            \n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True                 \n",
    "\n",
    "# pre-process the data for Keras\n",
    "train_tensors = paths_to_tensor(train_files).astype('float32')/255\n",
    "valid_tensors = paths_to_tensor(valid_files).astype('float32')/255\n",
    "test_tensors = paths_to_tensor(test_files).astype('float32')/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.6313726 , 0.38431373, 0.01960784],\n",
       "        [0.63529414, 0.3882353 , 0.02352941],\n",
       "        [0.6313726 , 0.38431373, 0.01176471],\n",
       "        ...,\n",
       "        [0.4745098 , 0.08627451, 0.00392157],\n",
       "        [0.44705883, 0.08627451, 0.        ],\n",
       "        [0.47058824, 0.10980392, 0.01960784]],\n",
       "\n",
       "       [[0.63529414, 0.3882353 , 0.02352941],\n",
       "        [0.6313726 , 0.38431373, 0.01960784],\n",
       "        [0.6392157 , 0.39215687, 0.01960784],\n",
       "        ...,\n",
       "        [0.4862745 , 0.09019608, 0.01176471],\n",
       "        [0.4509804 , 0.08235294, 0.        ],\n",
       "        [0.4745098 , 0.10588235, 0.00784314]],\n",
       "\n",
       "       [[0.63529414, 0.3882353 , 0.02352941],\n",
       "        [0.627451  , 0.38039216, 0.01568628],\n",
       "        [0.6431373 , 0.39607844, 0.02352941],\n",
       "        ...,\n",
       "        [0.4862745 , 0.09019608, 0.01176471],\n",
       "        [0.44313726, 0.06666667, 0.        ],\n",
       "        [0.4627451 , 0.08627451, 0.        ]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0.21176471, 0.10980392, 0.05098039],\n",
       "        [0.21568628, 0.11372549, 0.05490196],\n",
       "        [0.19607843, 0.10588235, 0.04313726],\n",
       "        ...,\n",
       "        [0.21568628, 0.10588235, 0.10196079],\n",
       "        [0.23921569, 0.11372549, 0.07058824],\n",
       "        [0.25490198, 0.12941177, 0.08627451]],\n",
       "\n",
       "       [[0.20392157, 0.10196079, 0.04313726],\n",
       "        [0.21176471, 0.10980392, 0.05098039],\n",
       "        [0.19607843, 0.10588235, 0.04313726],\n",
       "        ...,\n",
       "        [0.19607843, 0.08627451, 0.08235294],\n",
       "        [0.22352941, 0.09803922, 0.05490196],\n",
       "        [0.24313726, 0.11764706, 0.07450981]],\n",
       "\n",
       "       [[0.19607843, 0.09411765, 0.03529412],\n",
       "        [0.22352941, 0.12156863, 0.0627451 ],\n",
       "        [0.20392157, 0.11372549, 0.05098039],\n",
       "        ...,\n",
       "        [0.14117648, 0.04313726, 0.02745098],\n",
       "        [0.19215687, 0.07450981, 0.04313726],\n",
       "        [0.20784314, 0.09019608, 0.05882353]]], dtype=float32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tensors[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D,Dropout\n",
    "from keras.layers import Dropout, Flatten, Dense\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import ModelCheckpoint \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 224, 224, 16)      208       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 112, 112, 16)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 112, 112, 32)      2080      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 56, 56, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 56, 56, 64)        8256      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 28, 28, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 28, 28, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 50176)             0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 512)               25690624  \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 133)               68229     \n",
      "=================================================================\n",
      "Total params: 25,769,397\n",
      "Trainable params: 25,769,397\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(filters=16, kernel_size=2, padding='same', activation='relu', input_shape=(224, 224, 3)))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "\n",
    "model.add(Conv2D(filters=32, kernel_size=2, padding='same', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "\n",
    "model.add(Conv2D(filters=64, kernel_size=2, padding='same', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Dense(133, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "184/334 [===============>..............] - ETA: 0s - loss: 4.9214 - accuracy: 0.0000e+ - ETA: 5:07 - loss: 11.1449 - accuracy: 0.0000e+0 - ETA: 7:45 - loss: 10.0211 - accuracy: 0.0000e+0 - ETA: 8:05 - loss: 8.9283 - accuracy: 0.0000e+0 - ETA: 8:04 - loss: 8.2185 - accuracy: 0.0000e+ - ETA: 8:09 - loss: 7.7070 - accuracy: 0.0000e+ - ETA: 8:38 - loss: 7.3409 - accuracy: 0.0000e+ - ETA: 8:36 - loss: 7.1026 - accuracy: 0.0000e+ - ETA: 8:39 - loss: 6.8593 - accuracy: 0.0000e+ - ETA: 8:51 - loss: 6.6884 - accuracy: 0.0000e+ - ETA: 8:55 - loss: 6.5306 - accuracy: 0.0045   - ETA: 8:56 - loss: 6.3968 - accuracy: 0.00 - ETA: 9:21 - loss: 6.2946 - accuracy: 0.00 - ETA: 9:16 - loss: 6.2047 - accuracy: 0.00 - ETA: 9:13 - loss: 6.1257 - accuracy: 0.00 - ETA: 9:02 - loss: 6.0565 - accuracy: 0.00 - ETA: 8:55 - loss: 5.9906 - accuracy: 0.00 - ETA: 8:47 - loss: 5.9339 - accuracy: 0.00 - ETA: 8:41 - loss: 5.8793 - accuracy: 0.00 - ETA: 8:32 - loss: 5.8307 - accuracy: 0.00 - ETA: 8:26 - loss: 5.7851 - accuracy: 0.00 - ETA: 8:19 - loss: 5.7463 - accuracy: 0.00 - ETA: 8:15 - loss: 5.7077 - accuracy: 0.00 - ETA: 8:10 - loss: 5.6789 - accuracy: 0.00 - ETA: 8:06 - loss: 5.6481 - accuracy: 0.00 - ETA: 8:16 - loss: 5.6182 - accuracy: 0.00 - ETA: 8:14 - loss: 5.5912 - accuracy: 0.00 - ETA: 8:11 - loss: 5.5676 - accuracy: 0.00 - ETA: 8:06 - loss: 5.5444 - accuracy: 0.00 - ETA: 8:02 - loss: 5.5223 - accuracy: 0.00 - ETA: 7:59 - loss: 5.5010 - accuracy: 0.00 - ETA: 7:57 - loss: 5.4793 - accuracy: 0.00 - ETA: 7:52 - loss: 5.4624 - accuracy: 0.00 - ETA: 7:48 - loss: 5.4465 - accuracy: 0.00 - ETA: 7:45 - loss: 5.4300 - accuracy: 0.00 - ETA: 7:42 - loss: 5.4132 - accuracy: 0.00 - ETA: 7:39 - loss: 5.3977 - accuracy: 0.00 - ETA: 7:35 - loss: 5.3854 - accuracy: 0.00 - ETA: 7:35 - loss: 5.3725 - accuracy: 0.00 - ETA: 7:32 - loss: 5.3610 - accuracy: 0.00 - ETA: 7:29 - loss: 5.3497 - accuracy: 0.00 - ETA: 7:25 - loss: 5.3396 - accuracy: 0.00 - ETA: 7:22 - loss: 5.3301 - accuracy: 0.00 - ETA: 7:23 - loss: 5.3200 - accuracy: 0.00 - ETA: 7:20 - loss: 5.3103 - accuracy: 0.00 - ETA: 7:18 - loss: 5.3002 - accuracy: 0.00 - ETA: 7:19 - loss: 5.2910 - accuracy: 0.00 - ETA: 7:17 - loss: 5.2833 - accuracy: 0.00 - ETA: 7:14 - loss: 5.2745 - accuracy: 0.01 - ETA: 7:11 - loss: 5.2663 - accuracy: 0.01 - ETA: 7:08 - loss: 5.2582 - accuracy: 0.00 - ETA: 7:05 - loss: 5.2508 - accuracy: 0.00 - ETA: 7:02 - loss: 5.2443 - accuracy: 0.00 - ETA: 7:00 - loss: 5.2377 - accuracy: 0.00 - ETA: 6:56 - loss: 5.2307 - accuracy: 0.00 - ETA: 6:53 - loss: 5.2247 - accuracy: 0.00 - ETA: 6:51 - loss: 5.2175 - accuracy: 0.00 - ETA: 6:50 - loss: 5.2120 - accuracy: 0.00 - ETA: 6:47 - loss: 5.2060 - accuracy: 0.00 - ETA: 6:45 - loss: 5.2001 - accuracy: 0.00 - ETA: 6:42 - loss: 5.1953 - accuracy: 0.00 - ETA: 6:39 - loss: 5.1904 - accuracy: 0.00 - ETA: 6:37 - loss: 5.1857 - accuracy: 0.00 - ETA: 6:34 - loss: 5.1823 - accuracy: 0.00 - ETA: 6:36 - loss: 5.1775 - accuracy: 0.00 - ETA: 6:34 - loss: 5.1733 - accuracy: 0.00 - ETA: 6:32 - loss: 5.1692 - accuracy: 0.00 - ETA: 6:30 - loss: 5.1649 - accuracy: 0.00 - ETA: 6:29 - loss: 5.1605 - accuracy: 0.00 - ETA: 6:27 - loss: 5.1561 - accuracy: 0.00 - ETA: 6:25 - loss: 5.1521 - accuracy: 0.00 - ETA: 6:24 - loss: 5.1491 - accuracy: 0.00 - ETA: 6:22 - loss: 5.1452 - accuracy: 0.00 - ETA: 6:20 - loss: 5.1423 - accuracy: 0.00 - ETA: 6:19 - loss: 5.1388 - accuracy: 0.00 - ETA: 6:17 - loss: 5.1356 - accuracy: 0.00 - ETA: 6:15 - loss: 5.1326 - accuracy: 0.00 - ETA: 6:13 - loss: 5.1296 - accuracy: 0.01 - ETA: 6:11 - loss: 5.1261 - accuracy: 0.01 - ETA: 6:09 - loss: 5.1237 - accuracy: 0.01 - ETA: 6:08 - loss: 5.1207 - accuracy: 0.01 - ETA: 6:07 - loss: 5.1179 - accuracy: 0.01 - ETA: 6:05 - loss: 5.1147 - accuracy: 0.01 - ETA: 6:03 - loss: 5.1119 - accuracy: 0.01 - ETA: 6:00 - loss: 5.1090 - accuracy: 0.01 - ETA: 5:59 - loss: 5.1052 - accuracy: 0.00 - ETA: 5:57 - loss: 5.1042 - accuracy: 0.00 - ETA: 5:56 - loss: 5.1014 - accuracy: 0.00 - ETA: 5:54 - loss: 5.0991 - accuracy: 0.00 - ETA: 5:52 - loss: 5.0958 - accuracy: 0.00 - ETA: 5:50 - loss: 5.0935 - accuracy: 0.00 - ETA: 5:49 - loss: 5.0916 - accuracy: 0.00 - ETA: 5:48 - loss: 5.0894 - accuracy: 0.00 - ETA: 5:46 - loss: 5.0866 - accuracy: 0.00 - ETA: 5:44 - loss: 5.0844 - accuracy: 0.00 - ETA: 5:42 - loss: 5.0819 - accuracy: 0.00 - ETA: 5:40 - loss: 5.0795 - accuracy: 0.00 - ETA: 5:39 - loss: 5.0773 - accuracy: 0.00 - ETA: 5:36 - loss: 5.0758 - accuracy: 0.00 - ETA: 5:34 - loss: 5.0733 - accuracy: 0.00 - ETA: 5:33 - loss: 5.0715 - accuracy: 0.00 - ETA: 5:31 - loss: 5.0702 - accuracy: 0.00 - ETA: 5:29 - loss: 5.0679 - accuracy: 0.00 - ETA: 5:28 - loss: 5.0660 - accuracy: 0.00 - ETA: 5:26 - loss: 5.0643 - accuracy: 0.00 - ETA: 5:24 - loss: 5.0643 - accuracy: 0.00 - ETA: 5:23 - loss: 5.0622 - accuracy: 0.00 - ETA: 5:20 - loss: 5.0599 - accuracy: 0.00 - ETA: 5:19 - loss: 5.0575 - accuracy: 0.00 - ETA: 5:18 - loss: 5.0546 - accuracy: 0.00 - ETA: 5:17 - loss: 5.0529 - accuracy: 0.00 - ETA: 5:16 - loss: 5.0503 - accuracy: 0.00 - ETA: 5:14 - loss: 5.0493 - accuracy: 0.00 - ETA: 5:12 - loss: 5.0478 - accuracy: 0.00 - ETA: 5:11 - loss: 5.0474 - accuracy: 0.00 - ETA: 5:10 - loss: 5.0456 - accuracy: 0.00 - ETA: 5:08 - loss: 5.0436 - accuracy: 0.00 - ETA: 5:07 - loss: 5.0422 - accuracy: 0.00 - ETA: 5:05 - loss: 5.0406 - accuracy: 0.00 - ETA: 5:03 - loss: 5.0393 - accuracy: 0.01 - ETA: 5:02 - loss: 5.0372 - accuracy: 0.00 - ETA: 5:00 - loss: 5.0358 - accuracy: 0.00 - ETA: 4:59 - loss: 5.0350 - accuracy: 0.00 - ETA: 4:57 - loss: 5.0343 - accuracy: 0.00 - ETA: 4:55 - loss: 5.0335 - accuracy: 0.00 - ETA: 4:54 - loss: 5.0320 - accuracy: 0.00 - ETA: 4:52 - loss: 5.0305 - accuracy: 0.00 - ETA: 4:50 - loss: 5.0291 - accuracy: 0.00 - ETA: 4:49 - loss: 5.0280 - accuracy: 0.00 - ETA: 4:47 - loss: 5.0265 - accuracy: 0.00 - ETA: 4:46 - loss: 5.0248 - accuracy: 0.00 - ETA: 4:44 - loss: 5.0238 - accuracy: 0.01 - ETA: 4:43 - loss: 5.0225 - accuracy: 0.01 - ETA: 4:41 - loss: 5.0215 - accuracy: 0.01 - ETA: 4:40 - loss: 5.0207 - accuracy: 0.01 - ETA: 4:39 - loss: 5.0195 - accuracy: 0.01 - ETA: 4:37 - loss: 5.0187 - accuracy: 0.01 - ETA: 4:36 - loss: 5.0176 - accuracy: 0.01 - ETA: 4:34 - loss: 5.0163 - accuracy: 0.01 - ETA: 4:32 - loss: 5.0151 - accuracy: 0.01 - ETA: 4:31 - loss: 5.0143 - accuracy: 0.01 - ETA: 4:29 - loss: 5.0134 - accuracy: 0.01 - ETA: 4:27 - loss: 5.0130 - accuracy: 0.01 - ETA: 4:26 - loss: 5.0122 - accuracy: 0.01 - ETA: 4:24 - loss: 5.0112 - accuracy: 0.01 - ETA: 4:23 - loss: 5.0102 - accuracy: 0.01 - ETA: 4:21 - loss: 5.0092 - accuracy: 0.01 - ETA: 4:20 - loss: 5.0079 - accuracy: 0.01 - ETA: 4:18 - loss: 5.0070 - accuracy: 0.01 - ETA: 4:17 - loss: 5.0064 - accuracy: 0.01 - ETA: 4:15 - loss: 5.0054 - accuracy: 0.01 - ETA: 4:14 - loss: 5.0041 - accuracy: 0.01 - ETA: 4:12 - loss: 5.0030 - accuracy: 0.01 - ETA: 4:11 - loss: 5.0018 - accuracy: 0.01 - ETA: 4:09 - loss: 5.0011 - accuracy: 0.01 - ETA: 4:08 - loss: 5.0005 - accuracy: 0.01 - ETA: 4:06 - loss: 4.9991 - accuracy: 0.01 - ETA: 4:05 - loss: 4.9983 - accuracy: 0.01 - ETA: 4:03 - loss: 4.9981 - accuracy: 0.01 - ETA: 4:02 - loss: 4.9968 - accuracy: 0.01 - ETA: 4:01 - loss: 4.9958 - accuracy: 0.01 - ETA: 3:59 - loss: 4.9950 - accuracy: 0.01 - ETA: 3:58 - loss: 4.9943 - accuracy: 0.01 - ETA: 3:56 - loss: 4.9932 - accuracy: 0.01 - ETA: 3:55 - loss: 4.9924 - accuracy: 0.01 - ETA: 3:57 - loss: 4.9917 - accuracy: 0.01 - ETA: 4:01 - loss: 4.9905 - accuracy: 0.01 - ETA: 4:01 - loss: 4.9898 - accuracy: 0.01 - ETA: 4:00 - loss: 4.9883 - accuracy: 0.01 - ETA: 3:58 - loss: 4.9885 - accuracy: 0.01 - ETA: 3:56 - loss: 4.9877 - accuracy: 0.01 - ETA: 3:55 - loss: 4.9871 - accuracy: 0.01 - ETA: 3:53 - loss: 4.9858 - accuracy: 0.01 - ETA: 3:52 - loss: 4.9848 - accuracy: 0.01 - ETA: 3:50 - loss: 4.9838 - accuracy: 0.01 - ETA: 3:48 - loss: 4.9828 - accuracy: 0.01 - ETA: 3:47 - loss: 4.9825 - accuracy: 0.01 - ETA: 3:45 - loss: 4.9814 - accuracy: 0.01 - ETA: 3:44 - loss: 4.9811 - accuracy: 0.01 - ETA: 3:42 - loss: 4.9805 - accuracy: 0.01 - ETA: 3:41 - loss: 4.9796 - accuracy: 0.01 - ETA: 3:39 - loss: 4.9788 - accuracy: 0.01 - ETA: 3:38 - loss: 4.9781 - accuracy: 0.01 - ETA: 3:37 - loss: 4.9776 - accuracy: 0.0133\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "334/334 [==============================] - ETA: 3:36 - loss: 4.9765 - accuracy: 0.01 - ETA: 3:34 - loss: 4.9757 - accuracy: 0.01 - ETA: 3:33 - loss: 4.9747 - accuracy: 0.01 - ETA: 3:31 - loss: 4.9744 - accuracy: 0.01 - ETA: 3:30 - loss: 4.9737 - accuracy: 0.01 - ETA: 3:29 - loss: 4.9727 - accuracy: 0.01 - ETA: 3:27 - loss: 4.9715 - accuracy: 0.01 - ETA: 3:26 - loss: 4.9704 - accuracy: 0.01 - ETA: 3:24 - loss: 4.9689 - accuracy: 0.01 - ETA: 3:23 - loss: 4.9670 - accuracy: 0.01 - ETA: 3:21 - loss: 4.9654 - accuracy: 0.01 - ETA: 3:20 - loss: 4.9647 - accuracy: 0.01 - ETA: 3:18 - loss: 4.9640 - accuracy: 0.01 - ETA: 3:17 - loss: 4.9623 - accuracy: 0.01 - ETA: 3:15 - loss: 4.9603 - accuracy: 0.01 - ETA: 3:14 - loss: 4.9589 - accuracy: 0.01 - ETA: 3:12 - loss: 4.9579 - accuracy: 0.01 - ETA: 3:10 - loss: 4.9566 - accuracy: 0.01 - ETA: 3:09 - loss: 4.9561 - accuracy: 0.01 - ETA: 3:08 - loss: 4.9549 - accuracy: 0.01 - ETA: 3:06 - loss: 4.9533 - accuracy: 0.01 - ETA: 3:05 - loss: 4.9511 - accuracy: 0.01 - ETA: 3:04 - loss: 4.9513 - accuracy: 0.01 - ETA: 3:02 - loss: 4.9503 - accuracy: 0.01 - ETA: 3:01 - loss: 4.9508 - accuracy: 0.01 - ETA: 2:59 - loss: 4.9501 - accuracy: 0.01 - ETA: 2:58 - loss: 4.9492 - accuracy: 0.01 - ETA: 2:56 - loss: 4.9483 - accuracy: 0.01 - ETA: 2:56 - loss: 4.9470 - accuracy: 0.01 - ETA: 2:55 - loss: 4.9460 - accuracy: 0.01 - ETA: 2:54 - loss: 4.9440 - accuracy: 0.01 - ETA: 2:53 - loss: 4.9436 - accuracy: 0.01 - ETA: 2:52 - loss: 4.9425 - accuracy: 0.01 - ETA: 2:51 - loss: 4.9428 - accuracy: 0.01 - ETA: 2:49 - loss: 4.9417 - accuracy: 0.01 - ETA: 2:48 - loss: 4.9415 - accuracy: 0.01 - ETA: 2:46 - loss: 4.9405 - accuracy: 0.01 - ETA: 2:44 - loss: 4.9397 - accuracy: 0.01 - ETA: 2:43 - loss: 4.9380 - accuracy: 0.01 - ETA: 2:41 - loss: 4.9366 - accuracy: 0.01 - ETA: 2:40 - loss: 4.9361 - accuracy: 0.01 - ETA: 2:38 - loss: 4.9352 - accuracy: 0.01 - ETA: 2:38 - loss: 4.9344 - accuracy: 0.01 - ETA: 2:37 - loss: 4.9339 - accuracy: 0.01 - ETA: 2:36 - loss: 4.9331 - accuracy: 0.01 - ETA: 2:34 - loss: 4.9308 - accuracy: 0.01 - ETA: 2:33 - loss: 4.9301 - accuracy: 0.01 - ETA: 2:31 - loss: 4.9296 - accuracy: 0.01 - ETA: 2:30 - loss: 4.9288 - accuracy: 0.01 - ETA: 2:28 - loss: 4.9277 - accuracy: 0.01 - ETA: 2:26 - loss: 4.9268 - accuracy: 0.01 - ETA: 2:25 - loss: 4.9256 - accuracy: 0.01 - ETA: 2:23 - loss: 4.9242 - accuracy: 0.01 - ETA: 2:22 - loss: 4.9222 - accuracy: 0.01 - ETA: 2:20 - loss: 4.9227 - accuracy: 0.01 - ETA: 2:19 - loss: 4.9210 - accuracy: 0.01 - ETA: 2:17 - loss: 4.9207 - accuracy: 0.01 - ETA: 2:16 - loss: 4.9194 - accuracy: 0.01 - ETA: 2:14 - loss: 4.9186 - accuracy: 0.01 - ETA: 2:13 - loss: 4.9182 - accuracy: 0.01 - ETA: 2:11 - loss: 4.9175 - accuracy: 0.01 - ETA: 2:10 - loss: 4.9171 - accuracy: 0.01 - ETA: 2:08 - loss: 4.9160 - accuracy: 0.01 - ETA: 2:07 - loss: 4.9152 - accuracy: 0.01 - ETA: 2:05 - loss: 4.9135 - accuracy: 0.01 - ETA: 2:04 - loss: 4.9137 - accuracy: 0.02 - ETA: 2:02 - loss: 4.9128 - accuracy: 0.01 - ETA: 2:01 - loss: 4.9130 - accuracy: 0.02 - ETA: 1:59 - loss: 4.9121 - accuracy: 0.02 - ETA: 1:58 - loss: 4.9120 - accuracy: 0.01 - ETA: 1:56 - loss: 4.9103 - accuracy: 0.02 - ETA: 1:55 - loss: 4.9095 - accuracy: 0.01 - ETA: 1:53 - loss: 4.9081 - accuracy: 0.02 - ETA: 1:52 - loss: 4.9063 - accuracy: 0.02 - ETA: 1:50 - loss: 4.9059 - accuracy: 0.02 - ETA: 1:49 - loss: 4.9037 - accuracy: 0.02 - ETA: 1:47 - loss: 4.9029 - accuracy: 0.02 - ETA: 1:46 - loss: 4.9005 - accuracy: 0.02 - ETA: 1:44 - loss: 4.8997 - accuracy: 0.02 - ETA: 1:43 - loss: 4.8981 - accuracy: 0.02 - ETA: 1:41 - loss: 4.8964 - accuracy: 0.02 - ETA: 1:40 - loss: 4.8957 - accuracy: 0.02 - ETA: 1:38 - loss: 4.8941 - accuracy: 0.02 - ETA: 1:37 - loss: 4.8924 - accuracy: 0.02 - ETA: 1:35 - loss: 4.8917 - accuracy: 0.02 - ETA: 1:33 - loss: 4.8903 - accuracy: 0.02 - ETA: 1:32 - loss: 4.8903 - accuracy: 0.02 - ETA: 1:31 - loss: 4.8891 - accuracy: 0.02 - ETA: 1:29 - loss: 4.8885 - accuracy: 0.02 - ETA: 1:28 - loss: 4.8878 - accuracy: 0.02 - ETA: 1:26 - loss: 4.8862 - accuracy: 0.02 - ETA: 1:25 - loss: 4.8856 - accuracy: 0.02 - ETA: 1:23 - loss: 4.8861 - accuracy: 0.02 - ETA: 1:22 - loss: 4.8850 - accuracy: 0.02 - ETA: 1:20 - loss: 4.8854 - accuracy: 0.02 - ETA: 1:19 - loss: 4.8842 - accuracy: 0.02 - ETA: 1:17 - loss: 4.8828 - accuracy: 0.02 - ETA: 1:16 - loss: 4.8807 - accuracy: 0.02 - ETA: 1:14 - loss: 4.8786 - accuracy: 0.02 - ETA: 1:13 - loss: 4.8790 - accuracy: 0.02 - ETA: 1:11 - loss: 4.8777 - accuracy: 0.02 - ETA: 1:10 - loss: 4.8768 - accuracy: 0.02 - ETA: 1:08 - loss: 4.8760 - accuracy: 0.02 - ETA: 1:07 - loss: 4.8749 - accuracy: 0.02 - ETA: 1:06 - loss: 4.8741 - accuracy: 0.02 - ETA: 1:04 - loss: 4.8728 - accuracy: 0.02 - ETA: 1:03 - loss: 4.8732 - accuracy: 0.02 - ETA: 1:02 - loss: 4.8727 - accuracy: 0.02 - ETA: 1:01 - loss: 4.8720 - accuracy: 0.02 - ETA: 59s - loss: 4.8720 - accuracy: 0.0213 - ETA: 58s - loss: 4.8724 - accuracy: 0.021 - ETA: 56s - loss: 4.8723 - accuracy: 0.021 - ETA: 55s - loss: 4.8717 - accuracy: 0.021 - ETA: 53s - loss: 4.8710 - accuracy: 0.021 - ETA: 52s - loss: 4.8703 - accuracy: 0.021 - ETA: 50s - loss: 4.8689 - accuracy: 0.021 - ETA: 49s - loss: 4.8674 - accuracy: 0.021 - ETA: 47s - loss: 4.8666 - accuracy: 0.021 - ETA: 46s - loss: 4.8655 - accuracy: 0.021 - ETA: 44s - loss: 4.8653 - accuracy: 0.021 - ETA: 43s - loss: 4.8646 - accuracy: 0.021 - ETA: 41s - loss: 4.8629 - accuracy: 0.021 - ETA: 40s - loss: 4.8614 - accuracy: 0.021 - ETA: 38s - loss: 4.8602 - accuracy: 0.022 - ETA: 37s - loss: 4.8597 - accuracy: 0.022 - ETA: 35s - loss: 4.8586 - accuracy: 0.022 - ETA: 34s - loss: 4.8578 - accuracy: 0.022 - ETA: 32s - loss: 4.8568 - accuracy: 0.022 - ETA: 31s - loss: 4.8558 - accuracy: 0.022 - ETA: 29s - loss: 4.8548 - accuracy: 0.022 - ETA: 28s - loss: 4.8537 - accuracy: 0.022 - ETA: 26s - loss: 4.8525 - accuracy: 0.022 - ETA: 25s - loss: 4.8515 - accuracy: 0.022 - ETA: 23s - loss: 4.8503 - accuracy: 0.022 - ETA: 22s - loss: 4.8491 - accuracy: 0.022 - ETA: 21s - loss: 4.8489 - accuracy: 0.022 - ETA: 19s - loss: 4.8473 - accuracy: 0.022 - ETA: 18s - loss: 4.8465 - accuracy: 0.022 - ETA: 16s - loss: 4.8459 - accuracy: 0.022 - ETA: 15s - loss: 4.8449 - accuracy: 0.022 - ETA: 13s - loss: 4.8444 - accuracy: 0.022 - ETA: 12s - loss: 4.8437 - accuracy: 0.022 - ETA: 10s - loss: 4.8432 - accuracy: 0.022 - ETA: 9s - loss: 4.8424 - accuracy: 0.022 - ETA: 7s - loss: 4.8410 - accuracy: 0.02 - ETA: 6s - loss: 4.8410 - accuracy: 0.02 - ETA: 4s - loss: 4.8396 - accuracy: 0.02 - ETA: 3s - loss: 4.8383 - accuracy: 0.02 - ETA: 1s - loss: 4.8382 - accuracy: 0.02 - ETA: 0s - loss: 4.8375 - accuracy: 0.0228\n",
      "Epoch 00001: val_loss improved from inf to 4.51305, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "334/334 [==============================] - 580s 2s/step - loss: 4.8375 - accuracy: 0.0228 - val_loss: 4.5130 - val_accuracy: 0.0491\n",
      "Epoch 2/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "186/334 [===============>..............] - ETA: 0s - loss: 4.2558 - accuracy: 0.10 - ETA: 2:26 - loss: 4.3834 - accuracy: 0.05 - ETA: 3:14 - loss: 4.3846 - accuracy: 0.05 - ETA: 3:33 - loss: 4.4224 - accuracy: 0.03 - ETA: 3:46 - loss: 4.3398 - accuracy: 0.03 - ETA: 3:55 - loss: 4.3823 - accuracy: 0.03 - ETA: 4:11 - loss: 4.3461 - accuracy: 0.05 - ETA: 4:19 - loss: 4.4043 - accuracy: 0.05 - ETA: 4:23 - loss: 4.4170 - accuracy: 0.04 - ETA: 4:23 - loss: 4.4195 - accuracy: 0.04 - ETA: 4:23 - loss: 4.4236 - accuracy: 0.04 - ETA: 4:21 - loss: 4.4191 - accuracy: 0.03 - ETA: 4:19 - loss: 4.4208 - accuracy: 0.03 - ETA: 4:22 - loss: 4.4143 - accuracy: 0.03 - ETA: 4:25 - loss: 4.4148 - accuracy: 0.04 - ETA: 4:29 - loss: 4.4211 - accuracy: 0.04 - ETA: 4:28 - loss: 4.4276 - accuracy: 0.04 - ETA: 4:32 - loss: 4.4301 - accuracy: 0.04 - ETA: 4:33 - loss: 4.4331 - accuracy: 0.04 - ETA: 4:36 - loss: 4.4378 - accuracy: 0.04 - ETA: 4:38 - loss: 4.4408 - accuracy: 0.04 - ETA: 4:37 - loss: 4.4385 - accuracy: 0.04 - ETA: 4:36 - loss: 4.4405 - accuracy: 0.03 - ETA: 4:34 - loss: 4.4424 - accuracy: 0.03 - ETA: 4:33 - loss: 4.4427 - accuracy: 0.04 - ETA: 4:32 - loss: 4.4409 - accuracy: 0.04 - ETA: 4:31 - loss: 4.4459 - accuracy: 0.03 - ETA: 4:30 - loss: 4.4418 - accuracy: 0.03 - ETA: 4:29 - loss: 4.4457 - accuracy: 0.03 - ETA: 4:28 - loss: 4.4371 - accuracy: 0.04 - ETA: 4:26 - loss: 4.4349 - accuracy: 0.04 - ETA: 4:24 - loss: 4.4299 - accuracy: 0.04 - ETA: 4:22 - loss: 4.4212 - accuracy: 0.04 - ETA: 4:21 - loss: 4.4188 - accuracy: 0.04 - ETA: 4:20 - loss: 4.4093 - accuracy: 0.04 - ETA: 4:19 - loss: 4.4217 - accuracy: 0.04 - ETA: 4:17 - loss: 4.4229 - accuracy: 0.04 - ETA: 4:16 - loss: 4.4198 - accuracy: 0.04 - ETA: 4:15 - loss: 4.4182 - accuracy: 0.04 - ETA: 4:13 - loss: 4.4196 - accuracy: 0.04 - ETA: 4:12 - loss: 4.4192 - accuracy: 0.04 - ETA: 4:11 - loss: 4.4088 - accuracy: 0.04 - ETA: 4:09 - loss: 4.4134 - accuracy: 0.04 - ETA: 4:08 - loss: 4.4132 - accuracy: 0.04 - ETA: 4:06 - loss: 4.4110 - accuracy: 0.04 - ETA: 4:05 - loss: 4.4134 - accuracy: 0.04 - ETA: 4:04 - loss: 4.4221 - accuracy: 0.03 - ETA: 4:02 - loss: 4.4233 - accuracy: 0.03 - ETA: 4:02 - loss: 4.4204 - accuracy: 0.03 - ETA: 4:02 - loss: 4.4121 - accuracy: 0.04 - ETA: 4:01 - loss: 4.4071 - accuracy: 0.04 - ETA: 4:02 - loss: 4.4115 - accuracy: 0.04 - ETA: 4:02 - loss: 4.4070 - accuracy: 0.04 - ETA: 4:01 - loss: 4.4088 - accuracy: 0.04 - ETA: 4:00 - loss: 4.4076 - accuracy: 0.04 - ETA: 4:00 - loss: 4.4043 - accuracy: 0.04 - ETA: 3:59 - loss: 4.4053 - accuracy: 0.04 - ETA: 3:58 - loss: 4.4089 - accuracy: 0.04 - ETA: 3:56 - loss: 4.4057 - accuracy: 0.04 - ETA: 3:55 - loss: 4.4069 - accuracy: 0.04 - ETA: 3:54 - loss: 4.4052 - accuracy: 0.04 - ETA: 3:54 - loss: 4.4018 - accuracy: 0.04 - ETA: 3:55 - loss: 4.4025 - accuracy: 0.04 - ETA: 3:56 - loss: 4.4021 - accuracy: 0.04 - ETA: 3:55 - loss: 4.4046 - accuracy: 0.04 - ETA: 3:55 - loss: 4.4038 - accuracy: 0.04 - ETA: 3:54 - loss: 4.4044 - accuracy: 0.04 - ETA: 3:53 - loss: 4.4033 - accuracy: 0.04 - ETA: 3:53 - loss: 4.4008 - accuracy: 0.04 - ETA: 3:52 - loss: 4.4007 - accuracy: 0.04 - ETA: 3:51 - loss: 4.3985 - accuracy: 0.04 - ETA: 3:50 - loss: 4.3964 - accuracy: 0.04 - ETA: 3:50 - loss: 4.3991 - accuracy: 0.04 - ETA: 3:49 - loss: 4.4011 - accuracy: 0.04 - ETA: 3:48 - loss: 4.4037 - accuracy: 0.04 - ETA: 3:47 - loss: 4.4040 - accuracy: 0.04 - ETA: 3:46 - loss: 4.4043 - accuracy: 0.04 - ETA: 3:45 - loss: 4.4038 - accuracy: 0.04 - ETA: 3:44 - loss: 4.4057 - accuracy: 0.04 - ETA: 3:43 - loss: 4.4061 - accuracy: 0.04 - ETA: 3:41 - loss: 4.4009 - accuracy: 0.04 - ETA: 3:40 - loss: 4.3991 - accuracy: 0.04 - ETA: 3:39 - loss: 4.3987 - accuracy: 0.04 - ETA: 3:38 - loss: 4.3982 - accuracy: 0.04 - ETA: 3:37 - loss: 4.4009 - accuracy: 0.04 - ETA: 3:36 - loss: 4.4017 - accuracy: 0.04 - ETA: 3:35 - loss: 4.3948 - accuracy: 0.04 - ETA: 3:34 - loss: 4.3903 - accuracy: 0.04 - ETA: 3:33 - loss: 4.3888 - accuracy: 0.04 - ETA: 3:33 - loss: 4.3897 - accuracy: 0.04 - ETA: 3:32 - loss: 4.3954 - accuracy: 0.04 - ETA: 3:31 - loss: 4.3963 - accuracy: 0.04 - ETA: 3:30 - loss: 4.3926 - accuracy: 0.04 - ETA: 3:29 - loss: 4.3905 - accuracy: 0.04 - ETA: 3:28 - loss: 4.3887 - accuracy: 0.05 - ETA: 3:27 - loss: 4.3887 - accuracy: 0.04 - ETA: 3:26 - loss: 4.3923 - accuracy: 0.04 - ETA: 3:25 - loss: 4.3921 - accuracy: 0.04 - ETA: 3:24 - loss: 4.3899 - accuracy: 0.05 - ETA: 3:23 - loss: 4.3952 - accuracy: 0.05 - ETA: 3:22 - loss: 4.3934 - accuracy: 0.05 - ETA: 3:21 - loss: 4.3970 - accuracy: 0.05 - ETA: 3:20 - loss: 4.3979 - accuracy: 0.05 - ETA: 3:19 - loss: 4.3986 - accuracy: 0.04 - ETA: 3:18 - loss: 4.4003 - accuracy: 0.04 - ETA: 3:17 - loss: 4.3982 - accuracy: 0.05 - ETA: 3:16 - loss: 4.3998 - accuracy: 0.04 - ETA: 3:15 - loss: 4.4009 - accuracy: 0.05 - ETA: 3:14 - loss: 4.4025 - accuracy: 0.05 - ETA: 3:13 - loss: 4.4031 - accuracy: 0.05 - ETA: 3:12 - loss: 4.4004 - accuracy: 0.05 - ETA: 3:12 - loss: 4.3961 - accuracy: 0.05 - ETA: 3:11 - loss: 4.3965 - accuracy: 0.05 - ETA: 3:10 - loss: 4.3959 - accuracy: 0.05 - ETA: 3:09 - loss: 4.3973 - accuracy: 0.05 - ETA: 3:08 - loss: 4.3972 - accuracy: 0.05 - ETA: 3:07 - loss: 4.4000 - accuracy: 0.05 - ETA: 3:06 - loss: 4.3978 - accuracy: 0.05 - ETA: 3:05 - loss: 4.3993 - accuracy: 0.05 - ETA: 3:04 - loss: 4.3978 - accuracy: 0.05 - ETA: 3:03 - loss: 4.3980 - accuracy: 0.05 - ETA: 3:02 - loss: 4.4000 - accuracy: 0.05 - ETA: 3:01 - loss: 4.4007 - accuracy: 0.05 - ETA: 3:00 - loss: 4.4030 - accuracy: 0.05 - ETA: 3:00 - loss: 4.4027 - accuracy: 0.05 - ETA: 2:59 - loss: 4.4029 - accuracy: 0.05 - ETA: 2:58 - loss: 4.4015 - accuracy: 0.05 - ETA: 2:58 - loss: 4.4013 - accuracy: 0.05 - ETA: 2:57 - loss: 4.4030 - accuracy: 0.05 - ETA: 2:57 - loss: 4.4031 - accuracy: 0.05 - ETA: 2:57 - loss: 4.4042 - accuracy: 0.05 - ETA: 2:56 - loss: 4.4043 - accuracy: 0.05 - ETA: 2:55 - loss: 4.4022 - accuracy: 0.05 - ETA: 2:54 - loss: 4.3998 - accuracy: 0.05 - ETA: 2:53 - loss: 4.4022 - accuracy: 0.05 - ETA: 2:52 - loss: 4.4029 - accuracy: 0.05 - ETA: 2:51 - loss: 4.4028 - accuracy: 0.05 - ETA: 2:50 - loss: 4.4009 - accuracy: 0.05 - ETA: 2:49 - loss: 4.3974 - accuracy: 0.05 - ETA: 2:48 - loss: 4.4001 - accuracy: 0.05 - ETA: 2:47 - loss: 4.4005 - accuracy: 0.05 - ETA: 2:46 - loss: 4.4015 - accuracy: 0.05 - ETA: 2:45 - loss: 4.4008 - accuracy: 0.05 - ETA: 2:44 - loss: 4.4020 - accuracy: 0.05 - ETA: 2:43 - loss: 4.3994 - accuracy: 0.05 - ETA: 2:42 - loss: 4.3991 - accuracy: 0.05 - ETA: 2:41 - loss: 4.3980 - accuracy: 0.05 - ETA: 2:40 - loss: 4.3971 - accuracy: 0.05 - ETA: 2:40 - loss: 4.3966 - accuracy: 0.05 - ETA: 2:39 - loss: 4.3948 - accuracy: 0.05 - ETA: 2:38 - loss: 4.3945 - accuracy: 0.05 - ETA: 2:37 - loss: 4.3955 - accuracy: 0.05 - ETA: 2:37 - loss: 4.3961 - accuracy: 0.05 - ETA: 2:36 - loss: 4.3967 - accuracy: 0.05 - ETA: 2:35 - loss: 4.3960 - accuracy: 0.05 - ETA: 2:34 - loss: 4.3969 - accuracy: 0.05 - ETA: 2:33 - loss: 4.3972 - accuracy: 0.05 - ETA: 2:32 - loss: 4.3956 - accuracy: 0.05 - ETA: 2:31 - loss: 4.3955 - accuracy: 0.05 - ETA: 2:30 - loss: 4.3949 - accuracy: 0.05 - ETA: 2:29 - loss: 4.3949 - accuracy: 0.05 - ETA: 2:28 - loss: 4.3958 - accuracy: 0.05 - ETA: 2:27 - loss: 4.3961 - accuracy: 0.05 - ETA: 2:26 - loss: 4.3953 - accuracy: 0.05 - ETA: 2:25 - loss: 4.3962 - accuracy: 0.05 - ETA: 2:25 - loss: 4.3955 - accuracy: 0.05 - ETA: 2:24 - loss: 4.3956 - accuracy: 0.05 - ETA: 2:23 - loss: 4.3943 - accuracy: 0.05 - ETA: 2:22 - loss: 4.3921 - accuracy: 0.05 - ETA: 2:21 - loss: 4.3906 - accuracy: 0.05 - ETA: 2:20 - loss: 4.3887 - accuracy: 0.05 - ETA: 2:19 - loss: 4.3889 - accuracy: 0.05 - ETA: 2:18 - loss: 4.3886 - accuracy: 0.05 - ETA: 2:17 - loss: 4.3888 - accuracy: 0.05 - ETA: 2:17 - loss: 4.3882 - accuracy: 0.05 - ETA: 2:16 - loss: 4.3855 - accuracy: 0.05 - ETA: 2:15 - loss: 4.3838 - accuracy: 0.05 - ETA: 2:14 - loss: 4.3828 - accuracy: 0.05 - ETA: 2:13 - loss: 4.3829 - accuracy: 0.05 - ETA: 2:12 - loss: 4.3842 - accuracy: 0.05 - ETA: 2:12 - loss: 4.3840 - accuracy: 0.05 - ETA: 2:11 - loss: 4.3822 - accuracy: 0.05 - ETA: 2:10 - loss: 4.3827 - accuracy: 0.05 - ETA: 2:09 - loss: 4.3821 - accuracy: 0.05 - ETA: 2:08 - loss: 4.3816 - accuracy: 0.05 - ETA: 2:07 - loss: 4.3796 - accuracy: 0.0551"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "334/334 [==============================] - ETA: 2:06 - loss: 4.3803 - accuracy: 0.05 - ETA: 2:06 - loss: 4.3799 - accuracy: 0.05 - ETA: 2:05 - loss: 4.3791 - accuracy: 0.05 - ETA: 2:04 - loss: 4.3804 - accuracy: 0.05 - ETA: 2:03 - loss: 4.3801 - accuracy: 0.05 - ETA: 2:02 - loss: 4.3799 - accuracy: 0.05 - ETA: 2:02 - loss: 4.3790 - accuracy: 0.05 - ETA: 2:02 - loss: 4.3804 - accuracy: 0.05 - ETA: 2:01 - loss: 4.3790 - accuracy: 0.05 - ETA: 2:00 - loss: 4.3788 - accuracy: 0.05 - ETA: 1:59 - loss: 4.3775 - accuracy: 0.05 - ETA: 1:58 - loss: 4.3769 - accuracy: 0.05 - ETA: 1:58 - loss: 4.3793 - accuracy: 0.05 - ETA: 1:57 - loss: 4.3805 - accuracy: 0.05 - ETA: 1:56 - loss: 4.3804 - accuracy: 0.05 - ETA: 1:55 - loss: 4.3815 - accuracy: 0.05 - ETA: 1:54 - loss: 4.3810 - accuracy: 0.05 - ETA: 1:54 - loss: 4.3803 - accuracy: 0.05 - ETA: 1:53 - loss: 4.3793 - accuracy: 0.05 - ETA: 1:52 - loss: 4.3797 - accuracy: 0.05 - ETA: 1:51 - loss: 4.3800 - accuracy: 0.05 - ETA: 1:50 - loss: 4.3797 - accuracy: 0.05 - ETA: 1:50 - loss: 4.3793 - accuracy: 0.05 - ETA: 1:49 - loss: 4.3777 - accuracy: 0.05 - ETA: 1:48 - loss: 4.3759 - accuracy: 0.05 - ETA: 1:47 - loss: 4.3753 - accuracy: 0.05 - ETA: 1:46 - loss: 4.3725 - accuracy: 0.05 - ETA: 1:45 - loss: 4.3733 - accuracy: 0.05 - ETA: 1:44 - loss: 4.3751 - accuracy: 0.05 - ETA: 1:43 - loss: 4.3756 - accuracy: 0.05 - ETA: 1:43 - loss: 4.3745 - accuracy: 0.05 - ETA: 1:42 - loss: 4.3721 - accuracy: 0.05 - ETA: 1:41 - loss: 4.3722 - accuracy: 0.05 - ETA: 1:40 - loss: 4.3748 - accuracy: 0.05 - ETA: 1:39 - loss: 4.3750 - accuracy: 0.05 - ETA: 1:38 - loss: 4.3747 - accuracy: 0.05 - ETA: 1:38 - loss: 4.3731 - accuracy: 0.05 - ETA: 1:37 - loss: 4.3717 - accuracy: 0.05 - ETA: 1:36 - loss: 4.3704 - accuracy: 0.05 - ETA: 1:35 - loss: 4.3693 - accuracy: 0.05 - ETA: 1:34 - loss: 4.3704 - accuracy: 0.05 - ETA: 1:33 - loss: 4.3691 - accuracy: 0.05 - ETA: 1:32 - loss: 4.3713 - accuracy: 0.05 - ETA: 1:31 - loss: 4.3715 - accuracy: 0.05 - ETA: 1:31 - loss: 4.3715 - accuracy: 0.05 - ETA: 1:30 - loss: 4.3722 - accuracy: 0.05 - ETA: 1:29 - loss: 4.3714 - accuracy: 0.05 - ETA: 1:28 - loss: 4.3716 - accuracy: 0.05 - ETA: 1:27 - loss: 4.3716 - accuracy: 0.05 - ETA: 1:26 - loss: 4.3708 - accuracy: 0.05 - ETA: 1:25 - loss: 4.3699 - accuracy: 0.05 - ETA: 1:24 - loss: 4.3688 - accuracy: 0.05 - ETA: 1:24 - loss: 4.3678 - accuracy: 0.05 - ETA: 1:23 - loss: 4.3695 - accuracy: 0.05 - ETA: 1:22 - loss: 4.3690 - accuracy: 0.05 - ETA: 1:21 - loss: 4.3672 - accuracy: 0.05 - ETA: 1:20 - loss: 4.3670 - accuracy: 0.05 - ETA: 1:20 - loss: 4.3666 - accuracy: 0.05 - ETA: 1:19 - loss: 4.3660 - accuracy: 0.05 - ETA: 1:18 - loss: 4.3661 - accuracy: 0.05 - ETA: 1:17 - loss: 4.3659 - accuracy: 0.05 - ETA: 1:16 - loss: 4.3666 - accuracy: 0.05 - ETA: 1:15 - loss: 4.3656 - accuracy: 0.05 - ETA: 1:15 - loss: 4.3650 - accuracy: 0.05 - ETA: 1:14 - loss: 4.3650 - accuracy: 0.05 - ETA: 1:13 - loss: 4.3657 - accuracy: 0.05 - ETA: 1:12 - loss: 4.3659 - accuracy: 0.05 - ETA: 1:11 - loss: 4.3652 - accuracy: 0.05 - ETA: 1:10 - loss: 4.3652 - accuracy: 0.05 - ETA: 1:10 - loss: 4.3651 - accuracy: 0.05 - ETA: 1:09 - loss: 4.3645 - accuracy: 0.05 - ETA: 1:08 - loss: 4.3634 - accuracy: 0.05 - ETA: 1:07 - loss: 4.3622 - accuracy: 0.05 - ETA: 1:07 - loss: 4.3616 - accuracy: 0.05 - ETA: 1:06 - loss: 4.3621 - accuracy: 0.05 - ETA: 1:05 - loss: 4.3606 - accuracy: 0.05 - ETA: 1:04 - loss: 4.3612 - accuracy: 0.05 - ETA: 1:04 - loss: 4.3612 - accuracy: 0.05 - ETA: 1:03 - loss: 4.3602 - accuracy: 0.05 - ETA: 1:02 - loss: 4.3606 - accuracy: 0.05 - ETA: 1:01 - loss: 4.3616 - accuracy: 0.05 - ETA: 1:00 - loss: 4.3624 - accuracy: 0.05 - ETA: 1:00 - loss: 4.3611 - accuracy: 0.05 - ETA: 59s - loss: 4.3608 - accuracy: 0.0550 - ETA: 58s - loss: 4.3613 - accuracy: 0.055 - ETA: 57s - loss: 4.3608 - accuracy: 0.055 - ETA: 56s - loss: 4.3592 - accuracy: 0.055 - ETA: 56s - loss: 4.3594 - accuracy: 0.055 - ETA: 55s - loss: 4.3596 - accuracy: 0.055 - ETA: 54s - loss: 4.3596 - accuracy: 0.054 - ETA: 53s - loss: 4.3597 - accuracy: 0.054 - ETA: 52s - loss: 4.3590 - accuracy: 0.055 - ETA: 51s - loss: 4.3585 - accuracy: 0.055 - ETA: 50s - loss: 4.3577 - accuracy: 0.055 - ETA: 49s - loss: 4.3572 - accuracy: 0.055 - ETA: 49s - loss: 4.3565 - accuracy: 0.055 - ETA: 48s - loss: 4.3560 - accuracy: 0.055 - ETA: 47s - loss: 4.3547 - accuracy: 0.055 - ETA: 46s - loss: 4.3546 - accuracy: 0.055 - ETA: 45s - loss: 4.3564 - accuracy: 0.055 - ETA: 44s - loss: 4.3554 - accuracy: 0.055 - ETA: 43s - loss: 4.3559 - accuracy: 0.055 - ETA: 42s - loss: 4.3553 - accuracy: 0.055 - ETA: 42s - loss: 4.3541 - accuracy: 0.055 - ETA: 41s - loss: 4.3546 - accuracy: 0.055 - ETA: 40s - loss: 4.3543 - accuracy: 0.055 - ETA: 39s - loss: 4.3531 - accuracy: 0.055 - ETA: 38s - loss: 4.3525 - accuracy: 0.055 - ETA: 37s - loss: 4.3528 - accuracy: 0.055 - ETA: 36s - loss: 4.3520 - accuracy: 0.055 - ETA: 35s - loss: 4.3511 - accuracy: 0.055 - ETA: 34s - loss: 4.3498 - accuracy: 0.055 - ETA: 33s - loss: 4.3504 - accuracy: 0.055 - ETA: 32s - loss: 4.3494 - accuracy: 0.055 - ETA: 31s - loss: 4.3487 - accuracy: 0.055 - ETA: 31s - loss: 4.3474 - accuracy: 0.055 - ETA: 30s - loss: 4.3491 - accuracy: 0.055 - ETA: 29s - loss: 4.3493 - accuracy: 0.055 - ETA: 28s - loss: 4.3482 - accuracy: 0.055 - ETA: 27s - loss: 4.3476 - accuracy: 0.055 - ETA: 26s - loss: 4.3473 - accuracy: 0.055 - ETA: 25s - loss: 4.3466 - accuracy: 0.055 - ETA: 24s - loss: 4.3461 - accuracy: 0.056 - ETA: 23s - loss: 4.3468 - accuracy: 0.056 - ETA: 22s - loss: 4.3455 - accuracy: 0.056 - ETA: 21s - loss: 4.3459 - accuracy: 0.056 - ETA: 20s - loss: 4.3448 - accuracy: 0.056 - ETA: 19s - loss: 4.3433 - accuracy: 0.056 - ETA: 18s - loss: 4.3441 - accuracy: 0.056 - ETA: 17s - loss: 4.3442 - accuracy: 0.056 - ETA: 16s - loss: 4.3447 - accuracy: 0.056 - ETA: 15s - loss: 4.3441 - accuracy: 0.056 - ETA: 14s - loss: 4.3440 - accuracy: 0.056 - ETA: 13s - loss: 4.3428 - accuracy: 0.057 - ETA: 12s - loss: 4.3420 - accuracy: 0.057 - ETA: 11s - loss: 4.3420 - accuracy: 0.057 - ETA: 10s - loss: 4.3420 - accuracy: 0.057 - ETA: 9s - loss: 4.3422 - accuracy: 0.057 - ETA: 8s - loss: 4.3432 - accuracy: 0.05 - ETA: 7s - loss: 4.3422 - accuracy: 0.05 - ETA: 7s - loss: 4.3432 - accuracy: 0.05 - ETA: 6s - loss: 4.3422 - accuracy: 0.05 - ETA: 5s - loss: 4.3428 - accuracy: 0.05 - ETA: 4s - loss: 4.3429 - accuracy: 0.05 - ETA: 3s - loss: 4.3419 - accuracy: 0.05 - ETA: 2s - loss: 4.3411 - accuracy: 0.05 - ETA: 1s - loss: 4.3406 - accuracy: 0.05 - ETA: 0s - loss: 4.3406 - accuracy: 0.0585\n",
      "Epoch 00002: val_loss improved from 4.51305 to 4.25277, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "334/334 [==============================] - 353s 1s/step - loss: 4.3406 - accuracy: 0.0585 - val_loss: 4.2528 - val_accuracy: 0.0659\n",
      "Epoch 3/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "186/334 [===============>..............] - ETA: 0s - loss: 3.8881 - accuracy: 0.15 - ETA: 2:12 - loss: 3.8268 - accuracy: 0.12 - ETA: 2:57 - loss: 3.8883 - accuracy: 0.13 - ETA: 3:16 - loss: 3.8616 - accuracy: 0.15 - ETA: 3:33 - loss: 3.8526 - accuracy: 0.14 - ETA: 3:45 - loss: 3.8638 - accuracy: 0.15 - ETA: 3:51 - loss: 3.9017 - accuracy: 0.14 - ETA: 3:54 - loss: 3.9136 - accuracy: 0.13 - ETA: 3:55 - loss: 3.9041 - accuracy: 0.12 - ETA: 3:56 - loss: 3.9080 - accuracy: 0.12 - ETA: 3:56 - loss: 3.9014 - accuracy: 0.11 - ETA: 3:56 - loss: 3.8862 - accuracy: 0.12 - ETA: 3:56 - loss: 3.8627 - accuracy: 0.13 - ETA: 3:58 - loss: 3.8635 - accuracy: 0.12 - ETA: 3:59 - loss: 3.8815 - accuracy: 0.12 - ETA: 3:58 - loss: 3.8671 - accuracy: 0.11 - ETA: 3:58 - loss: 3.8697 - accuracy: 0.12 - ETA: 3:58 - loss: 3.8639 - accuracy: 0.11 - ETA: 3:59 - loss: 3.8967 - accuracy: 0.11 - ETA: 4:01 - loss: 3.9045 - accuracy: 0.11 - ETA: 4:02 - loss: 3.9110 - accuracy: 0.11 - ETA: 4:03 - loss: 3.8892 - accuracy: 0.12 - ETA: 4:03 - loss: 3.9097 - accuracy: 0.11 - ETA: 4:02 - loss: 3.9120 - accuracy: 0.12 - ETA: 4:01 - loss: 3.9174 - accuracy: 0.12 - ETA: 3:59 - loss: 3.8999 - accuracy: 0.12 - ETA: 3:58 - loss: 3.8787 - accuracy: 0.12 - ETA: 3:57 - loss: 3.8912 - accuracy: 0.12 - ETA: 3:57 - loss: 3.9039 - accuracy: 0.12 - ETA: 3:57 - loss: 3.9035 - accuracy: 0.11 - ETA: 3:56 - loss: 3.8932 - accuracy: 0.11 - ETA: 3:55 - loss: 3.9017 - accuracy: 0.12 - ETA: 3:54 - loss: 3.8994 - accuracy: 0.12 - ETA: 3:53 - loss: 3.8975 - accuracy: 0.12 - ETA: 3:52 - loss: 3.9070 - accuracy: 0.12 - ETA: 3:51 - loss: 3.9038 - accuracy: 0.12 - ETA: 3:50 - loss: 3.9137 - accuracy: 0.12 - ETA: 3:50 - loss: 3.9124 - accuracy: 0.11 - ETA: 3:50 - loss: 3.9113 - accuracy: 0.12 - ETA: 3:49 - loss: 3.9185 - accuracy: 0.11 - ETA: 3:49 - loss: 3.9095 - accuracy: 0.12 - ETA: 3:48 - loss: 3.9015 - accuracy: 0.12 - ETA: 3:47 - loss: 3.8962 - accuracy: 0.12 - ETA: 3:46 - loss: 3.9013 - accuracy: 0.12 - ETA: 3:45 - loss: 3.8937 - accuracy: 0.12 - ETA: 3:44 - loss: 3.8961 - accuracy: 0.12 - ETA: 3:44 - loss: 3.8937 - accuracy: 0.12 - ETA: 3:43 - loss: 3.8982 - accuracy: 0.12 - ETA: 3:42 - loss: 3.9034 - accuracy: 0.12 - ETA: 3:41 - loss: 3.9038 - accuracy: 0.12 - ETA: 3:40 - loss: 3.9064 - accuracy: 0.12 - ETA: 3:39 - loss: 3.9084 - accuracy: 0.12 - ETA: 3:38 - loss: 3.9114 - accuracy: 0.12 - ETA: 3:38 - loss: 3.9082 - accuracy: 0.12 - ETA: 3:37 - loss: 3.9001 - accuracy: 0.12 - ETA: 3:36 - loss: 3.8950 - accuracy: 0.12 - ETA: 3:36 - loss: 3.8990 - accuracy: 0.12 - ETA: 3:35 - loss: 3.8926 - accuracy: 0.12 - ETA: 3:34 - loss: 3.8944 - accuracy: 0.12 - ETA: 3:34 - loss: 3.8970 - accuracy: 0.12 - ETA: 3:33 - loss: 3.8986 - accuracy: 0.12 - ETA: 3:32 - loss: 3.8954 - accuracy: 0.12 - ETA: 3:32 - loss: 3.8970 - accuracy: 0.12 - ETA: 3:31 - loss: 3.9100 - accuracy: 0.12 - ETA: 3:30 - loss: 3.9136 - accuracy: 0.12 - ETA: 3:29 - loss: 3.9167 - accuracy: 0.12 - ETA: 3:28 - loss: 3.9199 - accuracy: 0.12 - ETA: 3:27 - loss: 3.9186 - accuracy: 0.12 - ETA: 3:26 - loss: 3.9145 - accuracy: 0.12 - ETA: 3:26 - loss: 3.9096 - accuracy: 0.12 - ETA: 3:25 - loss: 3.8993 - accuracy: 0.12 - ETA: 3:24 - loss: 3.8973 - accuracy: 0.12 - ETA: 3:23 - loss: 3.8944 - accuracy: 0.12 - ETA: 3:22 - loss: 3.8983 - accuracy: 0.12 - ETA: 3:22 - loss: 3.9003 - accuracy: 0.12 - ETA: 3:21 - loss: 3.8937 - accuracy: 0.12 - ETA: 3:20 - loss: 3.8923 - accuracy: 0.12 - ETA: 3:19 - loss: 3.8898 - accuracy: 0.12 - ETA: 3:19 - loss: 3.8932 - accuracy: 0.12 - ETA: 3:18 - loss: 3.8967 - accuracy: 0.12 - ETA: 3:18 - loss: 3.8960 - accuracy: 0.12 - ETA: 3:17 - loss: 3.8986 - accuracy: 0.12 - ETA: 3:16 - loss: 3.8976 - accuracy: 0.11 - ETA: 3:15 - loss: 3.8986 - accuracy: 0.11 - ETA: 3:15 - loss: 3.8921 - accuracy: 0.12 - ETA: 3:14 - loss: 3.8972 - accuracy: 0.12 - ETA: 3:13 - loss: 3.9005 - accuracy: 0.12 - ETA: 3:12 - loss: 3.9011 - accuracy: 0.12 - ETA: 3:12 - loss: 3.9005 - accuracy: 0.12 - ETA: 3:11 - loss: 3.8971 - accuracy: 0.12 - ETA: 3:10 - loss: 3.8999 - accuracy: 0.12 - ETA: 3:09 - loss: 3.9000 - accuracy: 0.12 - ETA: 3:08 - loss: 3.9014 - accuracy: 0.12 - ETA: 3:08 - loss: 3.9042 - accuracy: 0.11 - ETA: 3:07 - loss: 3.9038 - accuracy: 0.12 - ETA: 3:07 - loss: 3.9028 - accuracy: 0.12 - ETA: 3:06 - loss: 3.9003 - accuracy: 0.12 - ETA: 3:05 - loss: 3.9027 - accuracy: 0.12 - ETA: 3:04 - loss: 3.9023 - accuracy: 0.12 - ETA: 3:04 - loss: 3.9027 - accuracy: 0.11 - ETA: 3:03 - loss: 3.9036 - accuracy: 0.11 - ETA: 3:03 - loss: 3.9093 - accuracy: 0.11 - ETA: 3:02 - loss: 3.9124 - accuracy: 0.11 - ETA: 3:01 - loss: 3.9110 - accuracy: 0.11 - ETA: 3:00 - loss: 3.9095 - accuracy: 0.11 - ETA: 2:59 - loss: 3.9133 - accuracy: 0.11 - ETA: 2:58 - loss: 3.9147 - accuracy: 0.11 - ETA: 2:57 - loss: 3.9174 - accuracy: 0.11 - ETA: 2:57 - loss: 3.9161 - accuracy: 0.11 - ETA: 2:56 - loss: 3.9181 - accuracy: 0.11 - ETA: 2:55 - loss: 3.9177 - accuracy: 0.11 - ETA: 2:54 - loss: 3.9152 - accuracy: 0.11 - ETA: 2:53 - loss: 3.9097 - accuracy: 0.11 - ETA: 2:52 - loss: 3.9118 - accuracy: 0.11 - ETA: 2:52 - loss: 3.9129 - accuracy: 0.11 - ETA: 2:51 - loss: 3.9131 - accuracy: 0.11 - ETA: 2:50 - loss: 3.9143 - accuracy: 0.11 - ETA: 2:50 - loss: 3.9161 - accuracy: 0.11 - ETA: 2:49 - loss: 3.9168 - accuracy: 0.11 - ETA: 2:48 - loss: 3.9167 - accuracy: 0.11 - ETA: 2:48 - loss: 3.9152 - accuracy: 0.11 - ETA: 2:47 - loss: 3.9158 - accuracy: 0.11 - ETA: 2:46 - loss: 3.9135 - accuracy: 0.11 - ETA: 2:45 - loss: 3.9120 - accuracy: 0.11 - ETA: 2:45 - loss: 3.9135 - accuracy: 0.11 - ETA: 2:44 - loss: 3.9123 - accuracy: 0.11 - ETA: 2:43 - loss: 3.9111 - accuracy: 0.11 - ETA: 2:42 - loss: 3.9152 - accuracy: 0.11 - ETA: 2:41 - loss: 3.9160 - accuracy: 0.11 - ETA: 2:40 - loss: 3.9148 - accuracy: 0.11 - ETA: 2:40 - loss: 3.9135 - accuracy: 0.11 - ETA: 2:39 - loss: 3.9143 - accuracy: 0.11 - ETA: 2:38 - loss: 3.9147 - accuracy: 0.11 - ETA: 2:37 - loss: 3.9180 - accuracy: 0.11 - ETA: 2:36 - loss: 3.9153 - accuracy: 0.11 - ETA: 2:35 - loss: 3.9129 - accuracy: 0.11 - ETA: 2:35 - loss: 3.9109 - accuracy: 0.11 - ETA: 2:34 - loss: 3.9091 - accuracy: 0.11 - ETA: 2:33 - loss: 3.9081 - accuracy: 0.11 - ETA: 2:33 - loss: 3.9116 - accuracy: 0.11 - ETA: 2:32 - loss: 3.9114 - accuracy: 0.11 - ETA: 2:31 - loss: 3.9096 - accuracy: 0.11 - ETA: 2:30 - loss: 3.9100 - accuracy: 0.11 - ETA: 2:30 - loss: 3.9086 - accuracy: 0.11 - ETA: 2:29 - loss: 3.9118 - accuracy: 0.11 - ETA: 2:28 - loss: 3.9146 - accuracy: 0.11 - ETA: 2:27 - loss: 3.9122 - accuracy: 0.12 - ETA: 2:27 - loss: 3.9090 - accuracy: 0.12 - ETA: 2:26 - loss: 3.9104 - accuracy: 0.11 - ETA: 2:25 - loss: 3.9099 - accuracy: 0.11 - ETA: 2:24 - loss: 3.9081 - accuracy: 0.11 - ETA: 2:24 - loss: 3.9091 - accuracy: 0.12 - ETA: 2:23 - loss: 3.9099 - accuracy: 0.11 - ETA: 2:22 - loss: 3.9063 - accuracy: 0.11 - ETA: 2:21 - loss: 3.9068 - accuracy: 0.11 - ETA: 2:21 - loss: 3.9042 - accuracy: 0.11 - ETA: 2:20 - loss: 3.9057 - accuracy: 0.11 - ETA: 2:19 - loss: 3.9051 - accuracy: 0.11 - ETA: 2:18 - loss: 3.9039 - accuracy: 0.11 - ETA: 2:18 - loss: 3.9034 - accuracy: 0.11 - ETA: 2:17 - loss: 3.9039 - accuracy: 0.11 - ETA: 2:16 - loss: 3.9012 - accuracy: 0.11 - ETA: 2:15 - loss: 3.8988 - accuracy: 0.11 - ETA: 2:14 - loss: 3.8981 - accuracy: 0.11 - ETA: 2:14 - loss: 3.8956 - accuracy: 0.11 - ETA: 2:13 - loss: 3.8992 - accuracy: 0.11 - ETA: 2:12 - loss: 3.8978 - accuracy: 0.11 - ETA: 2:12 - loss: 3.8975 - accuracy: 0.11 - ETA: 2:11 - loss: 3.8946 - accuracy: 0.12 - ETA: 2:10 - loss: 3.8965 - accuracy: 0.11 - ETA: 2:09 - loss: 3.8933 - accuracy: 0.12 - ETA: 2:09 - loss: 3.8929 - accuracy: 0.12 - ETA: 2:08 - loss: 3.8930 - accuracy: 0.12 - ETA: 2:07 - loss: 3.8933 - accuracy: 0.11 - ETA: 2:07 - loss: 3.8938 - accuracy: 0.12 - ETA: 2:06 - loss: 3.8908 - accuracy: 0.12 - ETA: 2:05 - loss: 3.8859 - accuracy: 0.12 - ETA: 2:05 - loss: 3.8860 - accuracy: 0.12 - ETA: 2:04 - loss: 3.8852 - accuracy: 0.12 - ETA: 2:03 - loss: 3.8818 - accuracy: 0.12 - ETA: 2:02 - loss: 3.8849 - accuracy: 0.12 - ETA: 2:01 - loss: 3.8865 - accuracy: 0.11 - ETA: 2:01 - loss: 3.8872 - accuracy: 0.11 - ETA: 2:00 - loss: 3.8870 - accuracy: 0.11 - ETA: 1:59 - loss: 3.8867 - accuracy: 0.11 - ETA: 1:58 - loss: 3.8867 - accuracy: 0.1196"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "309/334 [==========================>...] - ETA: 1:57 - loss: 3.8841 - accuracy: 0.12 - ETA: 1:57 - loss: 3.8847 - accuracy: 0.11 - ETA: 1:56 - loss: 3.8848 - accuracy: 0.11 - ETA: 1:55 - loss: 3.8859 - accuracy: 0.11 - ETA: 1:54 - loss: 3.8893 - accuracy: 0.11 - ETA: 1:54 - loss: 3.8908 - accuracy: 0.11 - ETA: 1:53 - loss: 3.8903 - accuracy: 0.11 - ETA: 1:52 - loss: 3.8900 - accuracy: 0.11 - ETA: 1:51 - loss: 3.8890 - accuracy: 0.11 - ETA: 1:50 - loss: 3.8877 - accuracy: 0.11 - ETA: 1:50 - loss: 3.8832 - accuracy: 0.11 - ETA: 1:49 - loss: 3.8848 - accuracy: 0.11 - ETA: 1:48 - loss: 3.8855 - accuracy: 0.11 - ETA: 1:47 - loss: 3.8827 - accuracy: 0.12 - ETA: 1:46 - loss: 3.8812 - accuracy: 0.12 - ETA: 1:46 - loss: 3.8798 - accuracy: 0.12 - ETA: 1:45 - loss: 3.8836 - accuracy: 0.12 - ETA: 1:44 - loss: 3.8852 - accuracy: 0.12 - ETA: 1:44 - loss: 3.8834 - accuracy: 0.12 - ETA: 1:43 - loss: 3.8847 - accuracy: 0.12 - ETA: 1:42 - loss: 3.8850 - accuracy: 0.12 - ETA: 1:41 - loss: 3.8841 - accuracy: 0.12 - ETA: 1:41 - loss: 3.8828 - accuracy: 0.12 - ETA: 1:40 - loss: 3.8812 - accuracy: 0.12 - ETA: 1:39 - loss: 3.8803 - accuracy: 0.12 - ETA: 1:38 - loss: 3.8813 - accuracy: 0.12 - ETA: 1:37 - loss: 3.8806 - accuracy: 0.12 - ETA: 1:37 - loss: 3.8792 - accuracy: 0.12 - ETA: 1:36 - loss: 3.8786 - accuracy: 0.12 - ETA: 1:35 - loss: 3.8784 - accuracy: 0.12 - ETA: 1:34 - loss: 3.8771 - accuracy: 0.12 - ETA: 1:34 - loss: 3.8756 - accuracy: 0.12 - ETA: 1:33 - loss: 3.8762 - accuracy: 0.12 - ETA: 1:32 - loss: 3.8765 - accuracy: 0.12 - ETA: 1:31 - loss: 3.8758 - accuracy: 0.12 - ETA: 1:30 - loss: 3.8754 - accuracy: 0.12 - ETA: 1:30 - loss: 3.8741 - accuracy: 0.12 - ETA: 1:29 - loss: 3.8742 - accuracy: 0.12 - ETA: 1:28 - loss: 3.8740 - accuracy: 0.12 - ETA: 1:27 - loss: 3.8750 - accuracy: 0.12 - ETA: 1:26 - loss: 3.8739 - accuracy: 0.12 - ETA: 1:26 - loss: 3.8728 - accuracy: 0.12 - ETA: 1:25 - loss: 3.8714 - accuracy: 0.12 - ETA: 1:25 - loss: 3.8699 - accuracy: 0.12 - ETA: 1:25 - loss: 3.8692 - accuracy: 0.12 - ETA: 1:24 - loss: 3.8681 - accuracy: 0.12 - ETA: 1:23 - loss: 3.8696 - accuracy: 0.12 - ETA: 1:22 - loss: 3.8696 - accuracy: 0.12 - ETA: 1:21 - loss: 3.8707 - accuracy: 0.12 - ETA: 1:20 - loss: 3.8696 - accuracy: 0.12 - ETA: 1:20 - loss: 3.8705 - accuracy: 0.12 - ETA: 1:19 - loss: 3.8702 - accuracy: 0.12 - ETA: 1:18 - loss: 3.8720 - accuracy: 0.12 - ETA: 1:17 - loss: 3.8718 - accuracy: 0.12 - ETA: 1:16 - loss: 3.8719 - accuracy: 0.12 - ETA: 1:16 - loss: 3.8714 - accuracy: 0.12 - ETA: 1:15 - loss: 3.8723 - accuracy: 0.12 - ETA: 1:14 - loss: 3.8731 - accuracy: 0.12 - ETA: 1:13 - loss: 3.8737 - accuracy: 0.12 - ETA: 1:13 - loss: 3.8735 - accuracy: 0.12 - ETA: 1:12 - loss: 3.8757 - accuracy: 0.12 - ETA: 1:11 - loss: 3.8765 - accuracy: 0.12 - ETA: 1:10 - loss: 3.8784 - accuracy: 0.12 - ETA: 1:09 - loss: 3.8781 - accuracy: 0.12 - ETA: 1:09 - loss: 3.8775 - accuracy: 0.12 - ETA: 1:08 - loss: 3.8765 - accuracy: 0.12 - ETA: 1:07 - loss: 3.8756 - accuracy: 0.12 - ETA: 1:06 - loss: 3.8747 - accuracy: 0.12 - ETA: 1:05 - loss: 3.8765 - accuracy: 0.12 - ETA: 1:04 - loss: 3.8776 - accuracy: 0.12 - ETA: 1:04 - loss: 3.8769 - accuracy: 0.12 - ETA: 1:03 - loss: 3.8790 - accuracy: 0.12 - ETA: 1:02 - loss: 3.8789 - accuracy: 0.12 - ETA: 1:01 - loss: 3.8789 - accuracy: 0.12 - ETA: 1:00 - loss: 3.8796 - accuracy: 0.12 - ETA: 59s - loss: 3.8778 - accuracy: 0.1229 - ETA: 59s - loss: 3.8782 - accuracy: 0.122 - ETA: 58s - loss: 3.8764 - accuracy: 0.123 - ETA: 57s - loss: 3.8769 - accuracy: 0.123 - ETA: 56s - loss: 3.8758 - accuracy: 0.124 - ETA: 56s - loss: 3.8748 - accuracy: 0.124 - ETA: 55s - loss: 3.8739 - accuracy: 0.124 - ETA: 54s - loss: 3.8759 - accuracy: 0.124 - ETA: 53s - loss: 3.8753 - accuracy: 0.124 - ETA: 52s - loss: 3.8764 - accuracy: 0.124 - ETA: 51s - loss: 3.8748 - accuracy: 0.124 - ETA: 51s - loss: 3.8749 - accuracy: 0.124 - ETA: 50s - loss: 3.8740 - accuracy: 0.124 - ETA: 49s - loss: 3.8741 - accuracy: 0.124 - ETA: 48s - loss: 3.8735 - accuracy: 0.124 - ETA: 48s - loss: 3.8723 - accuracy: 0.124 - ETA: 47s - loss: 3.8724 - accuracy: 0.124 - ETA: 46s - loss: 3.8732 - accuracy: 0.124 - ETA: 45s - loss: 3.8719 - accuracy: 0.124 - ETA: 45s - loss: 3.8726 - accuracy: 0.124 - ETA: 44s - loss: 3.8736 - accuracy: 0.124 - ETA: 43s - loss: 3.8724 - accuracy: 0.124 - ETA: 42s - loss: 3.8718 - accuracy: 0.124 - ETA: 41s - loss: 3.8716 - accuracy: 0.124 - ETA: 41s - loss: 3.8715 - accuracy: 0.124 - ETA: 40s - loss: 3.8728 - accuracy: 0.124 - ETA: 39s - loss: 3.8733 - accuracy: 0.124 - ETA: 38s - loss: 3.8722 - accuracy: 0.124 - ETA: 37s - loss: 3.8721 - accuracy: 0.124 - ETA: 36s - loss: 3.8712 - accuracy: 0.124 - ETA: 36s - loss: 3.8719 - accuracy: 0.124 - ETA: 35s - loss: 3.8688 - accuracy: 0.124 - ETA: 34s - loss: 3.8675 - accuracy: 0.124 - ETA: 33s - loss: 3.8678 - accuracy: 0.124 - ETA: 32s - loss: 3.8669 - accuracy: 0.125 - ETA: 31s - loss: 3.8660 - accuracy: 0.124 - ETA: 31s - loss: 3.8651 - accuracy: 0.125 - ETA: 30s - loss: 3.8648 - accuracy: 0.124 - ETA: 29s - loss: 3.8641 - accuracy: 0.125 - ETA: 28s - loss: 3.8628 - accuracy: 0.125 - ETA: 27s - loss: 3.8631 - accuracy: 0.124 - ETA: 26s - loss: 3.8602 - accuracy: 0.125 - ETA: 26s - loss: 3.8568 - accuracy: 0.125 - ETA: 25s - loss: 3.8571 - accuracy: 0.125 - ETA: 24s - loss: 3.8574 - accuracy: 0.125 - ETA: 23s - loss: 3.8588 - accuracy: 0.125 - ETA: 22s - loss: 3.8563 - accuracy: 0.126 - ETA: 21s - loss: 3.8546 - accuracy: 0.1264"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-e66eab63f73d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m model.fit(train_tensors, train_targets, \n\u001b[0;32m      5\u001b[0m           \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalid_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_targets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m           epochs=20, batch_size=20, callbacks=[checkpointer], verbose=1)\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     64\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m    846\u001b[0m                 batch_size=batch_size):\n\u001b[0;32m    847\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 848\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    849\u001b[0m               \u001b[1;31m# Catch OutOfRangeError for Datasets of unknown size.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    850\u001b[0m               \u001b[1;31m# This blocks until the batch has finished executing.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    579\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 580\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    581\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    582\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    609\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    610\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 611\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    612\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    613\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2418\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2419\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2420\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2421\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2422\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   1663\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[0;32m   1664\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[1;32m-> 1665\u001b[1;33m         self.captured_inputs)\n\u001b[0m\u001b[0;32m   1666\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1667\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1744\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1745\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1746\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1748\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    596\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    597\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 598\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    599\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    600\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[1;32m---> 60\u001b[1;33m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "checkpointer = ModelCheckpoint(filepath='saved_models/weights.best.from_scratch.hdf5', \n",
    "                               verbose=1, save_best_only=True)\n",
    "\n",
    "model.fit(train_tensors, train_targets, \n",
    "          validation_data=(valid_tensors, valid_targets),\n",
    "          epochs=20, batch_size=20, callbacks=[checkpointer], verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('saved_models/weights.best.from_scratch.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 6.2201%\n"
     ]
    }
   ],
   "source": [
    "dog_breed_predictions = [np.argmax(model.predict(np.expand_dims(tensor, axis=0))) for tensor in test_tensors]\n",
    "\n",
    "# report test accuracy\n",
    "test_accuracy = 100*np.sum(np.array(dog_breed_predictions)==np.argmax(test_targets, axis=1))/len(dog_breed_predictions)\n",
    "print('Test accuracy: %.4f%%' % test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test accuracy: 6.2201%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain bottleneck features from another pre-trained CNN(Xception)\n",
    "\n",
    "bottleneck_features = np.load('bottleneck_features/DogXceptionData.npz')\n",
    "train_Xception = bottleneck_features['train']\n",
    "valid_Xception = bottleneck_features['valid']\n",
    "test_Xception = bottleneck_features['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.        , 0.        , 0.        , ..., 0.        ,\n",
       "         0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "         0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "         0.        , 0.        ],\n",
       "        ...,\n",
       "        [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "         0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "         0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "         0.        , 0.        ]],\n",
       "\n",
       "       [[0.        , 0.        , 0.        , ..., 0.        ,\n",
       "         0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "         0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "         0.        , 0.        ],\n",
       "        ...,\n",
       "        [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "         0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "         0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "         0.        , 0.        ]],\n",
       "\n",
       "       [[0.        , 0.        , 0.        , ..., 0.        ,\n",
       "         0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "         0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "         0.        , 0.        ],\n",
       "        ...,\n",
       "        [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "         0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "         0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "         0.        , 0.        ]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0.        , 0.        , 0.        , ..., 0.        ,\n",
       "         0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "         0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "         0.        , 0.        ],\n",
       "        ...,\n",
       "        [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "         0.        , 0.        ],\n",
       "        [0.        , 0.10807449, 0.2267896 , ..., 0.        ,\n",
       "         0.        , 0.        ],\n",
       "        [0.        , 0.00524163, 0.01071388, ..., 0.        ,\n",
       "         0.        , 0.        ]],\n",
       "\n",
       "       [[0.        , 0.        , 0.        , ..., 0.        ,\n",
       "         0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "         0.        , 0.2070558 ],\n",
       "        [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "         0.        , 0.        ],\n",
       "        ...,\n",
       "        [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "         0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "         0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "         0.        , 0.        ]],\n",
       "\n",
       "       [[0.        , 0.        , 0.        , ..., 0.        ,\n",
       "         0.        , 0.27110964],\n",
       "        [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "         0.        , 0.52149194],\n",
       "        [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "         0.        , 0.16974062],\n",
       "        ...,\n",
       "        [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "         0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "         0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "         0.        , 0.        ]]], dtype=float32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_Xception[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "global_average_pooling2d (Gl (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 500)               1024500   \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 133)               66633     \n",
      "=================================================================\n",
      "Total params: 1,091,133\n",
      "Trainable params: 1,091,133\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "Xception_model = Sequential()\n",
    "Xception_model.add(GlobalAveragePooling2D(input_shape=train_Xception.shape[1:]))\n",
    "Xception_model.add(Dense(500, activation='relu'))\n",
    "Xception_model.add(Dropout(0.4))\n",
    "Xception_model.add(Dense(133, activation='softmax'))\n",
    "\n",
    "Xception_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compile the model.\n",
    "Xception_model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "334/334 [==============================] - ETA: 0s - loss: 4.8967 - accuracy: 0.0000e+ - ETA: 20s - loss: 5.0299 - accuracy: 0.0250   - ETA: 15s - loss: 5.0546 - accuracy: 0.037 - ETA: 16s - loss: 5.0037 - accuracy: 0.060 - ETA: 14s - loss: 4.7685 - accuracy: 0.085 - ETA: 13s - loss: 4.7863 - accuracy: 0.100 - ETA: 12s - loss: 4.7155 - accuracy: 0.118 - ETA: 12s - loss: 4.6343 - accuracy: 0.134 - ETA: 11s - loss: 4.5878 - accuracy: 0.140 - ETA: 11s - loss: 4.4927 - accuracy: 0.150 - ETA: 11s - loss: 4.4058 - accuracy: 0.160 - ETA: 10s - loss: 4.2909 - accuracy: 0.181 - ETA: 10s - loss: 4.2251 - accuracy: 0.193 - ETA: 10s - loss: 4.1586 - accuracy: 0.204 - ETA: 10s - loss: 4.1012 - accuracy: 0.209 - ETA: 10s - loss: 4.0687 - accuracy: 0.213 - ETA: 9s - loss: 3.9863 - accuracy: 0.232 - ETA: 9s - loss: 3.9405 - accuracy: 0.23 - ETA: 9s - loss: 3.8689 - accuracy: 0.25 - ETA: 9s - loss: 3.8127 - accuracy: 0.26 - ETA: 9s - loss: 3.7402 - accuracy: 0.27 - ETA: 9s - loss: 3.7118 - accuracy: 0.28 - ETA: 9s - loss: 3.6623 - accuracy: 0.29 - ETA: 9s - loss: 3.6149 - accuracy: 0.30 - ETA: 9s - loss: 3.5496 - accuracy: 0.31 - ETA: 9s - loss: 3.4806 - accuracy: 0.32 - ETA: 9s - loss: 3.4228 - accuracy: 0.33 - ETA: 9s - loss: 3.3711 - accuracy: 0.34 - ETA: 8s - loss: 3.3180 - accuracy: 0.35 - ETA: 8s - loss: 3.2717 - accuracy: 0.35 - ETA: 8s - loss: 3.2336 - accuracy: 0.36 - ETA: 8s - loss: 3.1823 - accuracy: 0.37 - ETA: 8s - loss: 3.1493 - accuracy: 0.37 - ETA: 8s - loss: 3.0882 - accuracy: 0.39 - ETA: 8s - loss: 3.0373 - accuracy: 0.40 - ETA: 8s - loss: 2.9835 - accuracy: 0.40 - ETA: 8s - loss: 2.9580 - accuracy: 0.41 - ETA: 8s - loss: 2.9137 - accuracy: 0.41 - ETA: 8s - loss: 2.8718 - accuracy: 0.42 - ETA: 8s - loss: 2.8324 - accuracy: 0.43 - ETA: 8s - loss: 2.7978 - accuracy: 0.43 - ETA: 8s - loss: 2.7732 - accuracy: 0.44 - ETA: 7s - loss: 2.7299 - accuracy: 0.44 - ETA: 7s - loss: 2.7063 - accuracy: 0.45 - ETA: 7s - loss: 2.6764 - accuracy: 0.45 - ETA: 7s - loss: 2.6444 - accuracy: 0.46 - ETA: 7s - loss: 2.6202 - accuracy: 0.46 - ETA: 7s - loss: 2.5905 - accuracy: 0.47 - ETA: 7s - loss: 2.5656 - accuracy: 0.47 - ETA: 7s - loss: 2.5329 - accuracy: 0.48 - ETA: 7s - loss: 2.5065 - accuracy: 0.48 - ETA: 7s - loss: 2.4910 - accuracy: 0.48 - ETA: 7s - loss: 2.4637 - accuracy: 0.48 - ETA: 7s - loss: 2.4355 - accuracy: 0.49 - ETA: 7s - loss: 2.4139 - accuracy: 0.49 - ETA: 7s - loss: 2.3929 - accuracy: 0.49 - ETA: 6s - loss: 2.3736 - accuracy: 0.50 - ETA: 6s - loss: 2.3564 - accuracy: 0.50 - ETA: 6s - loss: 2.3359 - accuracy: 0.50 - ETA: 6s - loss: 2.3143 - accuracy: 0.51 - ETA: 6s - loss: 2.2938 - accuracy: 0.51 - ETA: 6s - loss: 2.2791 - accuracy: 0.51 - ETA: 6s - loss: 2.2590 - accuracy: 0.52 - ETA: 6s - loss: 2.2380 - accuracy: 0.52 - ETA: 6s - loss: 2.2270 - accuracy: 0.52 - ETA: 6s - loss: 2.2088 - accuracy: 0.52 - ETA: 6s - loss: 2.1931 - accuracy: 0.52 - ETA: 6s - loss: 2.1771 - accuracy: 0.53 - ETA: 6s - loss: 2.1573 - accuracy: 0.53 - ETA: 6s - loss: 2.1395 - accuracy: 0.53 - ETA: 6s - loss: 2.1190 - accuracy: 0.54 - ETA: 6s - loss: 2.1121 - accuracy: 0.54 - ETA: 5s - loss: 2.0963 - accuracy: 0.54 - ETA: 5s - loss: 2.0856 - accuracy: 0.54 - ETA: 5s - loss: 2.0784 - accuracy: 0.54 - ETA: 5s - loss: 2.0627 - accuracy: 0.54 - ETA: 5s - loss: 2.0532 - accuracy: 0.55 - ETA: 5s - loss: 2.0396 - accuracy: 0.55 - ETA: 5s - loss: 2.0233 - accuracy: 0.55 - ETA: 5s - loss: 2.0062 - accuracy: 0.55 - ETA: 5s - loss: 1.9935 - accuracy: 0.56 - ETA: 5s - loss: 1.9814 - accuracy: 0.56 - ETA: 5s - loss: 1.9682 - accuracy: 0.56 - ETA: 5s - loss: 1.9672 - accuracy: 0.56 - ETA: 5s - loss: 1.9509 - accuracy: 0.56 - ETA: 5s - loss: 1.9350 - accuracy: 0.56 - ETA: 5s - loss: 1.9223 - accuracy: 0.57 - ETA: 5s - loss: 1.9140 - accuracy: 0.57 - ETA: 5s - loss: 1.9048 - accuracy: 0.57 - ETA: 5s - loss: 1.8895 - accuracy: 0.57 - ETA: 4s - loss: 1.8742 - accuracy: 0.57 - ETA: 4s - loss: 1.8639 - accuracy: 0.58 - ETA: 4s - loss: 1.8501 - accuracy: 0.58 - ETA: 4s - loss: 1.8376 - accuracy: 0.58 - ETA: 4s - loss: 1.8266 - accuracy: 0.58 - ETA: 4s - loss: 1.8176 - accuracy: 0.59 - ETA: 4s - loss: 1.8079 - accuracy: 0.59 - ETA: 4s - loss: 1.8038 - accuracy: 0.59 - ETA: 4s - loss: 1.7983 - accuracy: 0.59 - ETA: 4s - loss: 1.7914 - accuracy: 0.59 - ETA: 4s - loss: 1.7841 - accuracy: 0.59 - ETA: 4s - loss: 1.7776 - accuracy: 0.59 - ETA: 4s - loss: 1.7705 - accuracy: 0.59 - ETA: 4s - loss: 1.7626 - accuracy: 0.59 - ETA: 4s - loss: 1.7525 - accuracy: 0.59 - ETA: 4s - loss: 1.7436 - accuracy: 0.60 - ETA: 4s - loss: 1.7373 - accuracy: 0.60 - ETA: 3s - loss: 1.7275 - accuracy: 0.60 - ETA: 3s - loss: 1.7188 - accuracy: 0.60 - ETA: 3s - loss: 1.7089 - accuracy: 0.60 - ETA: 3s - loss: 1.7051 - accuracy: 0.60 - ETA: 3s - loss: 1.6982 - accuracy: 0.60 - ETA: 3s - loss: 1.6938 - accuracy: 0.60 - ETA: 3s - loss: 1.6868 - accuracy: 0.60 - ETA: 3s - loss: 1.6857 - accuracy: 0.60 - ETA: 3s - loss: 1.6794 - accuracy: 0.60 - ETA: 3s - loss: 1.6715 - accuracy: 0.61 - ETA: 3s - loss: 1.6644 - accuracy: 0.61 - ETA: 3s - loss: 1.6508 - accuracy: 0.61 - ETA: 3s - loss: 1.6433 - accuracy: 0.61 - ETA: 3s - loss: 1.6376 - accuracy: 0.61 - ETA: 2s - loss: 1.6301 - accuracy: 0.61 - ETA: 2s - loss: 1.6238 - accuracy: 0.61 - ETA: 2s - loss: 1.6198 - accuracy: 0.61 - ETA: 2s - loss: 1.6127 - accuracy: 0.62 - ETA: 2s - loss: 1.6082 - accuracy: 0.62 - ETA: 2s - loss: 1.6033 - accuracy: 0.62 - ETA: 2s - loss: 1.6003 - accuracy: 0.62 - ETA: 2s - loss: 1.5941 - accuracy: 0.62 - ETA: 2s - loss: 1.5895 - accuracy: 0.62 - ETA: 2s - loss: 1.5868 - accuracy: 0.62 - ETA: 2s - loss: 1.5818 - accuracy: 0.62 - ETA: 2s - loss: 1.5754 - accuracy: 0.62 - ETA: 2s - loss: 1.5737 - accuracy: 0.62 - ETA: 2s - loss: 1.5681 - accuracy: 0.62 - ETA: 2s - loss: 1.5632 - accuracy: 0.62 - ETA: 1s - loss: 1.5556 - accuracy: 0.62 - ETA: 1s - loss: 1.5506 - accuracy: 0.62 - ETA: 1s - loss: 1.5437 - accuracy: 0.63 - ETA: 1s - loss: 1.5391 - accuracy: 0.63 - ETA: 1s - loss: 1.5350 - accuracy: 0.63 - ETA: 1s - loss: 1.5295 - accuracy: 0.63 - ETA: 1s - loss: 1.5250 - accuracy: 0.63 - ETA: 1s - loss: 1.5223 - accuracy: 0.63 - ETA: 1s - loss: 1.5173 - accuracy: 0.63 - ETA: 1s - loss: 1.5110 - accuracy: 0.63 - ETA: 1s - loss: 1.5056 - accuracy: 0.63 - ETA: 1s - loss: 1.5044 - accuracy: 0.63 - ETA: 1s - loss: 1.4983 - accuracy: 0.63 - ETA: 1s - loss: 1.4941 - accuracy: 0.63 - ETA: 1s - loss: 1.4906 - accuracy: 0.63 - ETA: 1s - loss: 1.4848 - accuracy: 0.64 - ETA: 0s - loss: 1.4794 - accuracy: 0.64 - ETA: 0s - loss: 1.4763 - accuracy: 0.64 - ETA: 0s - loss: 1.4729 - accuracy: 0.64 - ETA: 0s - loss: 1.4678 - accuracy: 0.64 - ETA: 0s - loss: 1.4648 - accuracy: 0.64 - ETA: 0s - loss: 1.4583 - accuracy: 0.64 - ETA: 0s - loss: 1.4562 - accuracy: 0.64 - ETA: 0s - loss: 1.4534 - accuracy: 0.64 - ETA: 0s - loss: 1.4499 - accuracy: 0.64 - ETA: 0s - loss: 1.4441 - accuracy: 0.64 - ETA: 0s - loss: 1.4411 - accuracy: 0.64 - ETA: 0s - loss: 1.4373 - accuracy: 0.64 - ETA: 0s - loss: 1.4352 - accuracy: 0.64 - ETA: 0s - loss: 1.4294 - accuracy: 0.64 - ETA: 0s - loss: 1.4262 - accuracy: 0.65 - ETA: 0s - loss: 1.4230 - accuracy: 0.65 - ETA: 0s - loss: 1.4187 - accuracy: 0.6512\n",
      "Epoch 00001: val_loss improved from inf to 0.62877, saving model to saved_models/weights.best.Xception.hdf5\n",
      "334/334 [==============================] - 17s 51ms/step - loss: 1.4187 - accuracy: 0.6512 - val_loss: 0.6288 - val_accuracy: 0.7916\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "333/334 [============================>.] - ETA: 0s - loss: 0.5220 - accuracy: 0.75 - ETA: 5s - loss: 0.6223 - accuracy: 0.78 - ETA: 7s - loss: 0.6615 - accuracy: 0.78 - ETA: 8s - loss: 0.7544 - accuracy: 0.74 - ETA: 9s - loss: 0.7555 - accuracy: 0.75 - ETA: 9s - loss: 0.7104 - accuracy: 0.76 - ETA: 9s - loss: 0.7507 - accuracy: 0.76 - ETA: 9s - loss: 0.7422 - accuracy: 0.78 - ETA: 9s - loss: 0.7160 - accuracy: 0.79 - ETA: 9s - loss: 0.7306 - accuracy: 0.79 - ETA: 9s - loss: 0.6821 - accuracy: 0.80 - ETA: 9s - loss: 0.7112 - accuracy: 0.80 - ETA: 9s - loss: 0.7445 - accuracy: 0.78 - ETA: 9s - loss: 0.7373 - accuracy: 0.78 - ETA: 9s - loss: 0.7331 - accuracy: 0.78 - ETA: 9s - loss: 0.7161 - accuracy: 0.78 - ETA: 9s - loss: 0.6939 - accuracy: 0.79 - ETA: 9s - loss: 0.7106 - accuracy: 0.78 - ETA: 9s - loss: 0.7195 - accuracy: 0.78 - ETA: 9s - loss: 0.7427 - accuracy: 0.77 - ETA: 9s - loss: 0.7268 - accuracy: 0.78 - ETA: 9s - loss: 0.7441 - accuracy: 0.77 - ETA: 9s - loss: 0.7429 - accuracy: 0.77 - ETA: 9s - loss: 0.7395 - accuracy: 0.77 - ETA: 9s - loss: 0.7309 - accuracy: 0.77 - ETA: 9s - loss: 0.7135 - accuracy: 0.78 - ETA: 9s - loss: 0.7034 - accuracy: 0.78 - ETA: 9s - loss: 0.6977 - accuracy: 0.78 - ETA: 8s - loss: 0.6907 - accuracy: 0.79 - ETA: 8s - loss: 0.6897 - accuracy: 0.79 - ETA: 8s - loss: 0.6886 - accuracy: 0.79 - ETA: 8s - loss: 0.6856 - accuracy: 0.79 - ETA: 8s - loss: 0.6989 - accuracy: 0.79 - ETA: 8s - loss: 0.7008 - accuracy: 0.78 - ETA: 8s - loss: 0.6998 - accuracy: 0.79 - ETA: 8s - loss: 0.6991 - accuracy: 0.79 - ETA: 8s - loss: 0.6930 - accuracy: 0.79 - ETA: 8s - loss: 0.6919 - accuracy: 0.79 - ETA: 8s - loss: 0.6982 - accuracy: 0.78 - ETA: 8s - loss: 0.7012 - accuracy: 0.78 - ETA: 8s - loss: 0.6951 - accuracy: 0.78 - ETA: 8s - loss: 0.6919 - accuracy: 0.78 - ETA: 8s - loss: 0.6891 - accuracy: 0.78 - ETA: 7s - loss: 0.6928 - accuracy: 0.79 - ETA: 7s - loss: 0.6918 - accuracy: 0.79 - ETA: 7s - loss: 0.6957 - accuracy: 0.79 - ETA: 7s - loss: 0.6923 - accuracy: 0.79 - ETA: 7s - loss: 0.6975 - accuracy: 0.79 - ETA: 7s - loss: 0.6992 - accuracy: 0.78 - ETA: 7s - loss: 0.6975 - accuracy: 0.78 - ETA: 7s - loss: 0.6948 - accuracy: 0.78 - ETA: 7s - loss: 0.6914 - accuracy: 0.79 - ETA: 7s - loss: 0.6920 - accuracy: 0.78 - ETA: 7s - loss: 0.6874 - accuracy: 0.79 - ETA: 7s - loss: 0.6890 - accuracy: 0.79 - ETA: 7s - loss: 0.6901 - accuracy: 0.79 - ETA: 7s - loss: 0.6827 - accuracy: 0.79 - ETA: 7s - loss: 0.6798 - accuracy: 0.79 - ETA: 7s - loss: 0.6810 - accuracy: 0.79 - ETA: 6s - loss: 0.6855 - accuracy: 0.79 - ETA: 6s - loss: 0.6859 - accuracy: 0.79 - ETA: 6s - loss: 0.6839 - accuracy: 0.79 - ETA: 6s - loss: 0.6852 - accuracy: 0.79 - ETA: 6s - loss: 0.6825 - accuracy: 0.79 - ETA: 6s - loss: 0.6840 - accuracy: 0.79 - ETA: 6s - loss: 0.6801 - accuracy: 0.79 - ETA: 6s - loss: 0.6782 - accuracy: 0.79 - ETA: 6s - loss: 0.6798 - accuracy: 0.79 - ETA: 6s - loss: 0.6817 - accuracy: 0.79 - ETA: 6s - loss: 0.6794 - accuracy: 0.79 - ETA: 6s - loss: 0.6805 - accuracy: 0.79 - ETA: 6s - loss: 0.6791 - accuracy: 0.79 - ETA: 6s - loss: 0.6790 - accuracy: 0.79 - ETA: 5s - loss: 0.6788 - accuracy: 0.79 - ETA: 5s - loss: 0.6764 - accuracy: 0.79 - ETA: 5s - loss: 0.6701 - accuracy: 0.79 - ETA: 5s - loss: 0.6708 - accuracy: 0.79 - ETA: 5s - loss: 0.6669 - accuracy: 0.79 - ETA: 5s - loss: 0.6667 - accuracy: 0.79 - ETA: 5s - loss: 0.6638 - accuracy: 0.79 - ETA: 5s - loss: 0.6692 - accuracy: 0.79 - ETA: 5s - loss: 0.6651 - accuracy: 0.79 - ETA: 5s - loss: 0.6655 - accuracy: 0.79 - ETA: 5s - loss: 0.6642 - accuracy: 0.80 - ETA: 5s - loss: 0.6638 - accuracy: 0.80 - ETA: 5s - loss: 0.6621 - accuracy: 0.80 - ETA: 5s - loss: 0.6625 - accuracy: 0.80 - ETA: 5s - loss: 0.6609 - accuracy: 0.80 - ETA: 4s - loss: 0.6614 - accuracy: 0.80 - ETA: 4s - loss: 0.6621 - accuracy: 0.80 - ETA: 4s - loss: 0.6607 - accuracy: 0.80 - ETA: 4s - loss: 0.6608 - accuracy: 0.80 - ETA: 4s - loss: 0.6584 - accuracy: 0.80 - ETA: 4s - loss: 0.6618 - accuracy: 0.80 - ETA: 4s - loss: 0.6652 - accuracy: 0.79 - ETA: 4s - loss: 0.6676 - accuracy: 0.79 - ETA: 4s - loss: 0.6681 - accuracy: 0.80 - ETA: 4s - loss: 0.6661 - accuracy: 0.80 - ETA: 4s - loss: 0.6643 - accuracy: 0.80 - ETA: 4s - loss: 0.6645 - accuracy: 0.80 - ETA: 4s - loss: 0.6657 - accuracy: 0.80 - ETA: 4s - loss: 0.6629 - accuracy: 0.80 - ETA: 4s - loss: 0.6646 - accuracy: 0.80 - ETA: 4s - loss: 0.6624 - accuracy: 0.80 - ETA: 3s - loss: 0.6590 - accuracy: 0.80 - ETA: 3s - loss: 0.6580 - accuracy: 0.80 - ETA: 3s - loss: 0.6592 - accuracy: 0.80 - ETA: 3s - loss: 0.6611 - accuracy: 0.80 - ETA: 3s - loss: 0.6601 - accuracy: 0.80 - ETA: 3s - loss: 0.6600 - accuracy: 0.80 - ETA: 3s - loss: 0.6573 - accuracy: 0.80 - ETA: 3s - loss: 0.6560 - accuracy: 0.80 - ETA: 3s - loss: 0.6554 - accuracy: 0.80 - ETA: 3s - loss: 0.6551 - accuracy: 0.80 - ETA: 3s - loss: 0.6558 - accuracy: 0.80 - ETA: 3s - loss: 0.6536 - accuracy: 0.80 - ETA: 3s - loss: 0.6548 - accuracy: 0.80 - ETA: 3s - loss: 0.6559 - accuracy: 0.80 - ETA: 3s - loss: 0.6554 - accuracy: 0.80 - ETA: 3s - loss: 0.6577 - accuracy: 0.80 - ETA: 3s - loss: 0.6587 - accuracy: 0.80 - ETA: 2s - loss: 0.6643 - accuracy: 0.79 - ETA: 2s - loss: 0.6641 - accuracy: 0.79 - ETA: 2s - loss: 0.6621 - accuracy: 0.79 - ETA: 2s - loss: 0.6629 - accuracy: 0.79 - ETA: 2s - loss: 0.6639 - accuracy: 0.79 - ETA: 2s - loss: 0.6643 - accuracy: 0.79 - ETA: 2s - loss: 0.6645 - accuracy: 0.79 - ETA: 2s - loss: 0.6630 - accuracy: 0.79 - ETA: 2s - loss: 0.6612 - accuracy: 0.79 - ETA: 2s - loss: 0.6602 - accuracy: 0.80 - ETA: 2s - loss: 0.6626 - accuracy: 0.79 - ETA: 2s - loss: 0.6601 - accuracy: 0.80 - ETA: 2s - loss: 0.6605 - accuracy: 0.80 - ETA: 2s - loss: 0.6601 - accuracy: 0.80 - ETA: 2s - loss: 0.6588 - accuracy: 0.80 - ETA: 2s - loss: 0.6614 - accuracy: 0.80 - ETA: 1s - loss: 0.6608 - accuracy: 0.80 - ETA: 1s - loss: 0.6615 - accuracy: 0.80 - ETA: 1s - loss: 0.6611 - accuracy: 0.80 - ETA: 1s - loss: 0.6609 - accuracy: 0.80 - ETA: 1s - loss: 0.6596 - accuracy: 0.80 - ETA: 1s - loss: 0.6566 - accuracy: 0.80 - ETA: 1s - loss: 0.6554 - accuracy: 0.80 - ETA: 1s - loss: 0.6544 - accuracy: 0.80 - ETA: 1s - loss: 0.6576 - accuracy: 0.80 - ETA: 1s - loss: 0.6578 - accuracy: 0.80 - ETA: 1s - loss: 0.6571 - accuracy: 0.80 - ETA: 1s - loss: 0.6573 - accuracy: 0.80 - ETA: 1s - loss: 0.6542 - accuracy: 0.80 - ETA: 1s - loss: 0.6554 - accuracy: 0.80 - ETA: 1s - loss: 0.6561 - accuracy: 0.80 - ETA: 0s - loss: 0.6555 - accuracy: 0.80 - ETA: 0s - loss: 0.6563 - accuracy: 0.80 - ETA: 0s - loss: 0.6563 - accuracy: 0.80 - ETA: 0s - loss: 0.6587 - accuracy: 0.80 - ETA: 0s - loss: 0.6572 - accuracy: 0.80 - ETA: 0s - loss: 0.6591 - accuracy: 0.80 - ETA: 0s - loss: 0.6586 - accuracy: 0.80 - ETA: 0s - loss: 0.6585 - accuracy: 0.80 - ETA: 0s - loss: 0.6575 - accuracy: 0.80 - ETA: 0s - loss: 0.6573 - accuracy: 0.80 - ETA: 0s - loss: 0.6566 - accuracy: 0.80 - ETA: 0s - loss: 0.6543 - accuracy: 0.80 - ETA: 0s - loss: 0.6552 - accuracy: 0.80 - ETA: 0s - loss: 0.6548 - accuracy: 0.80 - ETA: 0s - loss: 0.6550 - accuracy: 0.80 - ETA: 0s - loss: 0.6561 - accuracy: 0.8029\n",
      "Epoch 00002: val_loss improved from 0.62877 to 0.58779, saving model to saved_models/weights.best.Xception.hdf5\n",
      "334/334 [==============================] - 12s 37ms/step - loss: 0.6559 - accuracy: 0.8028 - val_loss: 0.5878 - val_accuracy: 0.8240\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "334/334 [==============================] - ETA: 0s - loss: 0.5120 - accuracy: 0.85 - ETA: 5s - loss: 0.5044 - accuracy: 0.83 - ETA: 7s - loss: 0.4088 - accuracy: 0.87 - ETA: 8s - loss: 0.4521 - accuracy: 0.84 - ETA: 8s - loss: 0.4220 - accuracy: 0.85 - ETA: 8s - loss: 0.4364 - accuracy: 0.84 - ETA: 8s - loss: 0.4525 - accuracy: 0.84 - ETA: 8s - loss: 0.4296 - accuracy: 0.85 - ETA: 8s - loss: 0.4237 - accuracy: 0.85 - ETA: 8s - loss: 0.4339 - accuracy: 0.85 - ETA: 8s - loss: 0.4616 - accuracy: 0.85 - ETA: 8s - loss: 0.4555 - accuracy: 0.84 - ETA: 8s - loss: 0.4668 - accuracy: 0.84 - ETA: 8s - loss: 0.4434 - accuracy: 0.85 - ETA: 8s - loss: 0.4472 - accuracy: 0.85 - ETA: 8s - loss: 0.4397 - accuracy: 0.85 - ETA: 8s - loss: 0.4286 - accuracy: 0.85 - ETA: 8s - loss: 0.4296 - accuracy: 0.85 - ETA: 7s - loss: 0.4428 - accuracy: 0.85 - ETA: 7s - loss: 0.4451 - accuracy: 0.85 - ETA: 7s - loss: 0.4460 - accuracy: 0.85 - ETA: 7s - loss: 0.4588 - accuracy: 0.84 - ETA: 7s - loss: 0.4727 - accuracy: 0.84 - ETA: 7s - loss: 0.4751 - accuracy: 0.84 - ETA: 7s - loss: 0.4750 - accuracy: 0.84 - ETA: 8s - loss: 0.4700 - accuracy: 0.84 - ETA: 8s - loss: 0.4706 - accuracy: 0.84 - ETA: 8s - loss: 0.4799 - accuracy: 0.84 - ETA: 8s - loss: 0.4787 - accuracy: 0.84 - ETA: 8s - loss: 0.4787 - accuracy: 0.84 - ETA: 8s - loss: 0.4813 - accuracy: 0.84 - ETA: 7s - loss: 0.4787 - accuracy: 0.84 - ETA: 7s - loss: 0.4728 - accuracy: 0.84 - ETA: 7s - loss: 0.4722 - accuracy: 0.84 - ETA: 7s - loss: 0.4729 - accuracy: 0.84 - ETA: 7s - loss: 0.4645 - accuracy: 0.84 - ETA: 7s - loss: 0.4643 - accuracy: 0.84 - ETA: 7s - loss: 0.4668 - accuracy: 0.84 - ETA: 7s - loss: 0.4699 - accuracy: 0.84 - ETA: 7s - loss: 0.4713 - accuracy: 0.84 - ETA: 7s - loss: 0.4933 - accuracy: 0.83 - ETA: 7s - loss: 0.4930 - accuracy: 0.83 - ETA: 7s - loss: 0.4942 - accuracy: 0.83 - ETA: 7s - loss: 0.4872 - accuracy: 0.84 - ETA: 7s - loss: 0.4824 - accuracy: 0.84 - ETA: 7s - loss: 0.4871 - accuracy: 0.84 - ETA: 7s - loss: 0.4943 - accuracy: 0.84 - ETA: 7s - loss: 0.4969 - accuracy: 0.84 - ETA: 7s - loss: 0.4923 - accuracy: 0.84 - ETA: 7s - loss: 0.4970 - accuracy: 0.84 - ETA: 7s - loss: 0.4953 - accuracy: 0.84 - ETA: 7s - loss: 0.4891 - accuracy: 0.84 - ETA: 7s - loss: 0.4899 - accuracy: 0.84 - ETA: 6s - loss: 0.4876 - accuracy: 0.84 - ETA: 6s - loss: 0.4900 - accuracy: 0.84 - ETA: 6s - loss: 0.4982 - accuracy: 0.83 - ETA: 6s - loss: 0.5022 - accuracy: 0.83 - ETA: 6s - loss: 0.5015 - accuracy: 0.83 - ETA: 6s - loss: 0.4956 - accuracy: 0.84 - ETA: 6s - loss: 0.4952 - accuracy: 0.84 - ETA: 6s - loss: 0.4903 - accuracy: 0.84 - ETA: 6s - loss: 0.4914 - accuracy: 0.84 - ETA: 6s - loss: 0.4909 - accuracy: 0.84 - ETA: 6s - loss: 0.4870 - accuracy: 0.84 - ETA: 6s - loss: 0.4894 - accuracy: 0.84 - ETA: 6s - loss: 0.4896 - accuracy: 0.84 - ETA: 6s - loss: 0.4901 - accuracy: 0.84 - ETA: 5s - loss: 0.4897 - accuracy: 0.84 - ETA: 5s - loss: 0.4873 - accuracy: 0.84 - ETA: 5s - loss: 0.4844 - accuracy: 0.84 - ETA: 5s - loss: 0.4849 - accuracy: 0.84 - ETA: 5s - loss: 0.4821 - accuracy: 0.84 - ETA: 5s - loss: 0.4841 - accuracy: 0.84 - ETA: 5s - loss: 0.4859 - accuracy: 0.84 - ETA: 5s - loss: 0.4879 - accuracy: 0.84 - ETA: 5s - loss: 0.4870 - accuracy: 0.84 - ETA: 5s - loss: 0.4889 - accuracy: 0.84 - ETA: 5s - loss: 0.4920 - accuracy: 0.84 - ETA: 5s - loss: 0.4950 - accuracy: 0.84 - ETA: 5s - loss: 0.4959 - accuracy: 0.84 - ETA: 5s - loss: 0.4960 - accuracy: 0.84 - ETA: 5s - loss: 0.4942 - accuracy: 0.84 - ETA: 4s - loss: 0.5009 - accuracy: 0.84 - ETA: 4s - loss: 0.5055 - accuracy: 0.83 - ETA: 4s - loss: 0.5074 - accuracy: 0.83 - ETA: 4s - loss: 0.5093 - accuracy: 0.83 - ETA: 4s - loss: 0.5087 - accuracy: 0.83 - ETA: 4s - loss: 0.5066 - accuracy: 0.83 - ETA: 4s - loss: 0.5101 - accuracy: 0.83 - ETA: 4s - loss: 0.5078 - accuracy: 0.83 - ETA: 4s - loss: 0.5120 - accuracy: 0.83 - ETA: 4s - loss: 0.5146 - accuracy: 0.83 - ETA: 4s - loss: 0.5109 - accuracy: 0.83 - ETA: 4s - loss: 0.5091 - accuracy: 0.83 - ETA: 4s - loss: 0.5088 - accuracy: 0.83 - ETA: 4s - loss: 0.5095 - accuracy: 0.83 - ETA: 4s - loss: 0.5132 - accuracy: 0.83 - ETA: 4s - loss: 0.5142 - accuracy: 0.83 - ETA: 3s - loss: 0.5117 - accuracy: 0.84 - ETA: 3s - loss: 0.5097 - accuracy: 0.84 - ETA: 3s - loss: 0.5106 - accuracy: 0.84 - ETA: 3s - loss: 0.5117 - accuracy: 0.84 - ETA: 3s - loss: 0.5105 - accuracy: 0.84 - ETA: 3s - loss: 0.5100 - accuracy: 0.84 - ETA: 3s - loss: 0.5136 - accuracy: 0.84 - ETA: 3s - loss: 0.5178 - accuracy: 0.83 - ETA: 3s - loss: 0.5177 - accuracy: 0.83 - ETA: 3s - loss: 0.5176 - accuracy: 0.83 - ETA: 3s - loss: 0.5178 - accuracy: 0.83 - ETA: 3s - loss: 0.5169 - accuracy: 0.83 - ETA: 3s - loss: 0.5168 - accuracy: 0.83 - ETA: 3s - loss: 0.5168 - accuracy: 0.83 - ETA: 3s - loss: 0.5164 - accuracy: 0.83 - ETA: 3s - loss: 0.5150 - accuracy: 0.83 - ETA: 2s - loss: 0.5145 - accuracy: 0.84 - ETA: 2s - loss: 0.5145 - accuracy: 0.84 - ETA: 2s - loss: 0.5212 - accuracy: 0.83 - ETA: 2s - loss: 0.5190 - accuracy: 0.83 - ETA: 2s - loss: 0.5190 - accuracy: 0.83 - ETA: 2s - loss: 0.5200 - accuracy: 0.83 - ETA: 2s - loss: 0.5204 - accuracy: 0.83 - ETA: 2s - loss: 0.5191 - accuracy: 0.83 - ETA: 2s - loss: 0.5163 - accuracy: 0.84 - ETA: 2s - loss: 0.5158 - accuracy: 0.84 - ETA: 2s - loss: 0.5136 - accuracy: 0.84 - ETA: 2s - loss: 0.5136 - accuracy: 0.84 - ETA: 2s - loss: 0.5159 - accuracy: 0.83 - ETA: 2s - loss: 0.5156 - accuracy: 0.83 - ETA: 2s - loss: 0.5185 - accuracy: 0.83 - ETA: 2s - loss: 0.5195 - accuracy: 0.83 - ETA: 2s - loss: 0.5207 - accuracy: 0.83 - ETA: 1s - loss: 0.5196 - accuracy: 0.83 - ETA: 1s - loss: 0.5172 - accuracy: 0.83 - ETA: 1s - loss: 0.5191 - accuracy: 0.83 - ETA: 1s - loss: 0.5179 - accuracy: 0.83 - ETA: 1s - loss: 0.5181 - accuracy: 0.83 - ETA: 1s - loss: 0.5176 - accuracy: 0.83 - ETA: 1s - loss: 0.5159 - accuracy: 0.83 - ETA: 1s - loss: 0.5142 - accuracy: 0.84 - ETA: 1s - loss: 0.5170 - accuracy: 0.83 - ETA: 1s - loss: 0.5161 - accuracy: 0.84 - ETA: 1s - loss: 0.5144 - accuracy: 0.84 - ETA: 1s - loss: 0.5152 - accuracy: 0.84 - ETA: 1s - loss: 0.5159 - accuracy: 0.83 - ETA: 1s - loss: 0.5147 - accuracy: 0.84 - ETA: 1s - loss: 0.5137 - accuracy: 0.84 - ETA: 1s - loss: 0.5135 - accuracy: 0.84 - ETA: 1s - loss: 0.5152 - accuracy: 0.84 - ETA: 1s - loss: 0.5174 - accuracy: 0.83 - ETA: 0s - loss: 0.5184 - accuracy: 0.83 - ETA: 0s - loss: 0.5171 - accuracy: 0.83 - ETA: 0s - loss: 0.5170 - accuracy: 0.83 - ETA: 0s - loss: 0.5163 - accuracy: 0.83 - ETA: 0s - loss: 0.5153 - accuracy: 0.83 - ETA: 0s - loss: 0.5144 - accuracy: 0.83 - ETA: 0s - loss: 0.5140 - accuracy: 0.83 - ETA: 0s - loss: 0.5157 - accuracy: 0.83 - ETA: 0s - loss: 0.5172 - accuracy: 0.83 - ETA: 0s - loss: 0.5166 - accuracy: 0.84 - ETA: 0s - loss: 0.5170 - accuracy: 0.83 - ETA: 0s - loss: 0.5185 - accuracy: 0.83 - ETA: 0s - loss: 0.5191 - accuracy: 0.83 - ETA: 0s - loss: 0.5173 - accuracy: 0.84 - ETA: 0s - loss: 0.5165 - accuracy: 0.84 - ETA: 0s - loss: 0.5158 - accuracy: 0.84 - ETA: 0s - loss: 0.5143 - accuracy: 0.8404\n",
      "Epoch 00003: val_loss improved from 0.58779 to 0.58498, saving model to saved_models/weights.best.Xception.hdf5\n",
      "334/334 [==============================] - 11s 33ms/step - loss: 0.5143 - accuracy: 0.8404 - val_loss: 0.5850 - val_accuracy: 0.8180\n",
      "Epoch 4/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "332/334 [============================>.] - ETA: 0s - loss: 0.1398 - accuracy: 0.95 - ETA: 5s - loss: 0.1621 - accuracy: 0.93 - ETA: 7s - loss: 0.2121 - accuracy: 0.92 - ETA: 8s - loss: 0.2771 - accuracy: 0.90 - ETA: 8s - loss: 0.2864 - accuracy: 0.90 - ETA: 8s - loss: 0.2931 - accuracy: 0.90 - ETA: 8s - loss: 0.2906 - accuracy: 0.90 - ETA: 8s - loss: 0.2820 - accuracy: 0.90 - ETA: 8s - loss: 0.2778 - accuracy: 0.91 - ETA: 8s - loss: 0.3018 - accuracy: 0.91 - ETA: 8s - loss: 0.3015 - accuracy: 0.90 - ETA: 8s - loss: 0.3362 - accuracy: 0.89 - ETA: 8s - loss: 0.3350 - accuracy: 0.89 - ETA: 8s - loss: 0.3297 - accuracy: 0.89 - ETA: 8s - loss: 0.3269 - accuracy: 0.89 - ETA: 8s - loss: 0.3709 - accuracy: 0.89 - ETA: 8s - loss: 0.3722 - accuracy: 0.88 - ETA: 8s - loss: 0.3668 - accuracy: 0.89 - ETA: 8s - loss: 0.3611 - accuracy: 0.89 - ETA: 8s - loss: 0.3548 - accuracy: 0.89 - ETA: 8s - loss: 0.3550 - accuracy: 0.89 - ETA: 8s - loss: 0.3574 - accuracy: 0.89 - ETA: 8s - loss: 0.3500 - accuracy: 0.89 - ETA: 8s - loss: 0.3595 - accuracy: 0.88 - ETA: 8s - loss: 0.3667 - accuracy: 0.88 - ETA: 7s - loss: 0.3676 - accuracy: 0.89 - ETA: 7s - loss: 0.3722 - accuracy: 0.89 - ETA: 7s - loss: 0.3741 - accuracy: 0.89 - ETA: 7s - loss: 0.3781 - accuracy: 0.88 - ETA: 7s - loss: 0.3849 - accuracy: 0.88 - ETA: 7s - loss: 0.3856 - accuracy: 0.88 - ETA: 7s - loss: 0.3868 - accuracy: 0.88 - ETA: 7s - loss: 0.3958 - accuracy: 0.88 - ETA: 7s - loss: 0.3934 - accuracy: 0.88 - ETA: 7s - loss: 0.4104 - accuracy: 0.87 - ETA: 7s - loss: 0.4078 - accuracy: 0.87 - ETA: 7s - loss: 0.4091 - accuracy: 0.87 - ETA: 7s - loss: 0.4074 - accuracy: 0.87 - ETA: 7s - loss: 0.4078 - accuracy: 0.87 - ETA: 7s - loss: 0.4044 - accuracy: 0.87 - ETA: 7s - loss: 0.4040 - accuracy: 0.87 - ETA: 7s - loss: 0.3980 - accuracy: 0.88 - ETA: 7s - loss: 0.3972 - accuracy: 0.87 - ETA: 7s - loss: 0.3995 - accuracy: 0.87 - ETA: 7s - loss: 0.3956 - accuracy: 0.87 - ETA: 7s - loss: 0.3966 - accuracy: 0.88 - ETA: 6s - loss: 0.3995 - accuracy: 0.88 - ETA: 6s - loss: 0.3995 - accuracy: 0.88 - ETA: 6s - loss: 0.3979 - accuracy: 0.88 - ETA: 6s - loss: 0.4006 - accuracy: 0.88 - ETA: 6s - loss: 0.3980 - accuracy: 0.88 - ETA: 6s - loss: 0.3978 - accuracy: 0.88 - ETA: 6s - loss: 0.4011 - accuracy: 0.87 - ETA: 6s - loss: 0.4032 - accuracy: 0.87 - ETA: 6s - loss: 0.3997 - accuracy: 0.87 - ETA: 6s - loss: 0.4000 - accuracy: 0.87 - ETA: 6s - loss: 0.4083 - accuracy: 0.87 - ETA: 6s - loss: 0.4175 - accuracy: 0.87 - ETA: 6s - loss: 0.4164 - accuracy: 0.87 - ETA: 6s - loss: 0.4221 - accuracy: 0.87 - ETA: 6s - loss: 0.4176 - accuracy: 0.87 - ETA: 6s - loss: 0.4163 - accuracy: 0.87 - ETA: 6s - loss: 0.4242 - accuracy: 0.87 - ETA: 6s - loss: 0.4224 - accuracy: 0.87 - ETA: 6s - loss: 0.4183 - accuracy: 0.87 - ETA: 5s - loss: 0.4185 - accuracy: 0.87 - ETA: 5s - loss: 0.4168 - accuracy: 0.87 - ETA: 5s - loss: 0.4155 - accuracy: 0.87 - ETA: 5s - loss: 0.4152 - accuracy: 0.87 - ETA: 5s - loss: 0.4154 - accuracy: 0.87 - ETA: 5s - loss: 0.4120 - accuracy: 0.87 - ETA: 5s - loss: 0.4137 - accuracy: 0.87 - ETA: 5s - loss: 0.4160 - accuracy: 0.87 - ETA: 5s - loss: 0.4131 - accuracy: 0.87 - ETA: 5s - loss: 0.4124 - accuracy: 0.87 - ETA: 5s - loss: 0.4107 - accuracy: 0.87 - ETA: 5s - loss: 0.4107 - accuracy: 0.87 - ETA: 5s - loss: 0.4142 - accuracy: 0.87 - ETA: 5s - loss: 0.4116 - accuracy: 0.87 - ETA: 5s - loss: 0.4132 - accuracy: 0.87 - ETA: 5s - loss: 0.4141 - accuracy: 0.87 - ETA: 5s - loss: 0.4135 - accuracy: 0.87 - ETA: 5s - loss: 0.4130 - accuracy: 0.87 - ETA: 5s - loss: 0.4099 - accuracy: 0.87 - ETA: 5s - loss: 0.4088 - accuracy: 0.87 - ETA: 5s - loss: 0.4095 - accuracy: 0.87 - ETA: 4s - loss: 0.4102 - accuracy: 0.87 - ETA: 4s - loss: 0.4080 - accuracy: 0.87 - ETA: 4s - loss: 0.4078 - accuracy: 0.87 - ETA: 4s - loss: 0.4092 - accuracy: 0.87 - ETA: 4s - loss: 0.4068 - accuracy: 0.87 - ETA: 4s - loss: 0.4126 - accuracy: 0.87 - ETA: 4s - loss: 0.4098 - accuracy: 0.87 - ETA: 4s - loss: 0.4135 - accuracy: 0.87 - ETA: 4s - loss: 0.4166 - accuracy: 0.87 - ETA: 4s - loss: 0.4234 - accuracy: 0.87 - ETA: 4s - loss: 0.4219 - accuracy: 0.87 - ETA: 4s - loss: 0.4189 - accuracy: 0.87 - ETA: 4s - loss: 0.4207 - accuracy: 0.87 - ETA: 4s - loss: 0.4194 - accuracy: 0.87 - ETA: 4s - loss: 0.4199 - accuracy: 0.87 - ETA: 4s - loss: 0.4187 - accuracy: 0.87 - ETA: 4s - loss: 0.4207 - accuracy: 0.87 - ETA: 3s - loss: 0.4216 - accuracy: 0.87 - ETA: 3s - loss: 0.4248 - accuracy: 0.87 - ETA: 3s - loss: 0.4293 - accuracy: 0.87 - ETA: 3s - loss: 0.4336 - accuracy: 0.87 - ETA: 3s - loss: 0.4340 - accuracy: 0.87 - ETA: 3s - loss: 0.4332 - accuracy: 0.87 - ETA: 3s - loss: 0.4298 - accuracy: 0.87 - ETA: 3s - loss: 0.4335 - accuracy: 0.87 - ETA: 3s - loss: 0.4334 - accuracy: 0.87 - ETA: 3s - loss: 0.4369 - accuracy: 0.87 - ETA: 3s - loss: 0.4363 - accuracy: 0.87 - ETA: 3s - loss: 0.4351 - accuracy: 0.87 - ETA: 3s - loss: 0.4365 - accuracy: 0.87 - ETA: 3s - loss: 0.4353 - accuracy: 0.87 - ETA: 3s - loss: 0.4393 - accuracy: 0.87 - ETA: 3s - loss: 0.4394 - accuracy: 0.87 - ETA: 2s - loss: 0.4430 - accuracy: 0.86 - ETA: 2s - loss: 0.4435 - accuracy: 0.86 - ETA: 2s - loss: 0.4424 - accuracy: 0.86 - ETA: 2s - loss: 0.4432 - accuracy: 0.86 - ETA: 2s - loss: 0.4434 - accuracy: 0.86 - ETA: 2s - loss: 0.4436 - accuracy: 0.86 - ETA: 2s - loss: 0.4425 - accuracy: 0.86 - ETA: 2s - loss: 0.4429 - accuracy: 0.86 - ETA: 2s - loss: 0.4450 - accuracy: 0.86 - ETA: 2s - loss: 0.4462 - accuracy: 0.86 - ETA: 2s - loss: 0.4449 - accuracy: 0.86 - ETA: 2s - loss: 0.4444 - accuracy: 0.86 - ETA: 2s - loss: 0.4439 - accuracy: 0.86 - ETA: 2s - loss: 0.4424 - accuracy: 0.86 - ETA: 1s - loss: 0.4404 - accuracy: 0.86 - ETA: 1s - loss: 0.4410 - accuracy: 0.86 - ETA: 1s - loss: 0.4390 - accuracy: 0.86 - ETA: 1s - loss: 0.4379 - accuracy: 0.86 - ETA: 1s - loss: 0.4362 - accuracy: 0.86 - ETA: 1s - loss: 0.4333 - accuracy: 0.86 - ETA: 1s - loss: 0.4357 - accuracy: 0.86 - ETA: 1s - loss: 0.4351 - accuracy: 0.86 - ETA: 1s - loss: 0.4370 - accuracy: 0.86 - ETA: 1s - loss: 0.4362 - accuracy: 0.86 - ETA: 1s - loss: 0.4369 - accuracy: 0.86 - ETA: 1s - loss: 0.4365 - accuracy: 0.86 - ETA: 0s - loss: 0.4374 - accuracy: 0.86 - ETA: 0s - loss: 0.4368 - accuracy: 0.86 - ETA: 0s - loss: 0.4408 - accuracy: 0.86 - ETA: 0s - loss: 0.4405 - accuracy: 0.86 - ETA: 0s - loss: 0.4394 - accuracy: 0.86 - ETA: 0s - loss: 0.4417 - accuracy: 0.86 - ETA: 0s - loss: 0.4399 - accuracy: 0.86 - ETA: 0s - loss: 0.4400 - accuracy: 0.86 - ETA: 0s - loss: 0.4380 - accuracy: 0.86 - ETA: 0s - loss: 0.4362 - accuracy: 0.86 - ETA: 0s - loss: 0.4342 - accuracy: 0.86 - ETA: 0s - loss: 0.4337 - accuracy: 0.8693\n",
      "Epoch 00004: val_loss did not improve from 0.58498\n",
      "334/334 [==============================] - 11s 32ms/step - loss: 0.4339 - accuracy: 0.8692 - val_loss: 0.6309 - val_accuracy: 0.8216\n",
      "Epoch 5/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "333/334 [============================>.] - ETA: 0s - loss: 0.3121 - accuracy: 0.85 - ETA: 5s - loss: 0.2163 - accuracy: 0.91 - ETA: 6s - loss: 0.2935 - accuracy: 0.91 - ETA: 7s - loss: 0.3761 - accuracy: 0.90 - ETA: 8s - loss: 0.4758 - accuracy: 0.88 - ETA: 8s - loss: 0.4538 - accuracy: 0.89 - ETA: 8s - loss: 0.4033 - accuracy: 0.90 - ETA: 8s - loss: 0.4072 - accuracy: 0.90 - ETA: 8s - loss: 0.3886 - accuracy: 0.89 - ETA: 8s - loss: 0.3895 - accuracy: 0.89 - ETA: 8s - loss: 0.3830 - accuracy: 0.89 - ETA: 8s - loss: 0.3899 - accuracy: 0.89 - ETA: 8s - loss: 0.3905 - accuracy: 0.88 - ETA: 8s - loss: 0.3809 - accuracy: 0.88 - ETA: 8s - loss: 0.3867 - accuracy: 0.88 - ETA: 8s - loss: 0.3882 - accuracy: 0.88 - ETA: 8s - loss: 0.3846 - accuracy: 0.88 - ETA: 8s - loss: 0.3808 - accuracy: 0.88 - ETA: 8s - loss: 0.3686 - accuracy: 0.89 - ETA: 8s - loss: 0.3641 - accuracy: 0.89 - ETA: 8s - loss: 0.3684 - accuracy: 0.89 - ETA: 8s - loss: 0.3745 - accuracy: 0.88 - ETA: 8s - loss: 0.3673 - accuracy: 0.88 - ETA: 8s - loss: 0.3735 - accuracy: 0.88 - ETA: 8s - loss: 0.3742 - accuracy: 0.88 - ETA: 8s - loss: 0.3669 - accuracy: 0.88 - ETA: 8s - loss: 0.3648 - accuracy: 0.88 - ETA: 8s - loss: 0.3660 - accuracy: 0.88 - ETA: 8s - loss: 0.3692 - accuracy: 0.88 - ETA: 8s - loss: 0.3706 - accuracy: 0.88 - ETA: 7s - loss: 0.3672 - accuracy: 0.88 - ETA: 7s - loss: 0.3620 - accuracy: 0.88 - ETA: 7s - loss: 0.3563 - accuracy: 0.88 - ETA: 7s - loss: 0.3561 - accuracy: 0.88 - ETA: 7s - loss: 0.3509 - accuracy: 0.88 - ETA: 7s - loss: 0.3512 - accuracy: 0.88 - ETA: 7s - loss: 0.3467 - accuracy: 0.88 - ETA: 7s - loss: 0.3514 - accuracy: 0.88 - ETA: 7s - loss: 0.3510 - accuracy: 0.88 - ETA: 7s - loss: 0.3464 - accuracy: 0.89 - ETA: 7s - loss: 0.3434 - accuracy: 0.89 - ETA: 7s - loss: 0.3469 - accuracy: 0.89 - ETA: 7s - loss: 0.3480 - accuracy: 0.89 - ETA: 7s - loss: 0.3480 - accuracy: 0.89 - ETA: 7s - loss: 0.3503 - accuracy: 0.89 - ETA: 7s - loss: 0.3534 - accuracy: 0.88 - ETA: 7s - loss: 0.3502 - accuracy: 0.89 - ETA: 7s - loss: 0.3518 - accuracy: 0.89 - ETA: 7s - loss: 0.3530 - accuracy: 0.89 - ETA: 6s - loss: 0.3572 - accuracy: 0.88 - ETA: 6s - loss: 0.3591 - accuracy: 0.88 - ETA: 6s - loss: 0.3603 - accuracy: 0.88 - ETA: 6s - loss: 0.3622 - accuracy: 0.88 - ETA: 6s - loss: 0.3639 - accuracy: 0.88 - ETA: 6s - loss: 0.3805 - accuracy: 0.88 - ETA: 6s - loss: 0.3800 - accuracy: 0.88 - ETA: 6s - loss: 0.3808 - accuracy: 0.88 - ETA: 6s - loss: 0.3784 - accuracy: 0.88 - ETA: 6s - loss: 0.3741 - accuracy: 0.88 - ETA: 6s - loss: 0.3730 - accuracy: 0.88 - ETA: 6s - loss: 0.3746 - accuracy: 0.88 - ETA: 6s - loss: 0.3708 - accuracy: 0.88 - ETA: 6s - loss: 0.3688 - accuracy: 0.88 - ETA: 6s - loss: 0.3661 - accuracy: 0.88 - ETA: 5s - loss: 0.3730 - accuracy: 0.88 - ETA: 5s - loss: 0.3709 - accuracy: 0.88 - ETA: 5s - loss: 0.3685 - accuracy: 0.88 - ETA: 5s - loss: 0.3702 - accuracy: 0.88 - ETA: 5s - loss: 0.3733 - accuracy: 0.88 - ETA: 5s - loss: 0.3721 - accuracy: 0.88 - ETA: 5s - loss: 0.3724 - accuracy: 0.88 - ETA: 5s - loss: 0.3739 - accuracy: 0.88 - ETA: 5s - loss: 0.3737 - accuracy: 0.88 - ETA: 5s - loss: 0.3714 - accuracy: 0.88 - ETA: 5s - loss: 0.3693 - accuracy: 0.88 - ETA: 5s - loss: 0.3682 - accuracy: 0.88 - ETA: 5s - loss: 0.3699 - accuracy: 0.88 - ETA: 5s - loss: 0.3699 - accuracy: 0.88 - ETA: 5s - loss: 0.3691 - accuracy: 0.88 - ETA: 4s - loss: 0.3647 - accuracy: 0.88 - ETA: 4s - loss: 0.3658 - accuracy: 0.88 - ETA: 4s - loss: 0.3661 - accuracy: 0.88 - ETA: 4s - loss: 0.3676 - accuracy: 0.88 - ETA: 4s - loss: 0.3649 - accuracy: 0.88 - ETA: 4s - loss: 0.3643 - accuracy: 0.88 - ETA: 4s - loss: 0.3647 - accuracy: 0.88 - ETA: 4s - loss: 0.3699 - accuracy: 0.88 - ETA: 4s - loss: 0.3727 - accuracy: 0.88 - ETA: 4s - loss: 0.3718 - accuracy: 0.88 - ETA: 4s - loss: 0.3717 - accuracy: 0.88 - ETA: 4s - loss: 0.3755 - accuracy: 0.88 - ETA: 4s - loss: 0.3741 - accuracy: 0.88 - ETA: 4s - loss: 0.3733 - accuracy: 0.88 - ETA: 4s - loss: 0.3713 - accuracy: 0.88 - ETA: 4s - loss: 0.3733 - accuracy: 0.88 - ETA: 4s - loss: 0.3732 - accuracy: 0.88 - ETA: 3s - loss: 0.3730 - accuracy: 0.88 - ETA: 3s - loss: 0.3714 - accuracy: 0.88 - ETA: 3s - loss: 0.3710 - accuracy: 0.88 - ETA: 3s - loss: 0.3701 - accuracy: 0.88 - ETA: 3s - loss: 0.3740 - accuracy: 0.88 - ETA: 3s - loss: 0.3758 - accuracy: 0.88 - ETA: 3s - loss: 0.3779 - accuracy: 0.88 - ETA: 3s - loss: 0.3760 - accuracy: 0.88 - ETA: 3s - loss: 0.3786 - accuracy: 0.88 - ETA: 3s - loss: 0.3776 - accuracy: 0.88 - ETA: 3s - loss: 0.3780 - accuracy: 0.88 - ETA: 3s - loss: 0.3776 - accuracy: 0.88 - ETA: 3s - loss: 0.3803 - accuracy: 0.88 - ETA: 3s - loss: 0.3794 - accuracy: 0.88 - ETA: 3s - loss: 0.3787 - accuracy: 0.88 - ETA: 3s - loss: 0.3794 - accuracy: 0.88 - ETA: 3s - loss: 0.3776 - accuracy: 0.88 - ETA: 3s - loss: 0.3773 - accuracy: 0.88 - ETA: 2s - loss: 0.3770 - accuracy: 0.88 - ETA: 2s - loss: 0.3779 - accuracy: 0.88 - ETA: 2s - loss: 0.3810 - accuracy: 0.88 - ETA: 2s - loss: 0.3794 - accuracy: 0.88 - ETA: 2s - loss: 0.3789 - accuracy: 0.88 - ETA: 2s - loss: 0.3794 - accuracy: 0.88 - ETA: 2s - loss: 0.3823 - accuracy: 0.88 - ETA: 2s - loss: 0.3828 - accuracy: 0.88 - ETA: 2s - loss: 0.3826 - accuracy: 0.88 - ETA: 2s - loss: 0.3828 - accuracy: 0.88 - ETA: 2s - loss: 0.3826 - accuracy: 0.88 - ETA: 2s - loss: 0.3835 - accuracy: 0.88 - ETA: 2s - loss: 0.3842 - accuracy: 0.88 - ETA: 2s - loss: 0.3833 - accuracy: 0.88 - ETA: 2s - loss: 0.3834 - accuracy: 0.88 - ETA: 2s - loss: 0.3850 - accuracy: 0.88 - ETA: 2s - loss: 0.3832 - accuracy: 0.88 - ETA: 2s - loss: 0.3832 - accuracy: 0.88 - ETA: 2s - loss: 0.3832 - accuracy: 0.88 - ETA: 2s - loss: 0.3840 - accuracy: 0.88 - ETA: 2s - loss: 0.3855 - accuracy: 0.88 - ETA: 1s - loss: 0.3851 - accuracy: 0.88 - ETA: 1s - loss: 0.3853 - accuracy: 0.88 - ETA: 1s - loss: 0.3860 - accuracy: 0.88 - ETA: 1s - loss: 0.3865 - accuracy: 0.88 - ETA: 1s - loss: 0.3853 - accuracy: 0.88 - ETA: 1s - loss: 0.3844 - accuracy: 0.88 - ETA: 1s - loss: 0.3845 - accuracy: 0.88 - ETA: 1s - loss: 0.3861 - accuracy: 0.88 - ETA: 1s - loss: 0.3849 - accuracy: 0.88 - ETA: 1s - loss: 0.3845 - accuracy: 0.88 - ETA: 1s - loss: 0.3845 - accuracy: 0.88 - ETA: 1s - loss: 0.3857 - accuracy: 0.88 - ETA: 1s - loss: 0.3862 - accuracy: 0.88 - ETA: 1s - loss: 0.3875 - accuracy: 0.88 - ETA: 1s - loss: 0.3855 - accuracy: 0.88 - ETA: 1s - loss: 0.3846 - accuracy: 0.88 - ETA: 1s - loss: 0.3832 - accuracy: 0.88 - ETA: 1s - loss: 0.3827 - accuracy: 0.88 - ETA: 0s - loss: 0.3818 - accuracy: 0.88 - ETA: 0s - loss: 0.3838 - accuracy: 0.88 - ETA: 0s - loss: 0.3825 - accuracy: 0.88 - ETA: 0s - loss: 0.3826 - accuracy: 0.88 - ETA: 0s - loss: 0.3828 - accuracy: 0.88 - ETA: 0s - loss: 0.3816 - accuracy: 0.88 - ETA: 0s - loss: 0.3805 - accuracy: 0.88 - ETA: 0s - loss: 0.3807 - accuracy: 0.88 - ETA: 0s - loss: 0.3811 - accuracy: 0.88 - ETA: 0s - loss: 0.3799 - accuracy: 0.88 - ETA: 0s - loss: 0.3801 - accuracy: 0.88 - ETA: 0s - loss: 0.3799 - accuracy: 0.88 - ETA: 0s - loss: 0.3796 - accuracy: 0.88 - ETA: 0s - loss: 0.3800 - accuracy: 0.88 - ETA: 0s - loss: 0.3794 - accuracy: 0.88 - ETA: 0s - loss: 0.3781 - accuracy: 0.8854\n",
      "Epoch 00005: val_loss did not improve from 0.58498\n",
      "334/334 [==============================] - 12s 35ms/step - loss: 0.3792 - accuracy: 0.8853 - val_loss: 0.5890 - val_accuracy: 0.8359\n",
      "Epoch 6/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "334/334 [==============================] - ETA: 0s - loss: 0.0325 - accuracy: 1.00 - ETA: 7s - loss: 0.1686 - accuracy: 0.98 - ETA: 8s - loss: 0.2354 - accuracy: 0.93 - ETA: 9s - loss: 0.2360 - accuracy: 0.91 - ETA: 9s - loss: 0.3027 - accuracy: 0.91 - ETA: 10s - loss: 0.2725 - accuracy: 0.922 - ETA: 10s - loss: 0.2463 - accuracy: 0.926 - ETA: 10s - loss: 0.2326 - accuracy: 0.926 - ETA: 10s - loss: 0.2745 - accuracy: 0.920 - ETA: 10s - loss: 0.2903 - accuracy: 0.918 - ETA: 10s - loss: 0.2746 - accuracy: 0.919 - ETA: 10s - loss: 0.2923 - accuracy: 0.913 - ETA: 10s - loss: 0.2820 - accuracy: 0.912 - ETA: 10s - loss: 0.2875 - accuracy: 0.909 - ETA: 10s - loss: 0.2806 - accuracy: 0.910 - ETA: 10s - loss: 0.2823 - accuracy: 0.908 - ETA: 10s - loss: 0.2880 - accuracy: 0.907 - ETA: 10s - loss: 0.2783 - accuracy: 0.910 - ETA: 10s - loss: 0.2664 - accuracy: 0.914 - ETA: 10s - loss: 0.2759 - accuracy: 0.912 - ETA: 10s - loss: 0.2682 - accuracy: 0.913 - ETA: 10s - loss: 0.2715 - accuracy: 0.912 - ETA: 10s - loss: 0.2797 - accuracy: 0.910 - ETA: 9s - loss: 0.2818 - accuracy: 0.909 - ETA: 9s - loss: 0.2801 - accuracy: 0.90 - ETA: 9s - loss: 0.2761 - accuracy: 0.90 - ETA: 9s - loss: 0.2749 - accuracy: 0.91 - ETA: 9s - loss: 0.2743 - accuracy: 0.91 - ETA: 9s - loss: 0.2806 - accuracy: 0.91 - ETA: 9s - loss: 0.2783 - accuracy: 0.91 - ETA: 9s - loss: 0.2834 - accuracy: 0.90 - ETA: 9s - loss: 0.2894 - accuracy: 0.90 - ETA: 9s - loss: 0.2956 - accuracy: 0.90 - ETA: 9s - loss: 0.2910 - accuracy: 0.90 - ETA: 9s - loss: 0.2934 - accuracy: 0.90 - ETA: 9s - loss: 0.2960 - accuracy: 0.90 - ETA: 9s - loss: 0.3042 - accuracy: 0.89 - ETA: 9s - loss: 0.3018 - accuracy: 0.90 - ETA: 9s - loss: 0.3048 - accuracy: 0.90 - ETA: 8s - loss: 0.3065 - accuracy: 0.90 - ETA: 9s - loss: 0.3089 - accuracy: 0.90 - ETA: 9s - loss: 0.3092 - accuracy: 0.90 - ETA: 9s - loss: 0.3135 - accuracy: 0.89 - ETA: 8s - loss: 0.3084 - accuracy: 0.89 - ETA: 8s - loss: 0.3054 - accuracy: 0.89 - ETA: 8s - loss: 0.3115 - accuracy: 0.89 - ETA: 8s - loss: 0.3130 - accuracy: 0.89 - ETA: 8s - loss: 0.3131 - accuracy: 0.89 - ETA: 8s - loss: 0.3183 - accuracy: 0.89 - ETA: 8s - loss: 0.3176 - accuracy: 0.89 - ETA: 8s - loss: 0.3132 - accuracy: 0.89 - ETA: 8s - loss: 0.3132 - accuracy: 0.89 - ETA: 8s - loss: 0.3156 - accuracy: 0.89 - ETA: 8s - loss: 0.3120 - accuracy: 0.89 - ETA: 8s - loss: 0.3111 - accuracy: 0.90 - ETA: 8s - loss: 0.3080 - accuracy: 0.90 - ETA: 8s - loss: 0.3085 - accuracy: 0.90 - ETA: 8s - loss: 0.3096 - accuracy: 0.90 - ETA: 8s - loss: 0.3086 - accuracy: 0.90 - ETA: 8s - loss: 0.3068 - accuracy: 0.90 - ETA: 7s - loss: 0.3054 - accuracy: 0.90 - ETA: 7s - loss: 0.3051 - accuracy: 0.90 - ETA: 7s - loss: 0.3074 - accuracy: 0.90 - ETA: 7s - loss: 0.3101 - accuracy: 0.89 - ETA: 7s - loss: 0.3077 - accuracy: 0.89 - ETA: 7s - loss: 0.3092 - accuracy: 0.89 - ETA: 7s - loss: 0.3074 - accuracy: 0.89 - ETA: 7s - loss: 0.3038 - accuracy: 0.89 - ETA: 7s - loss: 0.3068 - accuracy: 0.89 - ETA: 7s - loss: 0.3045 - accuracy: 0.90 - ETA: 7s - loss: 0.3045 - accuracy: 0.89 - ETA: 7s - loss: 0.3029 - accuracy: 0.89 - ETA: 7s - loss: 0.3033 - accuracy: 0.89 - ETA: 7s - loss: 0.3050 - accuracy: 0.89 - ETA: 7s - loss: 0.3069 - accuracy: 0.89 - ETA: 6s - loss: 0.3064 - accuracy: 0.89 - ETA: 6s - loss: 0.3098 - accuracy: 0.89 - ETA: 6s - loss: 0.3103 - accuracy: 0.89 - ETA: 6s - loss: 0.3125 - accuracy: 0.89 - ETA: 6s - loss: 0.3113 - accuracy: 0.89 - ETA: 6s - loss: 0.3127 - accuracy: 0.89 - ETA: 6s - loss: 0.3128 - accuracy: 0.89 - ETA: 6s - loss: 0.3169 - accuracy: 0.89 - ETA: 6s - loss: 0.3189 - accuracy: 0.89 - ETA: 6s - loss: 0.3213 - accuracy: 0.89 - ETA: 6s - loss: 0.3210 - accuracy: 0.89 - ETA: 6s - loss: 0.3229 - accuracy: 0.89 - ETA: 5s - loss: 0.3213 - accuracy: 0.89 - ETA: 5s - loss: 0.3207 - accuracy: 0.89 - ETA: 5s - loss: 0.3226 - accuracy: 0.89 - ETA: 5s - loss: 0.3203 - accuracy: 0.89 - ETA: 5s - loss: 0.3204 - accuracy: 0.89 - ETA: 5s - loss: 0.3195 - accuracy: 0.89 - ETA: 5s - loss: 0.3174 - accuracy: 0.89 - ETA: 5s - loss: 0.3165 - accuracy: 0.89 - ETA: 5s - loss: 0.3199 - accuracy: 0.89 - ETA: 5s - loss: 0.3213 - accuracy: 0.89 - ETA: 5s - loss: 0.3201 - accuracy: 0.89 - ETA: 5s - loss: 0.3204 - accuracy: 0.89 - ETA: 5s - loss: 0.3207 - accuracy: 0.89 - ETA: 4s - loss: 0.3206 - accuracy: 0.89 - ETA: 4s - loss: 0.3207 - accuracy: 0.89 - ETA: 4s - loss: 0.3203 - accuracy: 0.89 - ETA: 4s - loss: 0.3219 - accuracy: 0.89 - ETA: 4s - loss: 0.3227 - accuracy: 0.89 - ETA: 4s - loss: 0.3211 - accuracy: 0.89 - ETA: 4s - loss: 0.3203 - accuracy: 0.89 - ETA: 4s - loss: 0.3213 - accuracy: 0.89 - ETA: 4s - loss: 0.3219 - accuracy: 0.89 - ETA: 4s - loss: 0.3231 - accuracy: 0.89 - ETA: 4s - loss: 0.3215 - accuracy: 0.89 - ETA: 4s - loss: 0.3210 - accuracy: 0.89 - ETA: 4s - loss: 0.3189 - accuracy: 0.89 - ETA: 4s - loss: 0.3196 - accuracy: 0.89 - ETA: 3s - loss: 0.3226 - accuracy: 0.89 - ETA: 3s - loss: 0.3231 - accuracy: 0.89 - ETA: 3s - loss: 0.3235 - accuracy: 0.89 - ETA: 3s - loss: 0.3245 - accuracy: 0.89 - ETA: 3s - loss: 0.3235 - accuracy: 0.89 - ETA: 3s - loss: 0.3223 - accuracy: 0.89 - ETA: 3s - loss: 0.3219 - accuracy: 0.89 - ETA: 3s - loss: 0.3201 - accuracy: 0.89 - ETA: 3s - loss: 0.3184 - accuracy: 0.89 - ETA: 3s - loss: 0.3174 - accuracy: 0.89 - ETA: 3s - loss: 0.3163 - accuracy: 0.89 - ETA: 3s - loss: 0.3147 - accuracy: 0.89 - ETA: 3s - loss: 0.3152 - accuracy: 0.89 - ETA: 3s - loss: 0.3139 - accuracy: 0.89 - ETA: 2s - loss: 0.3130 - accuracy: 0.89 - ETA: 2s - loss: 0.3136 - accuracy: 0.89 - ETA: 2s - loss: 0.3161 - accuracy: 0.89 - ETA: 2s - loss: 0.3160 - accuracy: 0.89 - ETA: 2s - loss: 0.3168 - accuracy: 0.89 - ETA: 2s - loss: 0.3185 - accuracy: 0.89 - ETA: 2s - loss: 0.3197 - accuracy: 0.89 - ETA: 2s - loss: 0.3190 - accuracy: 0.89 - ETA: 2s - loss: 0.3196 - accuracy: 0.89 - ETA: 2s - loss: 0.3188 - accuracy: 0.89 - ETA: 2s - loss: 0.3193 - accuracy: 0.89 - ETA: 2s - loss: 0.3209 - accuracy: 0.89 - ETA: 2s - loss: 0.3246 - accuracy: 0.89 - ETA: 2s - loss: 0.3258 - accuracy: 0.89 - ETA: 1s - loss: 0.3247 - accuracy: 0.89 - ETA: 1s - loss: 0.3261 - accuracy: 0.89 - ETA: 1s - loss: 0.3258 - accuracy: 0.89 - ETA: 1s - loss: 0.3288 - accuracy: 0.89 - ETA: 1s - loss: 0.3304 - accuracy: 0.89 - ETA: 1s - loss: 0.3288 - accuracy: 0.89 - ETA: 1s - loss: 0.3279 - accuracy: 0.89 - ETA: 1s - loss: 0.3282 - accuracy: 0.89 - ETA: 1s - loss: 0.3277 - accuracy: 0.89 - ETA: 1s - loss: 0.3303 - accuracy: 0.89 - ETA: 1s - loss: 0.3292 - accuracy: 0.89 - ETA: 1s - loss: 0.3275 - accuracy: 0.89 - ETA: 1s - loss: 0.3267 - accuracy: 0.89 - ETA: 1s - loss: 0.3278 - accuracy: 0.89 - ETA: 1s - loss: 0.3304 - accuracy: 0.89 - ETA: 1s - loss: 0.3296 - accuracy: 0.89 - ETA: 1s - loss: 0.3305 - accuracy: 0.89 - ETA: 0s - loss: 0.3299 - accuracy: 0.89 - ETA: 0s - loss: 0.3291 - accuracy: 0.89 - ETA: 0s - loss: 0.3283 - accuracy: 0.89 - ETA: 0s - loss: 0.3287 - accuracy: 0.89 - ETA: 0s - loss: 0.3282 - accuracy: 0.89 - ETA: 0s - loss: 0.3309 - accuracy: 0.89 - ETA: 0s - loss: 0.3323 - accuracy: 0.89 - ETA: 0s - loss: 0.3325 - accuracy: 0.89 - ETA: 0s - loss: 0.3309 - accuracy: 0.89 - ETA: 0s - loss: 0.3321 - accuracy: 0.89 - ETA: 0s - loss: 0.3310 - accuracy: 0.89 - ETA: 0s - loss: 0.3328 - accuracy: 0.89 - ETA: 0s - loss: 0.3324 - accuracy: 0.89 - ETA: 0s - loss: 0.3336 - accuracy: 0.8960\n",
      "Epoch 00006: val_loss did not improve from 0.58498\n",
      "334/334 [==============================] - 13s 38ms/step - loss: 0.3336 - accuracy: 0.8960 - val_loss: 0.6430 - val_accuracy: 0.8192\n",
      "Epoch 7/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "333/334 [============================>.] - ETA: 0s - loss: 0.1106 - accuracy: 0.95 - ETA: 8s - loss: 0.1372 - accuracy: 0.92 - ETA: 9s - loss: 0.1288 - accuracy: 0.93 - ETA: 10s - loss: 0.2076 - accuracy: 0.941 - ETA: 11s - loss: 0.2578 - accuracy: 0.925 - ETA: 11s - loss: 0.2570 - accuracy: 0.925 - ETA: 11s - loss: 0.2550 - accuracy: 0.929 - ETA: 11s - loss: 0.2444 - accuracy: 0.930 - ETA: 11s - loss: 0.2735 - accuracy: 0.926 - ETA: 11s - loss: 0.2554 - accuracy: 0.932 - ETA: 11s - loss: 0.2350 - accuracy: 0.939 - ETA: 11s - loss: 0.2397 - accuracy: 0.933 - ETA: 11s - loss: 0.2526 - accuracy: 0.921 - ETA: 11s - loss: 0.2449 - accuracy: 0.922 - ETA: 11s - loss: 0.2438 - accuracy: 0.924 - ETA: 11s - loss: 0.2345 - accuracy: 0.927 - ETA: 11s - loss: 0.2356 - accuracy: 0.927 - ETA: 10s - loss: 0.2332 - accuracy: 0.928 - ETA: 10s - loss: 0.2399 - accuracy: 0.928 - ETA: 10s - loss: 0.2431 - accuracy: 0.928 - ETA: 10s - loss: 0.2371 - accuracy: 0.930 - ETA: 10s - loss: 0.2286 - accuracy: 0.932 - ETA: 10s - loss: 0.2215 - accuracy: 0.933 - ETA: 10s - loss: 0.2280 - accuracy: 0.931 - ETA: 10s - loss: 0.2263 - accuracy: 0.931 - ETA: 9s - loss: 0.2282 - accuracy: 0.931 - ETA: 9s - loss: 0.2316 - accuracy: 0.92 - ETA: 9s - loss: 0.2274 - accuracy: 0.92 - ETA: 9s - loss: 0.2319 - accuracy: 0.92 - ETA: 9s - loss: 0.2303 - accuracy: 0.92 - ETA: 9s - loss: 0.2295 - accuracy: 0.92 - ETA: 9s - loss: 0.2267 - accuracy: 0.92 - ETA: 9s - loss: 0.2217 - accuracy: 0.92 - ETA: 9s - loss: 0.2230 - accuracy: 0.92 - ETA: 9s - loss: 0.2192 - accuracy: 0.92 - ETA: 9s - loss: 0.2205 - accuracy: 0.92 - ETA: 9s - loss: 0.2186 - accuracy: 0.92 - ETA: 8s - loss: 0.2170 - accuracy: 0.92 - ETA: 8s - loss: 0.2188 - accuracy: 0.92 - ETA: 8s - loss: 0.2246 - accuracy: 0.92 - ETA: 8s - loss: 0.2228 - accuracy: 0.92 - ETA: 8s - loss: 0.2207 - accuracy: 0.92 - ETA: 8s - loss: 0.2195 - accuracy: 0.92 - ETA: 8s - loss: 0.2267 - accuracy: 0.92 - ETA: 8s - loss: 0.2266 - accuracy: 0.92 - ETA: 8s - loss: 0.2326 - accuracy: 0.92 - ETA: 8s - loss: 0.2294 - accuracy: 0.92 - ETA: 8s - loss: 0.2278 - accuracy: 0.92 - ETA: 8s - loss: 0.2341 - accuracy: 0.92 - ETA: 8s - loss: 0.2318 - accuracy: 0.92 - ETA: 7s - loss: 0.2353 - accuracy: 0.92 - ETA: 7s - loss: 0.2428 - accuracy: 0.92 - ETA: 7s - loss: 0.2404 - accuracy: 0.92 - ETA: 7s - loss: 0.2363 - accuracy: 0.92 - ETA: 7s - loss: 0.2366 - accuracy: 0.92 - ETA: 7s - loss: 0.2356 - accuracy: 0.92 - ETA: 7s - loss: 0.2369 - accuracy: 0.92 - ETA: 7s - loss: 0.2401 - accuracy: 0.92 - ETA: 7s - loss: 0.2390 - accuracy: 0.92 - ETA: 7s - loss: 0.2370 - accuracy: 0.92 - ETA: 7s - loss: 0.2367 - accuracy: 0.92 - ETA: 7s - loss: 0.2399 - accuracy: 0.92 - ETA: 7s - loss: 0.2456 - accuracy: 0.92 - ETA: 7s - loss: 0.2443 - accuracy: 0.92 - ETA: 7s - loss: 0.2447 - accuracy: 0.92 - ETA: 6s - loss: 0.2432 - accuracy: 0.92 - ETA: 6s - loss: 0.2432 - accuracy: 0.92 - ETA: 6s - loss: 0.2438 - accuracy: 0.92 - ETA: 6s - loss: 0.2428 - accuracy: 0.92 - ETA: 6s - loss: 0.2424 - accuracy: 0.92 - ETA: 6s - loss: 0.2437 - accuracy: 0.92 - ETA: 6s - loss: 0.2443 - accuracy: 0.92 - ETA: 6s - loss: 0.2420 - accuracy: 0.92 - ETA: 6s - loss: 0.2476 - accuracy: 0.92 - ETA: 6s - loss: 0.2496 - accuracy: 0.92 - ETA: 6s - loss: 0.2468 - accuracy: 0.92 - ETA: 6s - loss: 0.2465 - accuracy: 0.92 - ETA: 6s - loss: 0.2472 - accuracy: 0.92 - ETA: 6s - loss: 0.2496 - accuracy: 0.92 - ETA: 6s - loss: 0.2519 - accuracy: 0.91 - ETA: 6s - loss: 0.2537 - accuracy: 0.91 - ETA: 6s - loss: 0.2532 - accuracy: 0.91 - ETA: 6s - loss: 0.2530 - accuracy: 0.91 - ETA: 5s - loss: 0.2535 - accuracy: 0.91 - ETA: 5s - loss: 0.2549 - accuracy: 0.91 - ETA: 5s - loss: 0.2620 - accuracy: 0.91 - ETA: 5s - loss: 0.2610 - accuracy: 0.91 - ETA: 5s - loss: 0.2619 - accuracy: 0.91 - ETA: 5s - loss: 0.2603 - accuracy: 0.91 - ETA: 5s - loss: 0.2616 - accuracy: 0.91 - ETA: 5s - loss: 0.2642 - accuracy: 0.91 - ETA: 5s - loss: 0.2661 - accuracy: 0.91 - ETA: 5s - loss: 0.2654 - accuracy: 0.91 - ETA: 5s - loss: 0.2661 - accuracy: 0.91 - ETA: 5s - loss: 0.2677 - accuracy: 0.91 - ETA: 5s - loss: 0.2672 - accuracy: 0.91 - ETA: 5s - loss: 0.2685 - accuracy: 0.91 - ETA: 5s - loss: 0.2691 - accuracy: 0.91 - ETA: 5s - loss: 0.2751 - accuracy: 0.91 - ETA: 4s - loss: 0.2733 - accuracy: 0.91 - ETA: 4s - loss: 0.2712 - accuracy: 0.91 - ETA: 4s - loss: 0.2730 - accuracy: 0.91 - ETA: 4s - loss: 0.2734 - accuracy: 0.91 - ETA: 4s - loss: 0.2723 - accuracy: 0.91 - ETA: 4s - loss: 0.2702 - accuracy: 0.91 - ETA: 4s - loss: 0.2705 - accuracy: 0.91 - ETA: 4s - loss: 0.2746 - accuracy: 0.91 - ETA: 4s - loss: 0.2760 - accuracy: 0.91 - ETA: 4s - loss: 0.2819 - accuracy: 0.91 - ETA: 4s - loss: 0.2805 - accuracy: 0.91 - ETA: 4s - loss: 0.2786 - accuracy: 0.91 - ETA: 4s - loss: 0.2778 - accuracy: 0.91 - ETA: 4s - loss: 0.2774 - accuracy: 0.91 - ETA: 4s - loss: 0.2768 - accuracy: 0.91 - ETA: 3s - loss: 0.2764 - accuracy: 0.91 - ETA: 3s - loss: 0.2786 - accuracy: 0.91 - ETA: 3s - loss: 0.2782 - accuracy: 0.91 - ETA: 3s - loss: 0.2778 - accuracy: 0.91 - ETA: 3s - loss: 0.2781 - accuracy: 0.91 - ETA: 3s - loss: 0.2809 - accuracy: 0.91 - ETA: 3s - loss: 0.2809 - accuracy: 0.91 - ETA: 3s - loss: 0.2806 - accuracy: 0.91 - ETA: 3s - loss: 0.2794 - accuracy: 0.91 - ETA: 3s - loss: 0.2798 - accuracy: 0.91 - ETA: 3s - loss: 0.2813 - accuracy: 0.91 - ETA: 3s - loss: 0.2815 - accuracy: 0.91 - ETA: 3s - loss: 0.2797 - accuracy: 0.91 - ETA: 3s - loss: 0.2795 - accuracy: 0.91 - ETA: 3s - loss: 0.2807 - accuracy: 0.91 - ETA: 2s - loss: 0.2841 - accuracy: 0.91 - ETA: 2s - loss: 0.2826 - accuracy: 0.91 - ETA: 2s - loss: 0.2835 - accuracy: 0.91 - ETA: 2s - loss: 0.2839 - accuracy: 0.91 - ETA: 2s - loss: 0.2839 - accuracy: 0.91 - ETA: 2s - loss: 0.2867 - accuracy: 0.90 - ETA: 2s - loss: 0.2897 - accuracy: 0.90 - ETA: 2s - loss: 0.2891 - accuracy: 0.90 - ETA: 2s - loss: 0.2892 - accuracy: 0.90 - ETA: 2s - loss: 0.2882 - accuracy: 0.90 - ETA: 2s - loss: 0.2895 - accuracy: 0.90 - ETA: 2s - loss: 0.2880 - accuracy: 0.90 - ETA: 2s - loss: 0.2881 - accuracy: 0.90 - ETA: 1s - loss: 0.2875 - accuracy: 0.90 - ETA: 1s - loss: 0.2869 - accuracy: 0.90 - ETA: 1s - loss: 0.2870 - accuracy: 0.90 - ETA: 1s - loss: 0.2885 - accuracy: 0.90 - ETA: 1s - loss: 0.2890 - accuracy: 0.90 - ETA: 1s - loss: 0.2884 - accuracy: 0.90 - ETA: 1s - loss: 0.2897 - accuracy: 0.90 - ETA: 1s - loss: 0.2891 - accuracy: 0.90 - ETA: 1s - loss: 0.2884 - accuracy: 0.90 - ETA: 1s - loss: 0.2877 - accuracy: 0.91 - ETA: 1s - loss: 0.2874 - accuracy: 0.91 - ETA: 1s - loss: 0.2869 - accuracy: 0.91 - ETA: 1s - loss: 0.2900 - accuracy: 0.90 - ETA: 1s - loss: 0.2914 - accuracy: 0.90 - ETA: 1s - loss: 0.2917 - accuracy: 0.90 - ETA: 0s - loss: 0.2916 - accuracy: 0.90 - ETA: 0s - loss: 0.2905 - accuracy: 0.90 - ETA: 0s - loss: 0.2945 - accuracy: 0.90 - ETA: 0s - loss: 0.2932 - accuracy: 0.90 - ETA: 0s - loss: 0.2940 - accuracy: 0.90 - ETA: 0s - loss: 0.2938 - accuracy: 0.90 - ETA: 0s - loss: 0.2937 - accuracy: 0.90 - ETA: 0s - loss: 0.2930 - accuracy: 0.90 - ETA: 0s - loss: 0.2931 - accuracy: 0.90 - ETA: 0s - loss: 0.2932 - accuracy: 0.90 - ETA: 0s - loss: 0.2934 - accuracy: 0.90 - ETA: 0s - loss: 0.2935 - accuracy: 0.90 - ETA: 0s - loss: 0.2929 - accuracy: 0.90 - ETA: 0s - loss: 0.2935 - accuracy: 0.9089\n",
      "Epoch 00007: val_loss did not improve from 0.58498\n",
      "334/334 [==============================] - 12s 37ms/step - loss: 0.2930 - accuracy: 0.9090 - val_loss: 0.7287 - val_accuracy: 0.8371\n",
      "Epoch 8/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "333/334 [============================>.] - ETA: 0s - loss: 0.0986 - accuracy: 0.95 - ETA: 6s - loss: 0.3526 - accuracy: 0.88 - ETA: 8s - loss: 0.2430 - accuracy: 0.91 - ETA: 9s - loss: 0.2821 - accuracy: 0.90 - ETA: 9s - loss: 0.2670 - accuracy: 0.89 - ETA: 9s - loss: 0.2490 - accuracy: 0.90 - ETA: 9s - loss: 0.3295 - accuracy: 0.90 - ETA: 9s - loss: 0.3324 - accuracy: 0.89 - ETA: 9s - loss: 0.3369 - accuracy: 0.89 - ETA: 9s - loss: 0.3500 - accuracy: 0.88 - ETA: 9s - loss: 0.3518 - accuracy: 0.88 - ETA: 9s - loss: 0.3396 - accuracy: 0.88 - ETA: 9s - loss: 0.3311 - accuracy: 0.89 - ETA: 9s - loss: 0.3352 - accuracy: 0.89 - ETA: 9s - loss: 0.3511 - accuracy: 0.88 - ETA: 9s - loss: 0.3418 - accuracy: 0.89 - ETA: 9s - loss: 0.3286 - accuracy: 0.89 - ETA: 9s - loss: 0.3325 - accuracy: 0.89 - ETA: 9s - loss: 0.3285 - accuracy: 0.89 - ETA: 9s - loss: 0.3262 - accuracy: 0.89 - ETA: 9s - loss: 0.3286 - accuracy: 0.89 - ETA: 8s - loss: 0.3270 - accuracy: 0.89 - ETA: 8s - loss: 0.3276 - accuracy: 0.89 - ETA: 8s - loss: 0.3304 - accuracy: 0.89 - ETA: 8s - loss: 0.3419 - accuracy: 0.89 - ETA: 8s - loss: 0.3385 - accuracy: 0.89 - ETA: 8s - loss: 0.3327 - accuracy: 0.89 - ETA: 8s - loss: 0.3361 - accuracy: 0.89 - ETA: 8s - loss: 0.3373 - accuracy: 0.89 - ETA: 8s - loss: 0.3401 - accuracy: 0.89 - ETA: 8s - loss: 0.3374 - accuracy: 0.89 - ETA: 8s - loss: 0.3338 - accuracy: 0.89 - ETA: 8s - loss: 0.3299 - accuracy: 0.89 - ETA: 8s - loss: 0.3302 - accuracy: 0.89 - ETA: 8s - loss: 0.3261 - accuracy: 0.89 - ETA: 7s - loss: 0.3318 - accuracy: 0.89 - ETA: 7s - loss: 0.3290 - accuracy: 0.89 - ETA: 7s - loss: 0.3248 - accuracy: 0.89 - ETA: 7s - loss: 0.3268 - accuracy: 0.89 - ETA: 7s - loss: 0.3227 - accuracy: 0.89 - ETA: 7s - loss: 0.3172 - accuracy: 0.89 - ETA: 7s - loss: 0.3142 - accuracy: 0.89 - ETA: 7s - loss: 0.3160 - accuracy: 0.89 - ETA: 7s - loss: 0.3158 - accuracy: 0.89 - ETA: 7s - loss: 0.3126 - accuracy: 0.89 - ETA: 7s - loss: 0.3107 - accuracy: 0.89 - ETA: 7s - loss: 0.3057 - accuracy: 0.89 - ETA: 7s - loss: 0.3035 - accuracy: 0.89 - ETA: 7s - loss: 0.3046 - accuracy: 0.89 - ETA: 7s - loss: 0.3045 - accuracy: 0.90 - ETA: 7s - loss: 0.3028 - accuracy: 0.90 - ETA: 7s - loss: 0.2986 - accuracy: 0.90 - ETA: 7s - loss: 0.2956 - accuracy: 0.90 - ETA: 6s - loss: 0.2972 - accuracy: 0.90 - ETA: 6s - loss: 0.2935 - accuracy: 0.90 - ETA: 6s - loss: 0.2898 - accuracy: 0.90 - ETA: 6s - loss: 0.2908 - accuracy: 0.90 - ETA: 6s - loss: 0.2865 - accuracy: 0.90 - ETA: 6s - loss: 0.2866 - accuracy: 0.90 - ETA: 6s - loss: 0.2845 - accuracy: 0.90 - ETA: 6s - loss: 0.2860 - accuracy: 0.90 - ETA: 6s - loss: 0.2843 - accuracy: 0.90 - ETA: 6s - loss: 0.2834 - accuracy: 0.90 - ETA: 6s - loss: 0.2805 - accuracy: 0.90 - ETA: 6s - loss: 0.2797 - accuracy: 0.90 - ETA: 6s - loss: 0.2795 - accuracy: 0.90 - ETA: 6s - loss: 0.2771 - accuracy: 0.90 - ETA: 6s - loss: 0.2746 - accuracy: 0.90 - ETA: 6s - loss: 0.2765 - accuracy: 0.90 - ETA: 5s - loss: 0.2766 - accuracy: 0.90 - ETA: 5s - loss: 0.2747 - accuracy: 0.90 - ETA: 5s - loss: 0.2778 - accuracy: 0.90 - ETA: 5s - loss: 0.2797 - accuracy: 0.90 - ETA: 5s - loss: 0.2794 - accuracy: 0.90 - ETA: 5s - loss: 0.2794 - accuracy: 0.90 - ETA: 5s - loss: 0.2779 - accuracy: 0.90 - ETA: 5s - loss: 0.2756 - accuracy: 0.90 - ETA: 5s - loss: 0.2736 - accuracy: 0.90 - ETA: 5s - loss: 0.2745 - accuracy: 0.90 - ETA: 5s - loss: 0.2729 - accuracy: 0.90 - ETA: 5s - loss: 0.2734 - accuracy: 0.90 - ETA: 5s - loss: 0.2773 - accuracy: 0.90 - ETA: 5s - loss: 0.2768 - accuracy: 0.90 - ETA: 5s - loss: 0.2799 - accuracy: 0.90 - ETA: 5s - loss: 0.2793 - accuracy: 0.90 - ETA: 5s - loss: 0.2786 - accuracy: 0.90 - ETA: 4s - loss: 0.2794 - accuracy: 0.90 - ETA: 4s - loss: 0.2806 - accuracy: 0.90 - ETA: 4s - loss: 0.2870 - accuracy: 0.90 - ETA: 4s - loss: 0.2890 - accuracy: 0.90 - ETA: 4s - loss: 0.2866 - accuracy: 0.90 - ETA: 4s - loss: 0.2897 - accuracy: 0.90 - ETA: 4s - loss: 0.2868 - accuracy: 0.90 - ETA: 4s - loss: 0.2852 - accuracy: 0.90 - ETA: 4s - loss: 0.2855 - accuracy: 0.90 - ETA: 4s - loss: 0.2841 - accuracy: 0.90 - ETA: 4s - loss: 0.2842 - accuracy: 0.90 - ETA: 4s - loss: 0.2832 - accuracy: 0.90 - ETA: 4s - loss: 0.2809 - accuracy: 0.90 - ETA: 4s - loss: 0.2809 - accuracy: 0.90 - ETA: 4s - loss: 0.2793 - accuracy: 0.90 - ETA: 4s - loss: 0.2768 - accuracy: 0.90 - ETA: 4s - loss: 0.2833 - accuracy: 0.90 - ETA: 4s - loss: 0.2830 - accuracy: 0.90 - ETA: 3s - loss: 0.2832 - accuracy: 0.90 - ETA: 3s - loss: 0.2816 - accuracy: 0.90 - ETA: 3s - loss: 0.2815 - accuracy: 0.90 - ETA: 3s - loss: 0.2807 - accuracy: 0.90 - ETA: 3s - loss: 0.2810 - accuracy: 0.90 - ETA: 3s - loss: 0.2814 - accuracy: 0.90 - ETA: 3s - loss: 0.2817 - accuracy: 0.90 - ETA: 3s - loss: 0.2816 - accuracy: 0.90 - ETA: 3s - loss: 0.2806 - accuracy: 0.90 - ETA: 3s - loss: 0.2802 - accuracy: 0.90 - ETA: 3s - loss: 0.2822 - accuracy: 0.90 - ETA: 3s - loss: 0.2860 - accuracy: 0.90 - ETA: 3s - loss: 0.2869 - accuracy: 0.90 - ETA: 3s - loss: 0.2850 - accuracy: 0.90 - ETA: 3s - loss: 0.2882 - accuracy: 0.90 - ETA: 3s - loss: 0.2871 - accuracy: 0.90 - ETA: 3s - loss: 0.2871 - accuracy: 0.90 - ETA: 2s - loss: 0.2860 - accuracy: 0.90 - ETA: 2s - loss: 0.2896 - accuracy: 0.90 - ETA: 2s - loss: 0.2887 - accuracy: 0.90 - ETA: 2s - loss: 0.2887 - accuracy: 0.90 - ETA: 2s - loss: 0.2897 - accuracy: 0.90 - ETA: 2s - loss: 0.2900 - accuracy: 0.90 - ETA: 2s - loss: 0.2896 - accuracy: 0.90 - ETA: 2s - loss: 0.2897 - accuracy: 0.90 - ETA: 2s - loss: 0.2893 - accuracy: 0.90 - ETA: 2s - loss: 0.2872 - accuracy: 0.91 - ETA: 2s - loss: 0.2869 - accuracy: 0.91 - ETA: 2s - loss: 0.2877 - accuracy: 0.91 - ETA: 2s - loss: 0.2868 - accuracy: 0.91 - ETA: 2s - loss: 0.2872 - accuracy: 0.91 - ETA: 2s - loss: 0.2873 - accuracy: 0.91 - ETA: 2s - loss: 0.2859 - accuracy: 0.91 - ETA: 2s - loss: 0.2860 - accuracy: 0.91 - ETA: 1s - loss: 0.2842 - accuracy: 0.91 - ETA: 1s - loss: 0.2847 - accuracy: 0.91 - ETA: 1s - loss: 0.2842 - accuracy: 0.91 - ETA: 1s - loss: 0.2861 - accuracy: 0.91 - ETA: 1s - loss: 0.2852 - accuracy: 0.91 - ETA: 1s - loss: 0.2840 - accuracy: 0.91 - ETA: 1s - loss: 0.2863 - accuracy: 0.91 - ETA: 1s - loss: 0.2867 - accuracy: 0.91 - ETA: 1s - loss: 0.2872 - accuracy: 0.91 - ETA: 1s - loss: 0.2864 - accuracy: 0.91 - ETA: 1s - loss: 0.2864 - accuracy: 0.91 - ETA: 1s - loss: 0.2855 - accuracy: 0.91 - ETA: 1s - loss: 0.2854 - accuracy: 0.91 - ETA: 1s - loss: 0.2874 - accuracy: 0.91 - ETA: 1s - loss: 0.2859 - accuracy: 0.91 - ETA: 0s - loss: 0.2888 - accuracy: 0.90 - ETA: 0s - loss: 0.2880 - accuracy: 0.91 - ETA: 0s - loss: 0.2876 - accuracy: 0.91 - ETA: 0s - loss: 0.2879 - accuracy: 0.91 - ETA: 0s - loss: 0.2887 - accuracy: 0.90 - ETA: 0s - loss: 0.2909 - accuracy: 0.90 - ETA: 0s - loss: 0.2908 - accuracy: 0.90 - ETA: 0s - loss: 0.2910 - accuracy: 0.90 - ETA: 0s - loss: 0.2916 - accuracy: 0.90 - ETA: 0s - loss: 0.2930 - accuracy: 0.90 - ETA: 0s - loss: 0.2918 - accuracy: 0.90 - ETA: 0s - loss: 0.2911 - accuracy: 0.90 - ETA: 0s - loss: 0.2901 - accuracy: 0.90 - ETA: 0s - loss: 0.2893 - accuracy: 0.90 - ETA: 0s - loss: 0.2877 - accuracy: 0.9095\n",
      "Epoch 00008: val_loss did not improve from 0.58498\n",
      "334/334 [==============================] - 12s 34ms/step - loss: 0.2879 - accuracy: 0.9093 - val_loss: 0.7403 - val_accuracy: 0.8311\n",
      "Epoch 9/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "333/334 [============================>.] - ETA: 0s - loss: 0.0733 - accuracy: 0.95 - ETA: 7s - loss: 0.2377 - accuracy: 0.91 - ETA: 7s - loss: 0.2069 - accuracy: 0.93 - ETA: 8s - loss: 0.2832 - accuracy: 0.91 - ETA: 8s - loss: 0.2666 - accuracy: 0.92 - ETA: 8s - loss: 0.2563 - accuracy: 0.92 - ETA: 8s - loss: 0.2387 - accuracy: 0.92 - ETA: 8s - loss: 0.2242 - accuracy: 0.93 - ETA: 8s - loss: 0.2291 - accuracy: 0.93 - ETA: 8s - loss: 0.2063 - accuracy: 0.93 - ETA: 8s - loss: 0.2684 - accuracy: 0.92 - ETA: 8s - loss: 0.2525 - accuracy: 0.93 - ETA: 8s - loss: 0.2414 - accuracy: 0.93 - ETA: 8s - loss: 0.2354 - accuracy: 0.93 - ETA: 8s - loss: 0.2314 - accuracy: 0.93 - ETA: 8s - loss: 0.2239 - accuracy: 0.93 - ETA: 8s - loss: 0.2163 - accuracy: 0.93 - ETA: 8s - loss: 0.2077 - accuracy: 0.93 - ETA: 8s - loss: 0.2043 - accuracy: 0.93 - ETA: 8s - loss: 0.2025 - accuracy: 0.93 - ETA: 8s - loss: 0.2063 - accuracy: 0.93 - ETA: 8s - loss: 0.2102 - accuracy: 0.93 - ETA: 8s - loss: 0.2218 - accuracy: 0.93 - ETA: 8s - loss: 0.2162 - accuracy: 0.93 - ETA: 8s - loss: 0.2135 - accuracy: 0.93 - ETA: 8s - loss: 0.2137 - accuracy: 0.93 - ETA: 8s - loss: 0.2268 - accuracy: 0.93 - ETA: 7s - loss: 0.2325 - accuracy: 0.93 - ETA: 7s - loss: 0.2310 - accuracy: 0.93 - ETA: 7s - loss: 0.2440 - accuracy: 0.92 - ETA: 7s - loss: 0.2478 - accuracy: 0.92 - ETA: 7s - loss: 0.2465 - accuracy: 0.92 - ETA: 7s - loss: 0.2414 - accuracy: 0.92 - ETA: 7s - loss: 0.2449 - accuracy: 0.92 - ETA: 7s - loss: 0.2430 - accuracy: 0.92 - ETA: 7s - loss: 0.2431 - accuracy: 0.92 - ETA: 7s - loss: 0.2416 - accuracy: 0.92 - ETA: 7s - loss: 0.2375 - accuracy: 0.93 - ETA: 7s - loss: 0.2354 - accuracy: 0.93 - ETA: 7s - loss: 0.2416 - accuracy: 0.93 - ETA: 7s - loss: 0.2509 - accuracy: 0.92 - ETA: 7s - loss: 0.2468 - accuracy: 0.93 - ETA: 7s - loss: 0.2446 - accuracy: 0.93 - ETA: 7s - loss: 0.2423 - accuracy: 0.93 - ETA: 7s - loss: 0.2415 - accuracy: 0.93 - ETA: 7s - loss: 0.2437 - accuracy: 0.93 - ETA: 7s - loss: 0.2498 - accuracy: 0.93 - ETA: 7s - loss: 0.2505 - accuracy: 0.92 - ETA: 7s - loss: 0.2636 - accuracy: 0.92 - ETA: 7s - loss: 0.2596 - accuracy: 0.92 - ETA: 7s - loss: 0.2575 - accuracy: 0.92 - ETA: 7s - loss: 0.2567 - accuracy: 0.92 - ETA: 7s - loss: 0.2600 - accuracy: 0.92 - ETA: 7s - loss: 0.2612 - accuracy: 0.92 - ETA: 6s - loss: 0.2631 - accuracy: 0.92 - ETA: 6s - loss: 0.2671 - accuracy: 0.92 - ETA: 6s - loss: 0.2656 - accuracy: 0.92 - ETA: 6s - loss: 0.2663 - accuracy: 0.92 - ETA: 6s - loss: 0.2666 - accuracy: 0.92 - ETA: 6s - loss: 0.2717 - accuracy: 0.92 - ETA: 6s - loss: 0.2694 - accuracy: 0.92 - ETA: 6s - loss: 0.2696 - accuracy: 0.92 - ETA: 6s - loss: 0.2671 - accuracy: 0.92 - ETA: 6s - loss: 0.2646 - accuracy: 0.92 - ETA: 6s - loss: 0.2652 - accuracy: 0.92 - ETA: 6s - loss: 0.2647 - accuracy: 0.92 - ETA: 6s - loss: 0.2659 - accuracy: 0.92 - ETA: 6s - loss: 0.2661 - accuracy: 0.92 - ETA: 6s - loss: 0.2667 - accuracy: 0.92 - ETA: 6s - loss: 0.2670 - accuracy: 0.92 - ETA: 6s - loss: 0.2684 - accuracy: 0.92 - ETA: 6s - loss: 0.2674 - accuracy: 0.92 - ETA: 5s - loss: 0.2660 - accuracy: 0.92 - ETA: 5s - loss: 0.2645 - accuracy: 0.92 - ETA: 5s - loss: 0.2651 - accuracy: 0.92 - ETA: 5s - loss: 0.2652 - accuracy: 0.92 - ETA: 5s - loss: 0.2643 - accuracy: 0.92 - ETA: 5s - loss: 0.2667 - accuracy: 0.92 - ETA: 5s - loss: 0.2664 - accuracy: 0.92 - ETA: 5s - loss: 0.2675 - accuracy: 0.92 - ETA: 5s - loss: 0.2673 - accuracy: 0.92 - ETA: 5s - loss: 0.2657 - accuracy: 0.92 - ETA: 5s - loss: 0.2657 - accuracy: 0.92 - ETA: 5s - loss: 0.2662 - accuracy: 0.92 - ETA: 5s - loss: 0.2645 - accuracy: 0.92 - ETA: 5s - loss: 0.2624 - accuracy: 0.92 - ETA: 4s - loss: 0.2609 - accuracy: 0.92 - ETA: 4s - loss: 0.2607 - accuracy: 0.92 - ETA: 4s - loss: 0.2612 - accuracy: 0.92 - ETA: 4s - loss: 0.2628 - accuracy: 0.92 - ETA: 4s - loss: 0.2661 - accuracy: 0.92 - ETA: 4s - loss: 0.2674 - accuracy: 0.92 - ETA: 4s - loss: 0.2670 - accuracy: 0.92 - ETA: 4s - loss: 0.2662 - accuracy: 0.92 - ETA: 4s - loss: 0.2648 - accuracy: 0.92 - ETA: 4s - loss: 0.2636 - accuracy: 0.92 - ETA: 4s - loss: 0.2640 - accuracy: 0.92 - ETA: 4s - loss: 0.2619 - accuracy: 0.92 - ETA: 4s - loss: 0.2619 - accuracy: 0.92 - ETA: 4s - loss: 0.2607 - accuracy: 0.92 - ETA: 4s - loss: 0.2599 - accuracy: 0.92 - ETA: 4s - loss: 0.2577 - accuracy: 0.92 - ETA: 3s - loss: 0.2558 - accuracy: 0.92 - ETA: 3s - loss: 0.2577 - accuracy: 0.92 - ETA: 3s - loss: 0.2582 - accuracy: 0.92 - ETA: 3s - loss: 0.2585 - accuracy: 0.92 - ETA: 3s - loss: 0.2596 - accuracy: 0.92 - ETA: 3s - loss: 0.2595 - accuracy: 0.92 - ETA: 3s - loss: 0.2580 - accuracy: 0.92 - ETA: 3s - loss: 0.2602 - accuracy: 0.92 - ETA: 3s - loss: 0.2626 - accuracy: 0.92 - ETA: 3s - loss: 0.2625 - accuracy: 0.92 - ETA: 3s - loss: 0.2613 - accuracy: 0.92 - ETA: 3s - loss: 0.2595 - accuracy: 0.92 - ETA: 3s - loss: 0.2593 - accuracy: 0.92 - ETA: 3s - loss: 0.2587 - accuracy: 0.92 - ETA: 3s - loss: 0.2574 - accuracy: 0.92 - ETA: 3s - loss: 0.2596 - accuracy: 0.92 - ETA: 2s - loss: 0.2582 - accuracy: 0.92 - ETA: 2s - loss: 0.2572 - accuracy: 0.92 - ETA: 2s - loss: 0.2580 - accuracy: 0.92 - ETA: 2s - loss: 0.2580 - accuracy: 0.92 - ETA: 2s - loss: 0.2578 - accuracy: 0.92 - ETA: 2s - loss: 0.2564 - accuracy: 0.92 - ETA: 2s - loss: 0.2558 - accuracy: 0.92 - ETA: 2s - loss: 0.2555 - accuracy: 0.92 - ETA: 2s - loss: 0.2573 - accuracy: 0.92 - ETA: 2s - loss: 0.2584 - accuracy: 0.92 - ETA: 2s - loss: 0.2572 - accuracy: 0.92 - ETA: 2s - loss: 0.2573 - accuracy: 0.92 - ETA: 2s - loss: 0.2569 - accuracy: 0.92 - ETA: 2s - loss: 0.2558 - accuracy: 0.92 - ETA: 2s - loss: 0.2547 - accuracy: 0.92 - ETA: 2s - loss: 0.2550 - accuracy: 0.92 - ETA: 1s - loss: 0.2564 - accuracy: 0.92 - ETA: 1s - loss: 0.2556 - accuracy: 0.92 - ETA: 1s - loss: 0.2571 - accuracy: 0.92 - ETA: 1s - loss: 0.2555 - accuracy: 0.92 - ETA: 1s - loss: 0.2544 - accuracy: 0.92 - ETA: 1s - loss: 0.2541 - accuracy: 0.92 - ETA: 1s - loss: 0.2543 - accuracy: 0.92 - ETA: 1s - loss: 0.2534 - accuracy: 0.92 - ETA: 1s - loss: 0.2534 - accuracy: 0.92 - ETA: 1s - loss: 0.2523 - accuracy: 0.92 - ETA: 1s - loss: 0.2511 - accuracy: 0.92 - ETA: 1s - loss: 0.2503 - accuracy: 0.92 - ETA: 1s - loss: 0.2503 - accuracy: 0.92 - ETA: 1s - loss: 0.2508 - accuracy: 0.92 - ETA: 1s - loss: 0.2507 - accuracy: 0.92 - ETA: 1s - loss: 0.2513 - accuracy: 0.92 - ETA: 1s - loss: 0.2505 - accuracy: 0.92 - ETA: 0s - loss: 0.2513 - accuracy: 0.92 - ETA: 0s - loss: 0.2511 - accuracy: 0.92 - ETA: 0s - loss: 0.2520 - accuracy: 0.92 - ETA: 0s - loss: 0.2528 - accuracy: 0.92 - ETA: 0s - loss: 0.2554 - accuracy: 0.92 - ETA: 0s - loss: 0.2587 - accuracy: 0.92 - ETA: 0s - loss: 0.2587 - accuracy: 0.92 - ETA: 0s - loss: 0.2577 - accuracy: 0.92 - ETA: 0s - loss: 0.2580 - accuracy: 0.92 - ETA: 0s - loss: 0.2586 - accuracy: 0.92 - ETA: 0s - loss: 0.2591 - accuracy: 0.92 - ETA: 0s - loss: 0.2582 - accuracy: 0.92 - ETA: 0s - loss: 0.2587 - accuracy: 0.92 - ETA: 0s - loss: 0.2585 - accuracy: 0.92 - ETA: 0s - loss: 0.2584 - accuracy: 0.92 - ETA: 0s - loss: 0.2604 - accuracy: 0.9240\n",
      "Epoch 00009: val_loss did not improve from 0.58498\n",
      "334/334 [==============================] - 11s 33ms/step - loss: 0.2601 - accuracy: 0.9240 - val_loss: 0.6416 - val_accuracy: 0.8515\n",
      "Epoch 10/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "334/334 [==============================] - ETA: 0s - loss: 0.0614 - accuracy: 0.95 - ETA: 8s - loss: 0.0797 - accuracy: 0.95 - ETA: 9s - loss: 0.1268 - accuracy: 0.95 - ETA: 11s - loss: 0.1117 - accuracy: 0.960 - ETA: 10s - loss: 0.1068 - accuracy: 0.957 - ETA: 10s - loss: 0.1404 - accuracy: 0.955 - ETA: 10s - loss: 0.1445 - accuracy: 0.954 - ETA: 10s - loss: 0.1262 - accuracy: 0.961 - ETA: 10s - loss: 0.1117 - accuracy: 0.966 - ETA: 10s - loss: 0.1075 - accuracy: 0.967 - ETA: 9s - loss: 0.1289 - accuracy: 0.963 - ETA: 9s - loss: 0.1477 - accuracy: 0.95 - ETA: 9s - loss: 0.1571 - accuracy: 0.95 - ETA: 9s - loss: 0.1628 - accuracy: 0.95 - ETA: 9s - loss: 0.1675 - accuracy: 0.95 - ETA: 9s - loss: 0.1617 - accuracy: 0.95 - ETA: 9s - loss: 0.1631 - accuracy: 0.95 - ETA: 9s - loss: 0.1595 - accuracy: 0.95 - ETA: 9s - loss: 0.1619 - accuracy: 0.95 - ETA: 9s - loss: 0.1745 - accuracy: 0.94 - ETA: 9s - loss: 0.1873 - accuracy: 0.94 - ETA: 8s - loss: 0.1914 - accuracy: 0.93 - ETA: 8s - loss: 0.1942 - accuracy: 0.93 - ETA: 8s - loss: 0.1951 - accuracy: 0.93 - ETA: 8s - loss: 0.1913 - accuracy: 0.93 - ETA: 8s - loss: 0.2066 - accuracy: 0.93 - ETA: 8s - loss: 0.2047 - accuracy: 0.93 - ETA: 8s - loss: 0.2068 - accuracy: 0.93 - ETA: 8s - loss: 0.2077 - accuracy: 0.93 - ETA: 8s - loss: 0.2027 - accuracy: 0.93 - ETA: 8s - loss: 0.2041 - accuracy: 0.93 - ETA: 8s - loss: 0.2044 - accuracy: 0.93 - ETA: 8s - loss: 0.1999 - accuracy: 0.93 - ETA: 8s - loss: 0.1969 - accuracy: 0.93 - ETA: 8s - loss: 0.1957 - accuracy: 0.93 - ETA: 8s - loss: 0.1950 - accuracy: 0.93 - ETA: 8s - loss: 0.1950 - accuracy: 0.93 - ETA: 7s - loss: 0.1964 - accuracy: 0.93 - ETA: 7s - loss: 0.1997 - accuracy: 0.93 - ETA: 7s - loss: 0.2007 - accuracy: 0.93 - ETA: 7s - loss: 0.1999 - accuracy: 0.93 - ETA: 7s - loss: 0.2032 - accuracy: 0.93 - ETA: 7s - loss: 0.2027 - accuracy: 0.93 - ETA: 7s - loss: 0.2075 - accuracy: 0.93 - ETA: 7s - loss: 0.2069 - accuracy: 0.93 - ETA: 7s - loss: 0.2024 - accuracy: 0.93 - ETA: 7s - loss: 0.1995 - accuracy: 0.93 - ETA: 7s - loss: 0.1992 - accuracy: 0.93 - ETA: 7s - loss: 0.2050 - accuracy: 0.93 - ETA: 7s - loss: 0.2039 - accuracy: 0.93 - ETA: 7s - loss: 0.2051 - accuracy: 0.93 - ETA: 7s - loss: 0.2018 - accuracy: 0.93 - ETA: 7s - loss: 0.2012 - accuracy: 0.93 - ETA: 7s - loss: 0.2011 - accuracy: 0.93 - ETA: 6s - loss: 0.2073 - accuracy: 0.93 - ETA: 6s - loss: 0.2097 - accuracy: 0.93 - ETA: 6s - loss: 0.2077 - accuracy: 0.93 - ETA: 6s - loss: 0.2133 - accuracy: 0.93 - ETA: 6s - loss: 0.2126 - accuracy: 0.93 - ETA: 6s - loss: 0.2166 - accuracy: 0.93 - ETA: 6s - loss: 0.2137 - accuracy: 0.93 - ETA: 6s - loss: 0.2117 - accuracy: 0.93 - ETA: 6s - loss: 0.2181 - accuracy: 0.93 - ETA: 6s - loss: 0.2154 - accuracy: 0.93 - ETA: 6s - loss: 0.2157 - accuracy: 0.93 - ETA: 6s - loss: 0.2125 - accuracy: 0.93 - ETA: 6s - loss: 0.2133 - accuracy: 0.93 - ETA: 6s - loss: 0.2115 - accuracy: 0.93 - ETA: 6s - loss: 0.2112 - accuracy: 0.93 - ETA: 6s - loss: 0.2118 - accuracy: 0.93 - ETA: 6s - loss: 0.2095 - accuracy: 0.93 - ETA: 5s - loss: 0.2080 - accuracy: 0.93 - ETA: 5s - loss: 0.2067 - accuracy: 0.93 - ETA: 5s - loss: 0.2050 - accuracy: 0.93 - ETA: 5s - loss: 0.2055 - accuracy: 0.93 - ETA: 5s - loss: 0.2050 - accuracy: 0.93 - ETA: 5s - loss: 0.2028 - accuracy: 0.93 - ETA: 5s - loss: 0.2014 - accuracy: 0.93 - ETA: 5s - loss: 0.2012 - accuracy: 0.93 - ETA: 5s - loss: 0.1997 - accuracy: 0.93 - ETA: 5s - loss: 0.1976 - accuracy: 0.93 - ETA: 5s - loss: 0.2017 - accuracy: 0.93 - ETA: 5s - loss: 0.2003 - accuracy: 0.93 - ETA: 5s - loss: 0.1988 - accuracy: 0.93 - ETA: 5s - loss: 0.1982 - accuracy: 0.93 - ETA: 5s - loss: 0.1986 - accuracy: 0.93 - ETA: 5s - loss: 0.1991 - accuracy: 0.93 - ETA: 5s - loss: 0.2032 - accuracy: 0.93 - ETA: 4s - loss: 0.2055 - accuracy: 0.93 - ETA: 4s - loss: 0.2068 - accuracy: 0.93 - ETA: 4s - loss: 0.2078 - accuracy: 0.93 - ETA: 4s - loss: 0.2077 - accuracy: 0.93 - ETA: 4s - loss: 0.2072 - accuracy: 0.93 - ETA: 4s - loss: 0.2118 - accuracy: 0.93 - ETA: 4s - loss: 0.2104 - accuracy: 0.93 - ETA: 4s - loss: 0.2086 - accuracy: 0.93 - ETA: 4s - loss: 0.2084 - accuracy: 0.93 - ETA: 4s - loss: 0.2076 - accuracy: 0.93 - ETA: 4s - loss: 0.2085 - accuracy: 0.93 - ETA: 4s - loss: 0.2112 - accuracy: 0.93 - ETA: 4s - loss: 0.2123 - accuracy: 0.93 - ETA: 4s - loss: 0.2120 - accuracy: 0.93 - ETA: 4s - loss: 0.2119 - accuracy: 0.93 - ETA: 4s - loss: 0.2128 - accuracy: 0.93 - ETA: 3s - loss: 0.2166 - accuracy: 0.93 - ETA: 3s - loss: 0.2181 - accuracy: 0.93 - ETA: 3s - loss: 0.2188 - accuracy: 0.93 - ETA: 3s - loss: 0.2188 - accuracy: 0.93 - ETA: 3s - loss: 0.2185 - accuracy: 0.93 - ETA: 3s - loss: 0.2193 - accuracy: 0.93 - ETA: 3s - loss: 0.2179 - accuracy: 0.93 - ETA: 3s - loss: 0.2211 - accuracy: 0.93 - ETA: 3s - loss: 0.2211 - accuracy: 0.93 - ETA: 3s - loss: 0.2217 - accuracy: 0.93 - ETA: 3s - loss: 0.2235 - accuracy: 0.93 - ETA: 3s - loss: 0.2239 - accuracy: 0.93 - ETA: 3s - loss: 0.2229 - accuracy: 0.93 - ETA: 3s - loss: 0.2216 - accuracy: 0.93 - ETA: 3s - loss: 0.2213 - accuracy: 0.93 - ETA: 3s - loss: 0.2215 - accuracy: 0.93 - ETA: 2s - loss: 0.2214 - accuracy: 0.93 - ETA: 2s - loss: 0.2217 - accuracy: 0.93 - ETA: 2s - loss: 0.2207 - accuracy: 0.93 - ETA: 2s - loss: 0.2231 - accuracy: 0.93 - ETA: 2s - loss: 0.2250 - accuracy: 0.93 - ETA: 2s - loss: 0.2236 - accuracy: 0.93 - ETA: 2s - loss: 0.2232 - accuracy: 0.93 - ETA: 2s - loss: 0.2253 - accuracy: 0.93 - ETA: 2s - loss: 0.2247 - accuracy: 0.93 - ETA: 2s - loss: 0.2252 - accuracy: 0.93 - ETA: 2s - loss: 0.2237 - accuracy: 0.93 - ETA: 2s - loss: 0.2233 - accuracy: 0.93 - ETA: 2s - loss: 0.2232 - accuracy: 0.93 - ETA: 2s - loss: 0.2231 - accuracy: 0.93 - ETA: 2s - loss: 0.2245 - accuracy: 0.93 - ETA: 2s - loss: 0.2268 - accuracy: 0.93 - ETA: 1s - loss: 0.2283 - accuracy: 0.93 - ETA: 1s - loss: 0.2284 - accuracy: 0.93 - ETA: 1s - loss: 0.2271 - accuracy: 0.93 - ETA: 1s - loss: 0.2273 - accuracy: 0.93 - ETA: 1s - loss: 0.2300 - accuracy: 0.93 - ETA: 1s - loss: 0.2291 - accuracy: 0.93 - ETA: 1s - loss: 0.2292 - accuracy: 0.93 - ETA: 1s - loss: 0.2289 - accuracy: 0.93 - ETA: 1s - loss: 0.2286 - accuracy: 0.93 - ETA: 1s - loss: 0.2304 - accuracy: 0.93 - ETA: 1s - loss: 0.2306 - accuracy: 0.93 - ETA: 1s - loss: 0.2299 - accuracy: 0.93 - ETA: 1s - loss: 0.2290 - accuracy: 0.93 - ETA: 1s - loss: 0.2286 - accuracy: 0.93 - ETA: 1s - loss: 0.2288 - accuracy: 0.93 - ETA: 0s - loss: 0.2294 - accuracy: 0.92 - ETA: 0s - loss: 0.2313 - accuracy: 0.92 - ETA: 0s - loss: 0.2306 - accuracy: 0.92 - ETA: 0s - loss: 0.2300 - accuracy: 0.92 - ETA: 0s - loss: 0.2317 - accuracy: 0.92 - ETA: 0s - loss: 0.2317 - accuracy: 0.92 - ETA: 0s - loss: 0.2314 - accuracy: 0.92 - ETA: 0s - loss: 0.2328 - accuracy: 0.92 - ETA: 0s - loss: 0.2325 - accuracy: 0.92 - ETA: 0s - loss: 0.2333 - accuracy: 0.92 - ETA: 0s - loss: 0.2331 - accuracy: 0.92 - ETA: 0s - loss: 0.2334 - accuracy: 0.92 - ETA: 0s - loss: 0.2323 - accuracy: 0.92 - ETA: 0s - loss: 0.2327 - accuracy: 0.92 - ETA: 0s - loss: 0.2316 - accuracy: 0.92 - ETA: 0s - loss: 0.2307 - accuracy: 0.92 - ETA: 0s - loss: 0.2295 - accuracy: 0.9298\n",
      "Epoch 00010: val_loss did not improve from 0.58498\n",
      "334/334 [==============================] - 11s 33ms/step - loss: 0.2295 - accuracy: 0.9298 - val_loss: 0.7343 - val_accuracy: 0.8431\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x289031fe4e0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Train the model.\n",
    "from keras.callbacks import ModelCheckpoint \n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='saved_models/weights.best.Xception.hdf5', \n",
    "                               verbose=1, save_best_only=True)\n",
    "\n",
    "Xception_model.fit(train_Xception, train_targets, \n",
    "          validation_data=(valid_Xception, valid_targets),\n",
    "          epochs=10, batch_size=20, callbacks=[checkpointer], verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the model weights with the best validation loss.\n",
    "Xception_model.load_weights('saved_models/weights.best.Xception.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 81.6986%\n"
     ]
    }
   ],
   "source": [
    "#Calculate classification accuracy on the test dataset.\n",
    "Xception_predictions = [np.argmax(Xception_model.predict(np.expand_dims(feature, axis=0))) for feature in test_Xception]\n",
    "test_accuracy = 100*np.mean(np.array(Xception_predictions)==np.argmax(test_targets, axis=1))\n",
    "print('Test accuracy: %.4f%%' % test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test accuracy: 81.6986%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
